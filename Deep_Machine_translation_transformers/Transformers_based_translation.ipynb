{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers_based_translation.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-lsAd_-3lc3"
      },
      "source": [
        "!pip install tensorflow==2.6.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtHpc36l4_9-"
      },
      "source": [
        "import pathlib\n",
        "import random \n",
        "import string \n",
        "import re \n",
        "import numpy as np \n",
        "import tensorflow as tf\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras import layers "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1jryFpo5uoQ"
      },
      "source": [
        "from tensorflow.keras.layers import TextVectorization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGFccYL05ztN"
      },
      "source": [
        "from transformer import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iuk3_wiv574M"
      },
      "source": [
        "text_file = keras.utils.get_file(\n",
        "    fname= 'spa-eng.zip',\n",
        "    origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract = True, \n",
        ") \n",
        "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / 'spa.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--Hh9OL76Fwf"
      },
      "source": [
        "with open(text_file) as f: \n",
        "  lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines: \n",
        "  eng, spa = line.split('\\t')\n",
        "  spa = \"[start] \" + spa + \" [end]\"\n",
        "  text_pairs.append((eng,spa))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2arEWqR6Fs1"
      },
      "source": [
        "for _ in range(4):\n",
        "  print(random.choice(text_pairs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1hb08dy6Fo2"
      },
      "source": [
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15*len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples : num_train_samples+num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples: ]\n",
        "\n",
        "print(len(train_pairs))\n",
        "print(len(val_pairs))\n",
        "print(len(test_pairs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTBH0nn56FhL"
      },
      "source": [
        "train_pairs[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtSIOcMW9V6h"
      },
      "source": [
        "string.punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TwluBtB6FZi"
      },
      "source": [
        "#Vectorizing the text data \n",
        "strip_chars = string.punctuation\n",
        "strip_chars = strip_chars.replace(\"[\",\"\")\n",
        "strip_chars = strip_chars.replace(\"]\",\"\")\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20 \n",
        "batch_size = 64\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "  lowercase = tf.strings.lower(input_string)\n",
        "  return tf.strings.regex_replace(lowercase, \"[%s]\"%re.escape(strip_chars),\"\")\n",
        "\n",
        "eng_vectorization = TextVectorization(max_tokens = vocab_size, output_mode = 'int',output_sequence_length=sequence_length)\n",
        "\n",
        "\n",
        "spa_vectorization = TextVectorization(max_tokens = vocab_size, output_mode = 'int',output_sequence_length=sequence_length+1,standardize=custom_standardization)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KZqhuoHlcM8"
      },
      "source": [
        "random.seed(10)\n",
        "print(random.random())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ9F9frJ6FUI"
      },
      "source": [
        "train_eng_texts = [pair[0] for pair in train_pairs]\n",
        "train_spa_texts = [pair[1] for pair in train_pairs]\n",
        "train_spa_texts = [x.replace(\"Â¿\",'') for x in train_spa_texts]\n",
        "eng_vectorization.adapt(train_eng_texts)\n",
        "spa_vectorization.adapt(train_spa_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HmtIbamBMUR"
      },
      "source": [
        "def format_dataset(eng, spa):\n",
        "    eng = eng_vectorization(eng)\n",
        "    spa = spa_vectorization(spa)\n",
        "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": spa[:, :-1],}, spa[:, 1:])\n",
        "\n",
        "\n",
        "def make_dataset(pairs,batch_size = 64):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ120Fmb_4gG"
      },
      "source": [
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtwjJoAgCOzz"
      },
      "source": [
        "train_pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwpbA3u-APF6"
      },
      "source": [
        "embed_dim = 256\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim)(x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim)(x, encoded_seq_inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6Dw-1UTCHM7"
      },
      "source": [
        "transformer.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MazUKg7ICHKA"
      },
      "source": [
        "#Optimizer which holds on to the loss of RMS it balances the step size very very efficiently to avoid exploding as well as vanishing gradient descent\n",
        "transformer.compile(\n",
        "    optimizer ='rmsprop',loss = 'sparse_categorical_crossentropy',metrics = ['accuracy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD2mqNNlCHHW"
      },
      "source": [
        "#Perplexity and Entropy \n",
        "#BLEU Scores\n",
        "transformer.fit(train_ds,epochs = 1,validation_data = val_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "effvjknSCHEU"
      },
      "source": [
        "#Model serialization\n",
        "transformer.save_weights(\"Translate_de_eng_weights.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opy9E616CHBc"
      },
      "source": [
        "loaded_model = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkb9e5MgSjZF"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWtyYljwSxk9"
      },
      "source": [
        "!ls drive/MyDrive/Weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJrkAVz6CG7C"
      },
      "source": [
        "loaded_model.load_weights('drive/MyDrive/Weights_new/Translate_de_eng_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5wjfvndotht"
      },
      "source": [
        "import pickle\n",
        "train_eng_texts = pickle.load(open('drive/MyDrive/Weights_new/train_eng_texts.sav', 'rb'))\n",
        "train_spa_texts = pickle.load(open('drive/MyDrive/Weights_new/train_spa_texts.sav', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SUJDOpopGII"
      },
      "source": [
        "train_spa_texts = [x.replace(\"Â¿\",'') for x in train_spa_texts]\n",
        "eng_vectorization.adapt(train_eng_texts)\n",
        "spa_vectorization.adapt(train_spa_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjFXGIKNhyP5"
      },
      "source": [
        "spa_vocab = spa_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVThB92KSEVp"
      },
      "source": [
        "def decode_sequence(input_sentence,transformer = loaded_model):\n",
        "    #print(input_sentence)\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    #print(tokenized_input_sentence)\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7FqQsCASERJ"
      },
      "source": [
        "test_eng_text = 'My hotel told me to call you.'\n",
        "translated = decode_sequence(test_eng_text)\n",
        "print(translated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y12zqwh0mKFr"
      },
      "source": [
        "[[ 19 518 125  18   4 226   5   0   0   0   0   0   0   0   0   0   0   0\n",
        "    0   0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh2vDTd8SEOJ"
      },
      "source": [
        "loaded_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNRya4YrSELB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3vit_koSEHn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXa6cqyYPniE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}