{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "conda_python3",
      "language": "python",
      "name": "conda_python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "Copy of Disaster_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4RhQk_BpcwV"
      },
      "source": [
        "# About this kernel\n",
        "\n",
        "I've seen a lot of people pooling the output of BERT, then add some Dense layers. I also saw various learning rates for fine-tuning. In this kernel, I wanted to try some ideas that were used in the original paper that did not appear in many public kernel. Here are some examples:\n",
        "* *No pooling, directly use the CLS embedding*. The original paper uses the output embedding for the `[CLS]` token when it is finetuning for classification tasks, such as sentiment analysis. Since the `[CLS]` token is the first token in our sequence, we simply take the first slice of the 2nd dimension from our tensor of shape `(batch_size, max_len, hidden_dim)`, which result in a tensor of shape `(batch_size, hidden_dim)`.\n",
        "* *No Dense layer*. Simply add a sigmoid output directly to the last layer of BERT, rather than experimenting with different intermediate layers.\n",
        "* *Fixed learning rate, batch size, epochs, optimizer*. As specified by the paper, the optimizer used is Adam, with a learning rate between 2e-5 and 5e-5. Furthermore, they train the model for 3 epochs with a batch size of 32. I wanted to see how well it would perform with those default values.\n",
        "\n",
        "I also wanted to share this kernel as a **concise, reusable, and functional example of how to build a workflow around the TF2 version of BERT**. Indeed, it takes less than **50 lines of code to write a string-to-tokens preprocessing function and model builder**.\n",
        "\n",
        "## References\n",
        "\n",
        "* Source for `bert_encode` function: https://www.kaggle.com/user123454321/bert-starter-inference\n",
        "* All pre-trained BERT models from Tensorflow Hub: https://tfhub.dev/s?q=bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxYprS3Wp4sE",
        "outputId": "83f4edfc-f906-4d4b-d5d2-285360f60404"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnhWx3yRp-QP"
      },
      "source": [
        "!ls drive/MyDrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8rBT1VLpcwW"
      },
      "source": [
        "# We will use the official tokenization script created by the Google team\n",
        "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7agZACSpcwZ"
      },
      "source": [
        "!pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnQvF4m5pcwc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe91a17a-36df-45c3-d84c-89beb9e5c4f9"
      },
      "source": [
        "#Is a tokenizer us\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 34.2 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 22.5 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███                             | 112 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████                            | 153 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████                          | 225 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████                         | 266 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 296 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 307 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 368 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 389 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 409 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 440 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 460 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 471 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 481 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 501 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 512 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 542 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 552 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 563 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 573 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 583 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 593 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 614 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 624 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 634 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 645 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 655 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 665 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 675 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 696 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 706 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 727 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 737 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 747 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 757 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 768 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 778 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 788 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 808 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 819 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 829 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 839 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 849 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 860 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 870 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 880 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 890 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 901 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 911 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 921 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 931 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 942 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 952 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 962 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 972 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 983 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 993 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2 MB 7.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYAt2ZHzpcwe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "605b8cce-e890-4dd0-d81b-4221c74de117"
      },
      "source": [
        "!pip install tensorflow_hub"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_hub) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_hub) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow_hub) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "F9scnaAEpcwi"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import tokenization"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_8wF4A2pcwk"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9d8yLVJpcwl"
      },
      "source": [
        "def encoader_funct(docs, tokenizer, max_len=512):\n",
        "    tokens_list = []\n",
        "    masks = []\n",
        "    segments = []\n",
        "    \n",
        "    for doc in docs:\n",
        "        doc = tokenizer.tokenize(doc)\n",
        "            \n",
        "        doc = doc[:max_len-2]\n",
        "        input_sequence = [\"[CLS]\"] + doc + [\"[SEP]\"]\n",
        "        pad_len = max_len - len(input_sequence)\n",
        "        \n",
        "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
        "        tokens += [0] * pad_len\n",
        "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
        "        segment_ids = [0] * max_len\n",
        "        \n",
        "        tokens_list.append(tokens)\n",
        "        masks.append(pad_masks)\n",
        "        segments.append(segment_ids)\n",
        "    \n",
        "    return np.array(tokens_list), np.array(masks), np.array(segments)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "fgyJB9Zzpcwn"
      },
      "source": [
        "def build_model_funct(bert_layer, max_len=512):\n",
        "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
        "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "    clf_output = sequence_output[:, 0, :]\n",
        "    out = Dense(1, activation='sigmoid')(clf_output)\n",
        "    \n",
        "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
        "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDJRbx3Zpcwq"
      },
      "source": [
        "# Load and Preprocess\n",
        "\n",
        "- Load BERT from the Tensorflow Hub\n",
        "- Load CSV files containing training data\n",
        "- Load tokenizer from the bert layer\n",
        "- Encode the text into tokens, masks, and segment flags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqckKYj1pcwq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b66a3ab9-be49-43bd-80f3-afeb808a99a7"
      },
      "source": [
        "%%time\n",
        "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
        "bert_layer = hub.KerasLayer(module_url, trainable=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 24.6 s, sys: 5.71 s, total: 30.4 s\n",
            "Wall time: 35.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMU4P-bEpcws"
      },
      "source": [
        "train = pd.read_csv(\"train_clean.csv\")\n",
        "test = pd.read_csv(\"test_clean.csv\")\n",
        "submission = pd.read_csv(\"sample_submission.csv\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1duZPwW3efao"
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX-aH6p-pcwu"
      },
      "source": [
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeTuKHMGoIS7"
      },
      "source": [
        "#test = test.dropna(subset = ['trans']) \n",
        "train = train.dropna(subset = ['trans'])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQwkzY7tK4Ix"
      },
      "source": [
        "test = test.fillna(\"\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gD0bPEZCpcww"
      },
      "source": [
        "#tokenizing the text\n",
        "train_input = encoader_funct(train.trans.values, tokenizer, max_len=160)\n",
        "test_input = encoader_funct(test.trans.values, tokenizer, max_len=160)\n",
        "train_labels = train.target.values"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BTviW04VXNR",
        "outputId": "edc4ca75-62da-4b15-93f0-bad99a89c919"
      },
      "source": [
        "len(train_input[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7613"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHbxjZj5pcwz"
      },
      "source": [
        "# Model: Build, Train, Predict, Submit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPkNylhqpcwz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "554d2b4e-e8c6-470c-a0b9-57dc12cf0499"
      },
      "source": [
        "model = build_model_funct(bert_layer, max_len=160)\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 160)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 160)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 160)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem (Slici (None, 1024)         0           keras_layer[0][1]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            1025        tf.__operators__.getitem[0][0]   \n",
            "==================================================================================================\n",
            "Total params: 335,142,914\n",
            "Trainable params: 335,142,913\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c60Vnjm-pcw2"
      },
      "source": [
        "checkpoint = ModelCheckpoint('build_model_funct/MyDrive/model.h5', monitor='val_loss', save_best_only=True)\n",
        "#training the model\n",
        "train_history = model.fit(\n",
        "    train_input, train_labels,\n",
        "    validation_split=0.2,\n",
        "    epochs=5,\n",
        "    callbacks=[checkpoint],\n",
        "    batch_size=16\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "198GlxTZqbwo"
      },
      "source": [
        "!ls drive/MyDrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRRYSvg0qHEY"
      },
      "source": [
        "model.save_weights(\"drive/MyDrive/model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVUYIvAjpcw4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b4ae6409-dbf3-49b4-9169-faf73ad68f31"
      },
      "source": [
        "#with 3 epoch\n",
        "metrics=pd.DataFrame(model.history.history)\n",
        "metrics"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.448901</td>\n",
              "      <td>0.802299</td>\n",
              "      <td>0.394063</td>\n",
              "      <td>0.829941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.270500</td>\n",
              "      <td>0.895731</td>\n",
              "      <td>0.455615</td>\n",
              "      <td>0.814183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.134778</td>\n",
              "      <td>0.952874</td>\n",
              "      <td>0.603888</td>\n",
              "      <td>0.809586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.083507</td>\n",
              "      <td>0.969787</td>\n",
              "      <td>0.689941</td>\n",
              "      <td>0.812213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.055357</td>\n",
              "      <td>0.977176</td>\n",
              "      <td>0.785013</td>\n",
              "      <td>0.815496</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       loss  accuracy  val_loss  val_accuracy\n",
              "0  0.448901  0.802299  0.394063      0.829941\n",
              "1  0.270500  0.895731  0.455615      0.814183\n",
              "2  0.134778  0.952874  0.603888      0.809586\n",
              "3  0.083507  0.969787  0.689941      0.812213\n",
              "4  0.055357  0.977176  0.785013      0.815496"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMrryEubpcw6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1774d54a-a79c-4204-82fa-bef4882f8456"
      },
      "source": [
        "#with cleaned data\n",
        "#with 5 epoch\n",
        "metrics=pd.DataFrame(model.history.history)\n",
        "metrics"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.448901</td>\n",
              "      <td>0.802299</td>\n",
              "      <td>0.394063</td>\n",
              "      <td>0.829941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.270500</td>\n",
              "      <td>0.895731</td>\n",
              "      <td>0.455615</td>\n",
              "      <td>0.814183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.134778</td>\n",
              "      <td>0.952874</td>\n",
              "      <td>0.603888</td>\n",
              "      <td>0.809586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.083507</td>\n",
              "      <td>0.969787</td>\n",
              "      <td>0.689941</td>\n",
              "      <td>0.812213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.055357</td>\n",
              "      <td>0.977176</td>\n",
              "      <td>0.785013</td>\n",
              "      <td>0.815496</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       loss  accuracy  val_loss  val_accuracy\n",
              "0  0.448901  0.802299  0.394063      0.829941\n",
              "1  0.270500  0.895731  0.455615      0.814183\n",
              "2  0.134778  0.952874  0.603888      0.809586\n",
              "3  0.083507  0.969787  0.689941      0.812213\n",
              "4  0.055357  0.977176  0.785013      0.815496"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqwmKP_Npcw9"
      },
      "source": [
        "model.load_weights('drive/MyDrive/model.h5')\n",
        "test_pred = model.predict(test_input)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH-yfYs5pcw_"
      },
      "source": [
        "submission['target'] = test_pred.round().astype(int)\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkk1b0ZIpcxB"
      },
      "source": [
        "test= test.reset_index(drop = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJBcTRKUre_u"
      },
      "source": [
        "pd.DataFrame(test_pred.round().astype(int))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um2PQlIls3Qb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}