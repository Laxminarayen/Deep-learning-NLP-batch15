{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mysql-connector-python-rf\n",
      "  Downloading mysql-connector-python-rf-2.2.2.tar.gz (11.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.9 MB 7.7 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: mysql-connector-python-rf\n",
      "  Building wheel for mysql-connector-python-rf (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mysql-connector-python-rf: filename=mysql_connector_python_rf-2.2.2-cp36-cp36m-linux_x86_64.whl size=249455 sha256=3c6bd1f35136f0098ff8db6449025e43edbd0435c9551ed6901263886321b795\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/51/40/3f/136443b7177ee453aa9d6c8862fc2d1a1ea8ff8ee0999d1971\n",
      "Successfully built mysql-connector-python-rf\n",
      "Installing collected packages: mysql-connector-python-rf\n",
      "Successfully installed mysql-connector-python-rf-2.2.2\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting JPype1==0.6.3\n",
      "  Downloading JPype1-0.6.3.tar.gz (168 kB)\n",
      "\u001b[K     |████████████████████████████████| 168 kB 9.1 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: JPype1\n",
      "  Building wheel for JPype1 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for JPype1: filename=JPype1-0.6.3-cp36-cp36m-linux_x86_64.whl size=2508566 sha256=433a02e051b44c2a2d0f870b2bf975821d019c4dba7707ff8d15aaf05768b41a\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/26/1a/2a/2efafac3f262c651b1720038416cb6d196d283a32de884576e\n",
      "Successfully built JPype1\n",
      "Installing collected packages: JPype1\n",
      "Successfully installed JPype1-0.6.3\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting snowflake-sqlalchemy\n",
      "  Downloading snowflake_sqlalchemy-1.2.3-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied, skipping upgrade: sqlalchemy<2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from snowflake-sqlalchemy) (1.3.15)\n",
      "Collecting snowflake-connector-python<3.0.0\n",
      "  Downloading snowflake_connector_python-2.2.9-cp36-cp36m-manylinux2010_x86_64.whl (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 8.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: cffi<1.15,>=1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (2.9)\n",
      "Collecting pycryptodomex!=3.5.0,<4.0.0,>=3.2\n",
      "  Downloading pycryptodomex-3.9.8-cp36-cp36m-manylinux1_x86_64.whl (13.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.7 MB 73.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting oscrypto<2.0.0\n",
      "  Downloading oscrypto-1.2.0-py2.py3-none-any.whl (192 kB)\n",
      "\u001b[K     |████████████████████████████████| 192 kB 74.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: certifi<2021.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (2020.4.5.2)\n",
      "Collecting azure-common<2.0.0\n",
      "  Downloading azure_common-1.1.25-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied, skipping upgrade: pytz<2021.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (2019.3)\n",
      "Collecting pyjwt<2.0.0\n",
      "  Downloading PyJWT-1.7.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting azure-storage-blob<13.0.0,>=12.0.0; python_version >= \"3.5.2\"\n",
      "  Downloading azure_storage_blob-12.3.2-py2.py3-none-any.whl (280 kB)\n",
      "\u001b[K     |████████████████████████████████| 280 kB 78.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: asn1crypto<2.0.0,>0.24.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pyOpenSSL<21.0.0,>=16.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (19.1.0)\n",
      "Requirement already satisfied, skipping upgrade: cryptography<3.0.0,>=2.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: boto3<1.15,>=1.4.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (1.14.16)\n",
      "Requirement already satisfied, skipping upgrade: requests<2.24.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.26.0,>=1.20 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from cffi<1.15,>=1.9->snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (2.20)\n",
      "Collecting azure-core<2.0.0,>=1.6.0\n",
      "  Downloading azure_core-1.7.0-py2.py3-none-any.whl (121 kB)\n",
      "\u001b[K     |████████████████████████████████| 121 kB 76.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting msrest>=0.6.10\n",
      "  Downloading msrest-0.6.17-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 6.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.5.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyOpenSSL<21.0.0,>=16.2.0->snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.18.0,>=1.17.16 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3<1.15,>=1.4.4->snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (1.17.16)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3<1.15,>=1.4.4->snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3<1.15,>=1.4.4->snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<2.24.0->snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (3.0.4)\n",
      "Collecting requests-oauthlib>=0.5.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting isodate>=0.6.0\n",
      "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 5.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.16->boto3<1.15,>=1.4.4->snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (0.15.2)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.16->boto3<1.15,>=1.4.4->snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (2.8.1)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 78.1 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pycryptodomex, oscrypto, azure-common, pyjwt, azure-core, oauthlib, requests-oauthlib, isodate, msrest, azure-storage-blob, snowflake-connector-python, snowflake-sqlalchemy\n",
      "Successfully installed azure-common-1.1.25 azure-core-1.7.0 azure-storage-blob-12.3.2 isodate-0.6.0 msrest-0.6.17 oauthlib-3.1.0 oscrypto-1.2.0 pycryptodomex-3.9.8 pyjwt-1.7.1 requests-oauthlib-1.3.0 snowflake-connector-python-2.2.9 snowflake-sqlalchemy-1.2.3\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting jaydebeapi\n",
      "  Downloading JayDeBeApi-1.2.3-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: JPype1; python_version > \"2.7\" and platform_python_implementation != \"Jython\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from jaydebeapi) (0.6.3)\n",
      "Installing collected packages: jaydebeapi\n",
      "Successfully installed jaydebeapi-1.2.3\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting emoji\n",
      "  Downloading emoji-0.5.4.tar.gz (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 2.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for emoji: filename=emoji-0.5.4-py3-none-any.whl size=42175 sha256=9dccf6547574db9c743becae68f3bda40cfd931ce0ab0a2d1a8fb20aadb1e7c3\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/22/95/98/a21ffcc81fac65949a085ceff9dca4a145a32d9bbfcbf1cb31\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-0.5.4\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install mysql-connector-python-rf\n",
    "!pip install JPype1==0.6.3 --force-reinstall\n",
    "!pip install --upgrade snowflake-sqlalchemy\n",
    "!pip install jaydebeapi\n",
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT PACKAGES \n",
    "import os\n",
    "#os.system('python -m spacy download en')\n",
    "\n",
    "#to Ignore the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "from pandas import Series, DataFrame\n",
    "import re\n",
    "\n",
    "#to tokenize the text\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize#to tokenize the text\n",
    "import nltk\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from textblob import Word, Blobber\n",
    "import configparser\n",
    "\n",
    "\n",
    "import mysql.connector\n",
    "import psycopg2\n",
    "from collections import OrderedDict\n",
    "#for train and test splitting\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer# to convert the text into sequence of vectors\n",
    "# For evalution\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import boto3\n",
    "import textwrap\n",
    "#for encoading\n",
    "import ftfy\n",
    "from ftfy import fix_encoding\n",
    "from ftfy import fix_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#For lemmitization\n",
    "import en_core_web_sm\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords# for stopwords removal\n",
    "#for Bert modelling\n",
    "from sqlalchemy import create_engine\n",
    "from snowflake.sqlalchemy import URL\n",
    "\n",
    "#for loading pretrained word embedding\n",
    "import gensim \n",
    "from gensim import corpora\n",
    "import pyLDAvis \n",
    "import pyLDAvis.gensim \n",
    " \n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "#for mathematical operation\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (516.2 MB)\n",
      "\u001b[K     |█████████████████████████████▎  | 471.9 MB 90.3 MB/s eta 0:00:01    |█▊                              | 28.6 MB 11.4 MB/s eta 0:00:43"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 516.2 MB 16 kB/s \n",
      "\u001b[?25hCollecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "\u001b[K     |████████████████████████████████| 454 kB 73.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.7.0\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 76.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.12.2)\n",
      "Collecting tensorboard<2.3.0,>=2.2.0\n",
      "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 71.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.18.1)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.30.0-cp36-cp36m-manylinux2010_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 25.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 8.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting keras-preprocessing>=1.1.0\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 2.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 6.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py<2.11.0,>=2.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.4.1)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorflow) (46.1.3.post20200330)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "\u001b[K     |████████████████████████████████| 779 kB 69.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 13.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.0.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.23.0)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.19.2-py2.py3-none-any.whl (91 kB)\n",
      "\u001b[K     |████████████████████████████████| 91 kB 15.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.5.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2020.4.5.2)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 77.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\n",
      "Building wheels for collected packages: absl-py, termcolor\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121931 sha256=37e4b0e4afe046370e61270d97aa6f73fe0de9c2cc3e18d45aad71634a8ffb4c\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/c3/af/84/3962a6af7b4ab336e951b7877dcfb758cf94548bb1771e0679\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=30f8ee2cad2ee15f569a1bc8a14562265ba593ab58953eb5a55cb1ee14092745\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "Successfully built absl-py termcolor\n",
      "Installing collected packages: tensorflow-estimator, absl-py, tensorboard-plugin-wit, markdown, grpcio, pyasn1-modules, cachetools, google-auth, google-auth-oauthlib, tensorboard, google-pasta, gast, keras-preprocessing, opt-einsum, termcolor, astunparse, tensorflow\n",
      "Successfully installed absl-py-0.9.0 astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 google-auth-1.19.2 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.30.0 keras-preprocessing-1.1.2 markdown-3.2.2 opt-einsum-3.3.0 pyasn1-modules-0.2.8 tensorboard-2.2.2 tensorboard-plugin-wit-1.7.0 tensorflow-2.2.0 tensorflow-estimator-2.2.0 termcolor-1.1.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.18.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from h5py->keras) (1.14.0)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.4.3\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For constructing Depp leaning network\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional,RepeatVector,SpatialDropout1D\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Shape = (7613, 5)\n",
      "Training Set Memory Usage = 0.25 MB\n",
      "Test Set Shape = (3263, 4)\n",
      "Test Set Memory Usage = 0.08 MB\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"train.csv\",dtype={'id': np.int16})\n",
    "df_test = pd.read_csv('test.csv', dtype={'id': np.int16})\n",
    "df = pd.read_csv(\"train.csv\",dtype={'id': np.int16})\n",
    "print('Training Set Shape = {}'.format(df_train.shape))\n",
    "print('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() / 1024**2))\n",
    "print('Test Set Shape = {}'.format(df_test.shape))\n",
    "print('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() / 1024**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         0     NaN      NaN   \n",
       "1         2     NaN      NaN   \n",
       "2         3     NaN      NaN   \n",
       "3         9     NaN      NaN   \n",
       "4        11     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "3258  10861     NaN      NaN   \n",
       "3259  10865     NaN      NaN   \n",
       "3260  10868     NaN      NaN   \n",
       "3261  10874     NaN      NaN   \n",
       "3262  10875     NaN      NaN   \n",
       "\n",
       "                                                   text  \n",
       "0                    Just happened a terrible car crash  \n",
       "1     Heard about #earthquake is different cities, s...  \n",
       "2     there is a forest fire at spot pond, geese are...  \n",
       "3              Apocalypse lighting. #Spokane #wildfires  \n",
       "4         Typhoon Soudelor kills 28 in China and Taiwan  \n",
       "...                                                 ...  \n",
       "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...  \n",
       "3259  Storm in RI worse than last hurricane. My city...  \n",
       "3260  Green Line derailment in Chicago http://t.co/U...  \n",
       "3261  MEG issues Hazardous Weather Outlook (HWO) htt...  \n",
       "3262  #CityofCalgary has activated its Municipal Eme...  \n",
       "\n",
       "[3263 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <td>2533</td>\n",
       "      <td>0.332720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword</th>\n",
       "      <td>61</td>\n",
       "      <td>0.008013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Total   Percent\n",
       "location   2533  0.332720\n",
       "keyword      61  0.008013\n",
       "target        0  0.000000\n",
       "text          0  0.000000\n",
       "id            0  0.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get total count of data including missing data\n",
    "total = df_train.isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "#get percent of missing data relevant to all data\n",
    "percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\n",
    "\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(df_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting lower\n",
    "df['tweet'] = df.apply(lambda row: str(row['text']).lower(), axis=1)\n",
    "df_test['text']= df_test.apply(lambda row: str(row['text']).lower(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fix Encoding Issues\n",
    "df['tweet'] = df['tweet'].astype(str).apply(fix_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand contractions\n",
    "\n",
    "contractions_dict = {\n",
    "    \"didn't\": \"did not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    #\"cant\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    #\"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    #\"didnt\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    #\"doesnt \": \"does not \",\n",
    "    \"don't\": \"do not\",\n",
    "    #\"dont \" : \"do not \",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"here's\":\"here is\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he had\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'd\": \"i had\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she had\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they had\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we had\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "\n",
    "def expand_contractions(s, contractions_dict=contractions_dict):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, s)\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(expand_contractions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['text'] = df_test['text'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 58s, sys: 0 ns, total: 3min 58s\n",
      "Wall time: 3min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def clean(tweet): \n",
    "            \n",
    "    # Special characters\n",
    "    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n",
    "    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n",
    "    tweet = re.sub(r\"å_\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n",
    "    tweet = re.sub(r\"åÊ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"åÈ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n",
    "    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n",
    "    tweet = re.sub(r\"å¨\", \"\", tweet)\n",
    "    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n",
    "    tweet = re.sub(r\"åÇ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\n",
    "    tweet = re.sub(r\"åÀ\", \"\", tweet)\n",
    "    \n",
    "    # Contractions\n",
    "    tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
    "    tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
    "    tweet = re.sub(r\"We're\", \"We are\", tweet)\n",
    "    tweet = re.sub(r\"That's\", \"That is\", tweet)\n",
    "    tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
    "    tweet = re.sub(r\"they're\", \"they are\", tweet)\n",
    "    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n",
    "    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n",
    "    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n",
    "    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"What's\", \"What is\", tweet)\n",
    "    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
    "    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n",
    "    tweet = re.sub(r\"There's\", \"There is\", tweet)\n",
    "    tweet = re.sub(r\"He's\", \"He is\", tweet)\n",
    "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"You're\", \"You are\", tweet)\n",
    "    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n",
    "    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n",
    "    tweet = re.sub(r\"you've\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"we're\", \"we are\", tweet)\n",
    "    tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
    "    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
    "    tweet = re.sub(r\"we've\", \"we have\", tweet)\n",
    "    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n",
    "    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n",
    "    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n",
    "    tweet = re.sub(r\"who's\", \"who is\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n",
    "    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n",
    "    tweet = re.sub(r\"would've\", \"would have\", tweet)\n",
    "    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n",
    "    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n",
    "    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"We've\", \"We have\", tweet)\n",
    "    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n",
    "    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n",
    "    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n",
    "    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n",
    "    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n",
    "    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n",
    "    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n",
    "    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n",
    "    tweet = re.sub(r\"they've\", \"they have\", tweet)\n",
    "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"should've\", \"should have\", tweet)\n",
    "    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n",
    "    tweet = re.sub(r\"where's\", \"where is\", tweet)\n",
    "    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n",
    "    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n",
    "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n",
    "    tweet = re.sub(r\"They're\", \"They are\", tweet)\n",
    "    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n",
    "    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"let's\", \"let us\", tweet)\n",
    "    tweet = re.sub(r\"it's\", \"it is\", tweet)\n",
    "    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n",
    "    tweet = re.sub(r\"don't\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"you're\", \"you are\", tweet)\n",
    "    tweet = re.sub(r\"i've\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"that's\", \"that is\", tweet)\n",
    "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n",
    "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n",
    "    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n",
    "    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n",
    "    tweet = re.sub(r\"I've\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n",
    "    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n",
    "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n",
    "    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n",
    "    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n",
    "    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n",
    "    tweet = re.sub(r\"donå«t\", \"do not\", tweet)   \n",
    "            \n",
    "    # Character entity references\n",
    "    tweet = re.sub(r\"&gt;\", \">\", tweet)\n",
    "    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n",
    "    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n",
    "    \n",
    "    # Typos, slang and informal abbreviations\n",
    "    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n",
    "    tweet = re.sub(r\"w/\", \"with\", tweet)\n",
    "    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n",
    "    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n",
    "    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n",
    "    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n",
    "    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n",
    "    tweet = re.sub(r\"<3\", \"love\", tweet)\n",
    "    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n",
    "    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n",
    "    tweet = re.sub(r\"8/5/2015\", \"2015-08-05\", tweet)\n",
    "    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n",
    "    tweet = re.sub(r\"8/6/2015\", \"2015-08-06\", tweet)\n",
    "    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n",
    "    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n",
    "    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n",
    "    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)   \n",
    "    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n",
    "    \n",
    "    # Hashtags and usernames\n",
    "    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\n",
    "    tweet = re.sub(r\"ArianaGrande\", \"Ariana Grande\", tweet)\n",
    "    tweet = re.sub(r\"camilacabello97\", \"camila cabello\", tweet) \n",
    "    tweet = re.sub(r\"RondaRousey\", \"Ronda Rousey\", tweet)     \n",
    "    tweet = re.sub(r\"MTVHottest\", \"MTV Hottest\", tweet)\n",
    "    tweet = re.sub(r\"TrapMusic\", \"Trap Music\", tweet)\n",
    "    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\n",
    "    tweet = re.sub(r\"PantherAttack\", \"Panther Attack\", tweet)\n",
    "    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\n",
    "    tweet = re.sub(r\"socialnews\", \"social news\", tweet)\n",
    "    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n",
    "    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\n",
    "    tweet = re.sub(r\"humanconsumption\", \"human consumption\", tweet)\n",
    "    tweet = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", tweet)\n",
    "    tweet = re.sub(r\"Meat-Loving\", \"Meat Loving\", tweet)\n",
    "    tweet = re.sub(r\"facialabuse\", \"facial abuse\", tweet)\n",
    "    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\n",
    "    tweet = re.sub(r\"BeingAuthor\", \"Being Author\", tweet)\n",
    "    tweet = re.sub(r\"withheavenly\", \"with heavenly\", tweet)\n",
    "    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\n",
    "    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\n",
    "    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\n",
    "    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\n",
    "    tweet = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", tweet)\n",
    "    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\n",
    "    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\n",
    "    tweet = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", tweet)\n",
    "    tweet = re.sub(r\"aRmageddon\", \"armageddon\", tweet)\n",
    "    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\n",
    "    tweet = re.sub(r\"GodsLove\", \"God's Love\", tweet)\n",
    "    tweet = re.sub(r\"bookboost\", \"book boost\", tweet)\n",
    "    tweet = re.sub(r\"ibooklove\", \"I book love\", tweet)\n",
    "    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\n",
    "    tweet = re.sub(r\"realDonaldTrump\", \"Donald Trump\", tweet)\n",
    "    tweet = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", tweet)\n",
    "    tweet = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", tweet)\n",
    "    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\n",
    "    tweet = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", tweet)\n",
    "    tweet = re.sub(r\"Hostage&2\", \"Hostage & 2\", tweet)\n",
    "    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\n",
    "    tweet = re.sub(r\"RickPerry\", \"Rick Perry\", tweet)\n",
    "    tweet = re.sub(r\"frontpage\", \"front page\", tweet)\n",
    "    tweet = re.sub(r\"NewsInTweets\", \"News In Tweets\", tweet)\n",
    "    tweet = re.sub(r\"ViralSpell\", \"Viral Spell\", tweet)\n",
    "    tweet = re.sub(r\"til_now\", \"until now\", tweet)\n",
    "    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\n",
    "    tweet = re.sub(r\"ZippedNews\", \"Zipped News\", tweet)\n",
    "    tweet = re.sub(r\"MicheleBachman\", \"Michele Bachman\", tweet)\n",
    "    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\n",
    "    tweet = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", tweet)\n",
    "    tweet = re.sub(r\"abstorm\", \"Alberta Storm\", tweet)\n",
    "    tweet = re.sub(r\"Beyhive\", \"Beyonce hive\", tweet)\n",
    "    tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\n",
    "    tweet = re.sub(r\"DETECTADO\", \"Detected\", tweet)\n",
    "    tweet = re.sub(r\"RockyFire\", \"Rocky Fire\", tweet)\n",
    "    tweet = re.sub(r\"Listen/Buy\", \"Listen / Buy\", tweet)\n",
    "    tweet = re.sub(r\"NickCannon\", \"Nick Cannon\", tweet)\n",
    "    tweet = re.sub(r\"FaroeIslands\", \"Faroe Islands\", tweet)\n",
    "    tweet = re.sub(r\"yycstorm\", \"Calgary Storm\", tweet)\n",
    "    tweet = re.sub(r\"IDPs:\", \"Internally Displaced People :\", tweet)\n",
    "    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n",
    "    tweet = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", tweet)\n",
    "    tweet = re.sub(r\"jimmyfallon\", \"jimmy fallon\", tweet)\n",
    "    tweet = re.sub(r\"justinbieber\", \"justin bieber\", tweet)  \n",
    "    tweet = re.sub(r\"UTC2015\", \"UTC 2015\", tweet)\n",
    "    tweet = re.sub(r\"Time2015\", \"Time 2015\", tweet)\n",
    "    tweet = re.sub(r\"djicemoon\", \"dj icemoon\", tweet)\n",
    "    tweet = re.sub(r\"LivingSafely\", \"Living Safely\", tweet)\n",
    "    tweet = re.sub(r\"FIFA16\", \"Fifa 2016\", tweet)\n",
    "    tweet = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", tweet)\n",
    "    tweet = re.sub(r\"bbcnews\", \"bbc news\", tweet)\n",
    "    tweet = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", tweet)\n",
    "    tweet = re.sub(r\"c4news\", \"c4 news\", tweet)\n",
    "    tweet = re.sub(r\"OBLITERATION\", \"obliteration\", tweet)\n",
    "    tweet = re.sub(r\"MUDSLIDE\", \"mudslide\", tweet)\n",
    "    tweet = re.sub(r\"NoSurrender\", \"No Surrender\", tweet)\n",
    "    tweet = re.sub(r\"NotExplained\", \"Not Explained\", tweet)\n",
    "    tweet = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", tweet)\n",
    "    tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\n",
    "    tweet = re.sub(r\"KOTAWeather\", \"KOTA Weather\", tweet)\n",
    "    tweet = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", tweet)\n",
    "    tweet = re.sub(r\"KOIN6News\", \"KOIN 6 News\", tweet)\n",
    "    tweet = re.sub(r\"LiveOnK2\", \"Live On K2\", tweet)\n",
    "    tweet = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", tweet)\n",
    "    tweet = re.sub(r\"nikeplus\", \"nike plus\", tweet)\n",
    "    tweet = re.sub(r\"david_cameron\", \"David Cameron\", tweet)\n",
    "    tweet = re.sub(r\"peterjukes\", \"Peter Jukes\", tweet)\n",
    "    tweet = re.sub(r\"JamesMelville\", \"James Melville\", tweet)\n",
    "    tweet = re.sub(r\"megynkelly\", \"Megyn Kelly\", tweet)\n",
    "    tweet = re.sub(r\"cnewslive\", \"C News Live\", tweet)\n",
    "    tweet = re.sub(r\"JamaicaObserver\", \"Jamaica Observer\", tweet)\n",
    "    tweet = re.sub(r\"TweetLikeItsSeptember11th2001\", \"Tweet like it is september 11th 2001\", tweet)\n",
    "    tweet = re.sub(r\"cbplawyers\", \"cbp lawyers\", tweet)\n",
    "    tweet = re.sub(r\"fewmoretweets\", \"few more tweets\", tweet)\n",
    "    tweet = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", tweet)\n",
    "    tweet = re.sub(r\"cjoyner\", \"Chris Joyner\", tweet)\n",
    "    tweet = re.sub(r\"ENGvAUS\", \"England vs Australia\", tweet)\n",
    "    tweet = re.sub(r\"ScottWalker\", \"Scott Walker\", tweet)\n",
    "    tweet = re.sub(r\"MikeParrActor\", \"Michael Parr\", tweet)\n",
    "    tweet = re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\", tweet)\n",
    "    tweet = re.sub(r\"TGF2015\", \"Tontitown Grape Festival\", tweet)\n",
    "    tweet = re.sub(r\"realmandyrain\", \"Mandy Rain\", tweet)\n",
    "    tweet = re.sub(r\"GraysonDolan\", \"Grayson Dolan\", tweet)\n",
    "    tweet = re.sub(r\"ApolloBrown\", \"Apollo Brown\", tweet)\n",
    "    tweet = re.sub(r\"saddlebrooke\", \"Saddlebrooke\", tweet)\n",
    "    tweet = re.sub(r\"TontitownGrape\", \"Tontitown Grape\", tweet)\n",
    "    tweet = re.sub(r\"AbbsWinston\", \"Abbs Winston\", tweet)\n",
    "    tweet = re.sub(r\"ShaunKing\", \"Shaun King\", tweet)\n",
    "    tweet = re.sub(r\"MeekMill\", \"Meek Mill\", tweet)\n",
    "    tweet = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", tweet)\n",
    "    tweet = re.sub(r\"GRupdates\", \"GR updates\", tweet)\n",
    "    tweet = re.sub(r\"SouthDowns\", \"South Downs\", tweet)\n",
    "    tweet = re.sub(r\"braininjury\", \"brain injury\", tweet)\n",
    "    tweet = re.sub(r\"auspol\", \"Australian politics\", tweet)\n",
    "    tweet = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", tweet)\n",
    "    tweet = re.sub(r\"calgaryweather\", \"Calgary Weather\", tweet)\n",
    "    tweet = re.sub(r\"weallheartonedirection\", \"we all heart one direction\", tweet)\n",
    "    tweet = re.sub(r\"edsheeran\", \"Ed Sheeran\", tweet)\n",
    "    tweet = re.sub(r\"TrueHeroes\", \"True Heroes\", tweet)\n",
    "    tweet = re.sub(r\"S3XLEAK\", \"sex leak\", tweet)\n",
    "    tweet = re.sub(r\"ComplexMag\", \"Complex Magazine\", tweet)\n",
    "    tweet = re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\", tweet)\n",
    "    tweet = re.sub(r\"CityofCalgary\", \"City of Calgary\", tweet)\n",
    "    tweet = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", tweet)\n",
    "    tweet = re.sub(r\"SummerFate\", \"Summer Fate\", tweet)\n",
    "    tweet = re.sub(r\"RAmag\", \"Royal Academy Magazine\", tweet)\n",
    "    tweet = re.sub(r\"offers2go\", \"offers to go\", tweet)\n",
    "    tweet = re.sub(r\"foodscare\", \"food scare\", tweet)\n",
    "    tweet = re.sub(r\"MNPDNashville\", \"Metropolitan Nashville Police Department\", tweet)\n",
    "    tweet = re.sub(r\"TfLBusAlerts\", \"TfL Bus Alerts\", tweet)\n",
    "    tweet = re.sub(r\"GamerGate\", \"Gamer Gate\", tweet)\n",
    "    tweet = re.sub(r\"IHHen\", \"Humanitarian Relief\", tweet)\n",
    "    tweet = re.sub(r\"spinningbot\", \"spinning bot\", tweet)\n",
    "    tweet = re.sub(r\"ModiMinistry\", \"Modi Ministry\", tweet)\n",
    "    tweet = re.sub(r\"TAXIWAYS\", \"taxi ways\", tweet)\n",
    "    tweet = re.sub(r\"Calum5SOS\", \"Calum Hood\", tweet)\n",
    "    tweet = re.sub(r\"po_st\", \"po.st\", tweet)\n",
    "    tweet = re.sub(r\"scoopit\", \"scoop.it\", tweet)\n",
    "    tweet = re.sub(r\"UltimaLucha\", \"Ultima Lucha\", tweet)\n",
    "    tweet = re.sub(r\"JonathanFerrell\", \"Jonathan Ferrell\", tweet)\n",
    "    tweet = re.sub(r\"aria_ahrary\", \"Aria Ahrary\", tweet)\n",
    "    tweet = re.sub(r\"rapidcity\", \"Rapid City\", tweet)\n",
    "    tweet = re.sub(r\"OutBid\", \"outbid\", tweet)\n",
    "    tweet = re.sub(r\"lavenderpoetrycafe\", \"lavender poetry cafe\", tweet)\n",
    "    tweet = re.sub(r\"EudryLantiqua\", \"Eudry Lantiqua\", tweet)\n",
    "    tweet = re.sub(r\"15PM\", \"15 PM\", tweet)\n",
    "    tweet = re.sub(r\"OriginalFunko\", \"Funko\", tweet)\n",
    "    tweet = re.sub(r\"rightwaystan\", \"Richard Tan\", tweet)\n",
    "    tweet = re.sub(r\"CindyNoonan\", \"Cindy Noonan\", tweet)\n",
    "    tweet = re.sub(r\"RT_America\", \"RT America\", tweet)\n",
    "    tweet = re.sub(r\"narendramodi\", \"Narendra Modi\", tweet)\n",
    "    tweet = re.sub(r\"BakeOffFriends\", \"Bake Off Friends\", tweet)\n",
    "    tweet = re.sub(r\"TeamHendrick\", \"Hendrick Motorsports\", tweet)\n",
    "    tweet = re.sub(r\"alexbelloli\", \"Alex Belloli\", tweet)\n",
    "    tweet = re.sub(r\"itsjustinstuart\", \"Justin Stuart\", tweet)\n",
    "    tweet = re.sub(r\"gunsense\", \"gun sense\", tweet)\n",
    "    tweet = re.sub(r\"DebateQuestionsWeWantToHear\", \"debate questions we want to hear\", tweet)\n",
    "    tweet = re.sub(r\"RoyalCarribean\", \"Royal Carribean\", tweet)\n",
    "    tweet = re.sub(r\"samanthaturne19\", \"Samantha Turner\", tweet)\n",
    "    tweet = re.sub(r\"JonVoyage\", \"Jon Stewart\", tweet)\n",
    "    tweet = re.sub(r\"renew911health\", \"renew 911 health\", tweet)\n",
    "    tweet = re.sub(r\"SuryaRay\", \"Surya Ray\", tweet)\n",
    "    tweet = re.sub(r\"pattonoswalt\", \"Patton Oswalt\", tweet)\n",
    "    tweet = re.sub(r\"minhazmerchant\", \"Minhaz Merchant\", tweet)\n",
    "    tweet = re.sub(r\"TLVFaces\", \"Israel Diaspora Coalition\", tweet)\n",
    "    tweet = re.sub(r\"pmarca\", \"Marc Andreessen\", tweet)\n",
    "    tweet = re.sub(r\"pdx911\", \"Portland Police\", tweet)\n",
    "    tweet = re.sub(r\"jamaicaplain\", \"Jamaica Plain\", tweet)\n",
    "    tweet = re.sub(r\"Japton\", \"Arkansas\", tweet)\n",
    "    tweet = re.sub(r\"RouteComplex\", \"Route Complex\", tweet)\n",
    "    tweet = re.sub(r\"INSubcontinent\", \"Indian Subcontinent\", tweet)\n",
    "    tweet = re.sub(r\"NJTurnpike\", \"New Jersey Turnpike\", tweet)\n",
    "    tweet = re.sub(r\"Politifiact\", \"PolitiFact\", tweet)\n",
    "    tweet = re.sub(r\"Hiroshima70\", \"Hiroshima\", tweet)\n",
    "    tweet = re.sub(r\"GMMBC\", \"Greater Mt Moriah Baptist Church\", tweet)\n",
    "    tweet = re.sub(r\"versethe\", \"verse the\", tweet)\n",
    "    tweet = re.sub(r\"TubeStrike\", \"Tube Strike\", tweet)\n",
    "    tweet = re.sub(r\"MissionHills\", \"Mission Hills\", tweet)\n",
    "    tweet = re.sub(r\"ProtectDenaliWolves\", \"Protect Denali Wolves\", tweet)\n",
    "    tweet = re.sub(r\"NANKANA\", \"Nankana\", tweet)\n",
    "    tweet = re.sub(r\"SAHIB\", \"Sahib\", tweet)\n",
    "    tweet = re.sub(r\"PAKPATTAN\", \"Pakpattan\", tweet)\n",
    "    tweet = re.sub(r\"Newz_Sacramento\", \"News Sacramento\", tweet)\n",
    "    tweet = re.sub(r\"gofundme\", \"go fund me\", tweet)\n",
    "    tweet = re.sub(r\"pmharper\", \"Stephen Harper\", tweet)\n",
    "    tweet = re.sub(r\"IvanBerroa\", \"Ivan Berroa\", tweet)\n",
    "    tweet = re.sub(r\"LosDelSonido\", \"Los Del Sonido\", tweet)\n",
    "    tweet = re.sub(r\"bancodeseries\", \"banco de series\", tweet)\n",
    "    tweet = re.sub(r\"timkaine\", \"Tim Kaine\", tweet)\n",
    "    tweet = re.sub(r\"IdentityTheft\", \"Identity Theft\", tweet)\n",
    "    tweet = re.sub(r\"AllLivesMatter\", \"All Lives Matter\", tweet)\n",
    "    tweet = re.sub(r\"mishacollins\", \"Misha Collins\", tweet)\n",
    "    tweet = re.sub(r\"BillNeelyNBC\", \"Bill Neely\", tweet)\n",
    "    tweet = re.sub(r\"BeClearOnCancer\", \"be clear on cancer\", tweet)\n",
    "    tweet = re.sub(r\"Kowing\", \"Knowing\", tweet)\n",
    "    tweet = re.sub(r\"ScreamQueens\", \"Scream Queens\", tweet)\n",
    "    tweet = re.sub(r\"AskCharley\", \"Ask Charley\", tweet)\n",
    "    tweet = re.sub(r\"BlizzHeroes\", \"Heroes of the Storm\", tweet)\n",
    "    tweet = re.sub(r\"BradleyBrad47\", \"Bradley Brad\", tweet)\n",
    "    tweet = re.sub(r\"HannaPH\", \"Typhoon Hanna\", tweet)\n",
    "    tweet = re.sub(r\"meinlcymbals\", \"MEINL Cymbals\", tweet)\n",
    "    tweet = re.sub(r\"Ptbo\", \"Peterborough\", tweet)\n",
    "    tweet = re.sub(r\"cnnbrk\", \"CNN Breaking News\", tweet)\n",
    "    tweet = re.sub(r\"IndianNews\", \"Indian News\", tweet)\n",
    "    tweet = re.sub(r\"savebees\", \"save bees\", tweet)\n",
    "    tweet = re.sub(r\"GreenHarvard\", \"Green Harvard\", tweet)\n",
    "    tweet = re.sub(r\"StandwithPP\", \"Stand with planned parenthood\", tweet)\n",
    "    tweet = re.sub(r\"hermancranston\", \"Herman Cranston\", tweet)\n",
    "    tweet = re.sub(r\"WMUR9\", \"WMUR-TV\", tweet)\n",
    "    tweet = re.sub(r\"RockBottomRadFM\", \"Rock Bottom Radio\", tweet)\n",
    "    tweet = re.sub(r\"ameenshaikh3\", \"Ameen Shaikh\", tweet)\n",
    "    tweet = re.sub(r\"ProSyn\", \"Project Syndicate\", tweet)\n",
    "    tweet = re.sub(r\"Daesh\", \"ISIS\", tweet)\n",
    "    tweet = re.sub(r\"s2g\", \"swear to god\", tweet)\n",
    "    tweet = re.sub(r\"listenlive\", \"listen live\", tweet)\n",
    "    tweet = re.sub(r\"CDCgov\", \"Centers for Disease Control and Prevention\", tweet)\n",
    "    tweet = re.sub(r\"FoxNew\", \"Fox News\", tweet)\n",
    "    tweet = re.sub(r\"CBSBigBrother\", \"Big Brother\", tweet)\n",
    "    tweet = re.sub(r\"JulieDiCaro\", \"Julie DiCaro\", tweet)\n",
    "    tweet = re.sub(r\"theadvocatemag\", \"The Advocate Magazine\", tweet)\n",
    "    tweet = re.sub(r\"RohnertParkDPS\", \"Rohnert Park Police Department\", tweet)\n",
    "    tweet = re.sub(r\"THISIZBWRIGHT\", \"Bonnie Wright\", tweet)\n",
    "    tweet = re.sub(r\"Popularmmos\", \"Popular MMOs\", tweet)\n",
    "    tweet = re.sub(r\"WildHorses\", \"Wild Horses\", tweet)\n",
    "    tweet = re.sub(r\"FantasticFour\", \"Fantastic Four\", tweet)\n",
    "    tweet = re.sub(r\"HORNDALE\", \"Horndale\", tweet)\n",
    "    tweet = re.sub(r\"PINER\", \"Piner\", tweet)\n",
    "    tweet = re.sub(r\"BathAndNorthEastSomerset\", \"Bath and North East Somerset\", tweet)\n",
    "    tweet = re.sub(r\"thatswhatfriendsarefor\", \"that is what friends are for\", tweet)\n",
    "    tweet = re.sub(r\"residualincome\", \"residual income\", tweet)\n",
    "    tweet = re.sub(r\"YahooNewsDigest\", \"Yahoo News Digest\", tweet)\n",
    "    tweet = re.sub(r\"MalaysiaAirlines\", \"Malaysia Airlines\", tweet)\n",
    "    tweet = re.sub(r\"AmazonDeals\", \"Amazon Deals\", tweet)\n",
    "    tweet = re.sub(r\"MissCharleyWebb\", \"Charley Webb\", tweet)\n",
    "    tweet = re.sub(r\"shoalstraffic\", \"shoals traffic\", tweet)\n",
    "    tweet = re.sub(r\"GeorgeFoster72\", \"George Foster\", tweet)\n",
    "    tweet = re.sub(r\"pop2015\", \"pop 2015\", tweet)\n",
    "    tweet = re.sub(r\"_PokemonCards_\", \"Pokemon Cards\", tweet)\n",
    "    tweet = re.sub(r\"DianneG\", \"Dianne Gallagher\", tweet)\n",
    "    tweet = re.sub(r\"KashmirConflict\", \"Kashmir Conflict\", tweet)\n",
    "    tweet = re.sub(r\"BritishBakeOff\", \"British Bake Off\", tweet)\n",
    "    tweet = re.sub(r\"FreeKashmir\", \"Free Kashmir\", tweet)\n",
    "    tweet = re.sub(r\"mattmosley\", \"Matt Mosley\", tweet)\n",
    "    tweet = re.sub(r\"BishopFred\", \"Bishop Fred\", tweet)\n",
    "    tweet = re.sub(r\"EndConflict\", \"End Conflict\", tweet)\n",
    "    tweet = re.sub(r\"EndOccupation\", \"End Occupation\", tweet)\n",
    "    tweet = re.sub(r\"UNHEALED\", \"unhealed\", tweet)\n",
    "    tweet = re.sub(r\"CharlesDagnall\", \"Charles Dagnall\", tweet)\n",
    "    tweet = re.sub(r\"Latestnews\", \"Latest news\", tweet)\n",
    "    tweet = re.sub(r\"KindleCountdown\", \"Kindle Countdown\", tweet)\n",
    "    tweet = re.sub(r\"NoMoreHandouts\", \"No More Handouts\", tweet)\n",
    "    tweet = re.sub(r\"datingtips\", \"dating tips\", tweet)\n",
    "    tweet = re.sub(r\"charlesadler\", \"Charles Adler\", tweet)\n",
    "    tweet = re.sub(r\"twia\", \"Texas Windstorm Insurance Association\", tweet)\n",
    "    tweet = re.sub(r\"txlege\", \"Texas Legislature\", tweet)\n",
    "    tweet = re.sub(r\"WindstormInsurer\", \"Windstorm Insurer\", tweet)\n",
    "    tweet = re.sub(r\"Newss\", \"News\", tweet)\n",
    "    tweet = re.sub(r\"hempoil\", \"hemp oil\", tweet)\n",
    "    tweet = re.sub(r\"CommoditiesAre\", \"Commodities are\", tweet)\n",
    "    tweet = re.sub(r\"tubestrike\", \"tube strike\", tweet)\n",
    "    tweet = re.sub(r\"JoeNBC\", \"Joe Scarborough\", tweet)\n",
    "    tweet = re.sub(r\"LiteraryCakes\", \"Literary Cakes\", tweet)\n",
    "    tweet = re.sub(r\"TI5\", \"The International 5\", tweet)\n",
    "    tweet = re.sub(r\"thehill\", \"the hill\", tweet)\n",
    "    tweet = re.sub(r\"3others\", \"3 others\", tweet)\n",
    "    tweet = re.sub(r\"stighefootball\", \"Sam Tighe\", tweet)\n",
    "    tweet = re.sub(r\"whatstheimportantvideo\", \"what is the important video\", tweet)\n",
    "    tweet = re.sub(r\"ClaudioMeloni\", \"Claudio Meloni\", tweet)\n",
    "    tweet = re.sub(r\"DukeSkywalker\", \"Duke Skywalker\", tweet)\n",
    "    tweet = re.sub(r\"carsonmwr\", \"Fort Carson\", tweet)\n",
    "    tweet = re.sub(r\"offdishduty\", \"off dish duty\", tweet)\n",
    "    tweet = re.sub(r\"andword\", \"and word\", tweet)\n",
    "    tweet = re.sub(r\"rhodeisland\", \"Rhode Island\", tweet)\n",
    "    tweet = re.sub(r\"easternoregon\", \"Eastern Oregon\", tweet)\n",
    "    tweet = re.sub(r\"WAwildfire\", \"Washington Wildfire\", tweet)\n",
    "    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n",
    "    tweet = re.sub(r\"57am\", \"57 am\", tweet)\n",
    "    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n",
    "    tweet = re.sub(r\"JacobHoggard\", \"Jacob Hoggard\", tweet)\n",
    "    tweet = re.sub(r\"newnewnew\", \"new new new\", tweet)\n",
    "    tweet = re.sub(r\"under50\", \"under 50\", tweet)\n",
    "    tweet = re.sub(r\"getitbeforeitsgone\", \"get it before it is gone\", tweet)\n",
    "    tweet = re.sub(r\"freshoutofthebox\", \"fresh out of the box\", tweet)\n",
    "    tweet = re.sub(r\"amwriting\", \"am writing\", tweet)\n",
    "    tweet = re.sub(r\"Bokoharm\", \"Boko Haram\", tweet)\n",
    "    tweet = re.sub(r\"Nowlike\", \"Now like\", tweet)\n",
    "    tweet = re.sub(r\"seasonfrom\", \"season from\", tweet)\n",
    "    tweet = re.sub(r\"epicente\", \"epicenter\", tweet)\n",
    "    tweet = re.sub(r\"epicenterr\", \"epicenter\", tweet)\n",
    "    tweet = re.sub(r\"sicklife\", \"sick life\", tweet)\n",
    "    tweet = re.sub(r\"yycweather\", \"Calgary Weather\", tweet)\n",
    "    tweet = re.sub(r\"calgarysun\", \"Calgary Sun\", tweet)\n",
    "    tweet = re.sub(r\"approachng\", \"approaching\", tweet)\n",
    "    tweet = re.sub(r\"evng\", \"evening\", tweet)\n",
    "    tweet = re.sub(r\"Sumthng\", \"something\", tweet)\n",
    "    tweet = re.sub(r\"EllenPompeo\", \"Ellen Pompeo\", tweet)\n",
    "    tweet = re.sub(r\"shondarhimes\", \"Shonda Rhimes\", tweet)\n",
    "    tweet = re.sub(r\"ABCNetwork\", \"ABC Network\", tweet)\n",
    "    tweet = re.sub(r\"SushmaSwaraj\", \"Sushma Swaraj\", tweet)\n",
    "    tweet = re.sub(r\"pray4japan\", \"Pray for Japan\", tweet)\n",
    "    tweet = re.sub(r\"hope4japan\", \"Hope for Japan\", tweet)\n",
    "    tweet = re.sub(r\"Illusionimagess\", \"Illusion images\", tweet)\n",
    "    tweet = re.sub(r\"SummerUnderTheStars\", \"Summer Under The Stars\", tweet)\n",
    "    tweet = re.sub(r\"ShallWeDance\", \"Shall We Dance\", tweet)\n",
    "    tweet = re.sub(r\"TCMParty\", \"TCM Party\", tweet)\n",
    "    tweet = re.sub(r\"marijuananews\", \"marijuana news\", tweet)\n",
    "    tweet = re.sub(r\"onbeingwithKristaTippett\", \"on being with Krista Tippett\", tweet)\n",
    "    tweet = re.sub(r\"Beingtweets\", \"Being tweets\", tweet)\n",
    "    tweet = re.sub(r\"newauthors\", \"new authors\", tweet)\n",
    "    tweet = re.sub(r\"remedyyyy\", \"remedy\", tweet)\n",
    "    tweet = re.sub(r\"44PM\", \"44 PM\", tweet)\n",
    "    tweet = re.sub(r\"HeadlinesApp\", \"Headlines App\", tweet)\n",
    "    tweet = re.sub(r\"40PM\", \"40 PM\", tweet)\n",
    "    tweet = re.sub(r\"myswc\", \"Severe Weather Center\", tweet)\n",
    "    tweet = re.sub(r\"ithats\", \"that is\", tweet)\n",
    "    tweet = re.sub(r\"icouldsitinthismomentforever\", \"I could sit in this moment forever\", tweet)\n",
    "    tweet = re.sub(r\"FatLoss\", \"Fat Loss\", tweet)\n",
    "    tweet = re.sub(r\"02PM\", \"02 PM\", tweet)\n",
    "    tweet = re.sub(r\"MetroFmTalk\", \"Metro Fm Talk\", tweet)\n",
    "    tweet = re.sub(r\"Bstrd\", \"bastard\", tweet)\n",
    "    tweet = re.sub(r\"bldy\", \"bloody\", tweet)\n",
    "    tweet = re.sub(r\"MetrofmTalk\", \"Metro Fm Talk\", tweet)\n",
    "    tweet = re.sub(r\"terrorismturn\", \"terrorism turn\", tweet)\n",
    "    tweet = re.sub(r\"BBCNewsAsia\", \"BBC News Asia\", tweet)\n",
    "    tweet = re.sub(r\"BehindTheScenes\", \"Behind The Scenes\", tweet)\n",
    "    tweet = re.sub(r\"GeorgeTakei\", \"George Takei\", tweet)\n",
    "    tweet = re.sub(r\"WomensWeeklyMag\", \"Womens Weekly Magazine\", tweet)\n",
    "    tweet = re.sub(r\"SurvivorsGuidetoEarth\", \"Survivors Guide to Earth\", tweet)\n",
    "    tweet = re.sub(r\"incubusband\", \"incubus band\", tweet)\n",
    "    tweet = re.sub(r\"Babypicturethis\", \"Baby picture this\", tweet)\n",
    "    tweet = re.sub(r\"BombEffects\", \"Bomb Effects\", tweet)\n",
    "    tweet = re.sub(r\"win10\", \"Windows 10\", tweet)\n",
    "    tweet = re.sub(r\"idkidk\", \"I do not know I do not know\", tweet)\n",
    "    tweet = re.sub(r\"TheWalkingDead\", \"The Walking Dead\", tweet)\n",
    "    tweet = re.sub(r\"amyschumer\", \"Amy Schumer\", tweet)\n",
    "    tweet = re.sub(r\"crewlist\", \"crew list\", tweet)\n",
    "    tweet = re.sub(r\"Erdogans\", \"Erdogan\", tweet)\n",
    "    tweet = re.sub(r\"BBCLive\", \"BBC Live\", tweet)\n",
    "    tweet = re.sub(r\"TonyAbbottMHR\", \"Tony Abbott\", tweet)\n",
    "    tweet = re.sub(r\"paulmyerscough\", \"Paul Myerscough\", tweet)\n",
    "    tweet = re.sub(r\"georgegallagher\", \"George Gallagher\", tweet)\n",
    "    tweet = re.sub(r\"JimmieJohnson\", \"Jimmie Johnson\", tweet)\n",
    "    tweet = re.sub(r\"pctool\", \"pc tool\", tweet)\n",
    "    tweet = re.sub(r\"DoingHashtagsRight\", \"Doing Hashtags Right\", tweet)\n",
    "    tweet = re.sub(r\"ThrowbackThursday\", \"Throwback Thursday\", tweet)\n",
    "    tweet = re.sub(r\"SnowBackSunday\", \"Snowback Sunday\", tweet)\n",
    "    tweet = re.sub(r\"LakeEffect\", \"Lake Effect\", tweet)\n",
    "    tweet = re.sub(r\"RTphotographyUK\", \"Richard Thomas Photography UK\", tweet)\n",
    "    tweet = re.sub(r\"BigBang_CBS\", \"Big Bang CBS\", tweet)\n",
    "    tweet = re.sub(r\"writerslife\", \"writers life\", tweet)\n",
    "    tweet = re.sub(r\"NaturalBirth\", \"Natural Birth\", tweet)\n",
    "    tweet = re.sub(r\"UnusualWords\", \"Unusual Words\", tweet)\n",
    "    tweet = re.sub(r\"wizkhalifa\", \"Wiz Khalifa\", tweet)\n",
    "    tweet = re.sub(r\"acreativedc\", \"a creative DC\", tweet)\n",
    "    tweet = re.sub(r\"vscodc\", \"vsco DC\", tweet)\n",
    "    tweet = re.sub(r\"VSCOcam\", \"vsco camera\", tweet)\n",
    "    tweet = re.sub(r\"TheBEACHDC\", \"The beach DC\", tweet)\n",
    "    tweet = re.sub(r\"buildingmuseum\", \"building museum\", tweet)\n",
    "    tweet = re.sub(r\"WorldOil\", \"World Oil\", tweet)\n",
    "    tweet = re.sub(r\"redwedding\", \"red wedding\", tweet)\n",
    "    tweet = re.sub(r\"AmazingRaceCanada\", \"Amazing Race Canada\", tweet)\n",
    "    tweet = re.sub(r\"WakeUpAmerica\", \"Wake Up America\", tweet)\n",
    "    tweet = re.sub(r\"\\\\Allahuakbar\\\\\", \"Allahu Akbar\", tweet)\n",
    "    tweet = re.sub(r\"bleased\", \"blessed\", tweet)\n",
    "    tweet = re.sub(r\"nigeriantribune\", \"Nigerian Tribune\", tweet)\n",
    "    tweet = re.sub(r\"HIDEO_KOJIMA_EN\", \"Hideo Kojima\", tweet)\n",
    "    tweet = re.sub(r\"FusionFestival\", \"Fusion Festival\", tweet)\n",
    "    tweet = re.sub(r\"50Mixed\", \"50 Mixed\", tweet)\n",
    "    tweet = re.sub(r\"NoAgenda\", \"No Agenda\", tweet)\n",
    "    tweet = re.sub(r\"WhiteGenocide\", \"White Genocide\", tweet)\n",
    "    tweet = re.sub(r\"dirtylying\", \"dirty lying\", tweet)\n",
    "    tweet = re.sub(r\"SyrianRefugees\", \"Syrian Refugees\", tweet)\n",
    "    tweet = re.sub(r\"changetheworld\", \"change the world\", tweet)\n",
    "    tweet = re.sub(r\"Ebolacase\", \"Ebola case\", tweet)\n",
    "    tweet = re.sub(r\"mcgtech\", \"mcg technologies\", tweet)\n",
    "    tweet = re.sub(r\"withweapons\", \"with weapons\", tweet)\n",
    "    tweet = re.sub(r\"advancedwarfare\", \"advanced warfare\", tweet)\n",
    "    tweet = re.sub(r\"letsFootball\", \"let us Football\", tweet)\n",
    "    tweet = re.sub(r\"LateNiteMix\", \"late night mix\", tweet)\n",
    "    tweet = re.sub(r\"PhilCollinsFeed\", \"Phil Collins\", tweet)\n",
    "    tweet = re.sub(r\"RudyHavenstein\", \"Rudy Havenstein\", tweet)\n",
    "    tweet = re.sub(r\"22PM\", \"22 PM\", tweet)\n",
    "    tweet = re.sub(r\"54am\", \"54 AM\", tweet)\n",
    "    tweet = re.sub(r\"38am\", \"38 AM\", tweet)\n",
    "    tweet = re.sub(r\"OldFolkExplainStuff\", \"Old Folk Explain Stuff\", tweet)\n",
    "    tweet = re.sub(r\"BlacklivesMatter\", \"Black Lives Matter\", tweet)\n",
    "    tweet = re.sub(r\"InsaneLimits\", \"Insane Limits\", tweet)\n",
    "    tweet = re.sub(r\"youcantsitwithus\", \"you cannot sit with us\", tweet)\n",
    "    tweet = re.sub(r\"2k15\", \"2015\", tweet)\n",
    "    tweet = re.sub(r\"TheIran\", \"Iran\", tweet)\n",
    "    tweet = re.sub(r\"JimmyFallon\", \"Jimmy Fallon\", tweet)\n",
    "    tweet = re.sub(r\"AlbertBrooks\", \"Albert Brooks\", tweet)\n",
    "    tweet = re.sub(r\"defense_news\", \"defense news\", tweet)\n",
    "    tweet = re.sub(r\"nuclearrcSA\", \"Nuclear Risk Control Self Assessment\", tweet)\n",
    "    tweet = re.sub(r\"Auspol\", \"Australia Politics\", tweet)\n",
    "    tweet = re.sub(r\"NuclearPower\", \"Nuclear Power\", tweet)\n",
    "    tweet = re.sub(r\"WhiteTerrorism\", \"White Terrorism\", tweet)\n",
    "    tweet = re.sub(r\"truthfrequencyradio\", \"Truth Frequency Radio\", tweet)\n",
    "    tweet = re.sub(r\"ErasureIsNotEquality\", \"Erasure is not equality\", tweet)\n",
    "    tweet = re.sub(r\"ProBonoNews\", \"Pro Bono News\", tweet)\n",
    "    tweet = re.sub(r\"JakartaPost\", \"Jakarta Post\", tweet)\n",
    "    tweet = re.sub(r\"toopainful\", \"too painful\", tweet)\n",
    "    tweet = re.sub(r\"melindahaunton\", \"Melinda Haunton\", tweet)\n",
    "    tweet = re.sub(r\"NoNukes\", \"No Nukes\", tweet)\n",
    "    tweet = re.sub(r\"curryspcworld\", \"Currys PC World\", tweet)\n",
    "    tweet = re.sub(r\"ineedcake\", \"I need cake\", tweet)\n",
    "    tweet = re.sub(r\"blackforestgateau\", \"black forest gateau\", tweet)\n",
    "    tweet = re.sub(r\"BBCOne\", \"BBC One\", tweet)\n",
    "    tweet = re.sub(r\"AlexxPage\", \"Alex Page\", tweet)\n",
    "    tweet = re.sub(r\"jonathanserrie\", \"Jonathan Serrie\", tweet)\n",
    "    tweet = re.sub(r\"SocialJerkBlog\", \"Social Jerk Blog\", tweet)\n",
    "    tweet = re.sub(r\"ChelseaVPeretti\", \"Chelsea Peretti\", tweet)\n",
    "    tweet = re.sub(r\"irongiant\", \"iron giant\", tweet)\n",
    "    tweet = re.sub(r\"RonFunches\", \"Ron Funches\", tweet)\n",
    "    tweet = re.sub(r\"TimCook\", \"Tim Cook\", tweet)\n",
    "    tweet = re.sub(r\"sebastianstanisaliveandwell\", \"Sebastian Stan is alive and well\", tweet)\n",
    "    tweet = re.sub(r\"Madsummer\", \"Mad summer\", tweet)\n",
    "    tweet = re.sub(r\"NowYouKnow\", \"Now you know\", tweet)\n",
    "    tweet = re.sub(r\"concertphotography\", \"concert photography\", tweet)\n",
    "    tweet = re.sub(r\"TomLandry\", \"Tom Landry\", tweet)\n",
    "    tweet = re.sub(r\"showgirldayoff\", \"show girl day off\", tweet)\n",
    "    tweet = re.sub(r\"Yougslavia\", \"Yugoslavia\", tweet)\n",
    "    tweet = re.sub(r\"QuantumDataInformatics\", \"Quantum Data Informatics\", tweet)\n",
    "    tweet = re.sub(r\"FromTheDesk\", \"From The Desk\", tweet)\n",
    "    tweet = re.sub(r\"TheaterTrial\", \"Theater Trial\", tweet)\n",
    "    tweet = re.sub(r\"CatoInstitute\", \"Cato Institute\", tweet)\n",
    "    tweet = re.sub(r\"EmekaGift\", \"Emeka Gift\", tweet)\n",
    "    tweet = re.sub(r\"LetsBe_Rational\", \"Let us be rational\", tweet)\n",
    "    tweet = re.sub(r\"Cynicalreality\", \"Cynical reality\", tweet)\n",
    "    tweet = re.sub(r\"FredOlsenCruise\", \"Fred Olsen Cruise\", tweet)\n",
    "    tweet = re.sub(r\"NotSorry\", \"not sorry\", tweet)\n",
    "    tweet = re.sub(r\"UseYourWords\", \"use your words\", tweet)\n",
    "    tweet = re.sub(r\"WordoftheDay\", \"word of the day\", tweet)\n",
    "    tweet = re.sub(r\"Dictionarycom\", \"Dictionary.com\", tweet)\n",
    "    tweet = re.sub(r\"TheBrooklynLife\", \"The Brooklyn Life\", tweet)\n",
    "    tweet = re.sub(r\"jokethey\", \"joke they\", tweet)\n",
    "    tweet = re.sub(r\"nflweek1picks\", \"NFL week 1 picks\", tweet)\n",
    "    tweet = re.sub(r\"uiseful\", \"useful\", tweet)\n",
    "    tweet = re.sub(r\"JusticeDotOrg\", \"The American Association for Justice\", tweet)\n",
    "    tweet = re.sub(r\"autoaccidents\", \"auto accidents\", tweet)\n",
    "    tweet = re.sub(r\"SteveGursten\", \"Steve Gursten\", tweet)\n",
    "    tweet = re.sub(r\"MichiganAutoLaw\", \"Michigan Auto Law\", tweet)\n",
    "    tweet = re.sub(r\"birdgang\", \"bird gang\", tweet)\n",
    "    tweet = re.sub(r\"nflnetwork\", \"NFL Network\", tweet)\n",
    "    tweet = re.sub(r\"NYDNSports\", \"NY Daily News Sports\", tweet)\n",
    "    tweet = re.sub(r\"RVacchianoNYDN\", \"Ralph Vacchiano NY Daily News\", tweet)\n",
    "    tweet = re.sub(r\"EdmontonEsks\", \"Edmonton Eskimos\", tweet)\n",
    "    tweet = re.sub(r\"david_brelsford\", \"David Brelsford\", tweet)\n",
    "    tweet = re.sub(r\"TOI_India\", \"The Times of India\", tweet)\n",
    "    tweet = re.sub(r\"hegot\", \"he got\", tweet)\n",
    "    tweet = re.sub(r\"SkinsOn9\", \"Skins on 9\", tweet)\n",
    "    tweet = re.sub(r\"sothathappened\", \"so that happened\", tweet)\n",
    "    tweet = re.sub(r\"LCOutOfDoors\", \"LC Out Of Doors\", tweet)\n",
    "    tweet = re.sub(r\"NationFirst\", \"Nation First\", tweet)\n",
    "    tweet = re.sub(r\"IndiaToday\", \"India Today\", tweet)\n",
    "    tweet = re.sub(r\"HLPS\", \"helps\", tweet)\n",
    "    tweet = re.sub(r\"HOSTAGESTHROSW\", \"hostages throw\", tweet)\n",
    "    tweet = re.sub(r\"SNCTIONS\", \"sanctions\", tweet)\n",
    "    tweet = re.sub(r\"BidTime\", \"Bid Time\", tweet)\n",
    "    tweet = re.sub(r\"crunchysensible\", \"crunchy sensible\", tweet)\n",
    "    tweet = re.sub(r\"RandomActsOfRomance\", \"Random acts of romance\", tweet)\n",
    "    tweet = re.sub(r\"MomentsAtHill\", \"Moments at hill\", tweet)\n",
    "    tweet = re.sub(r\"eatshit\", \"eat shit\", tweet)\n",
    "    tweet = re.sub(r\"liveleakfun\", \"live leak fun\", tweet)\n",
    "    tweet = re.sub(r\"SahelNews\", \"Sahel News\", tweet)\n",
    "    tweet = re.sub(r\"abc7newsbayarea\", \"ABC 7 News Bay Area\", tweet)\n",
    "    tweet = re.sub(r\"facilitiesmanagement\", \"facilities management\", tweet)\n",
    "    tweet = re.sub(r\"facilitydude\", \"facility dude\", tweet)\n",
    "    tweet = re.sub(r\"CampLogistics\", \"Camp logistics\", tweet)\n",
    "    tweet = re.sub(r\"alaskapublic\", \"Alaska public\", tweet)\n",
    "    tweet = re.sub(r\"MarketResearch\", \"Market Research\", tweet)\n",
    "    tweet = re.sub(r\"AccuracyEsports\", \"Accuracy Esports\", tweet)\n",
    "    tweet = re.sub(r\"TheBodyShopAust\", \"The Body Shop Australia\", tweet)\n",
    "    tweet = re.sub(r\"yychail\", \"Calgary hail\", tweet)\n",
    "    tweet = re.sub(r\"yyctraffic\", \"Calgary traffic\", tweet)\n",
    "    tweet = re.sub(r\"eliotschool\", \"eliot school\", tweet)\n",
    "    tweet = re.sub(r\"TheBrokenCity\", \"The Broken City\", tweet)\n",
    "    tweet = re.sub(r\"OldsFireDept\", \"Olds Fire Department\", tweet)\n",
    "    tweet = re.sub(r\"RiverComplex\", \"River Complex\", tweet)\n",
    "    tweet = re.sub(r\"fieldworksmells\", \"field work smells\", tweet)\n",
    "    tweet = re.sub(r\"IranElection\", \"Iran Election\", tweet)\n",
    "    tweet = re.sub(r\"glowng\", \"glowing\", tweet)\n",
    "    tweet = re.sub(r\"kindlng\", \"kindling\", tweet)\n",
    "    tweet = re.sub(r\"riggd\", \"rigged\", tweet)\n",
    "    tweet = re.sub(r\"slownewsday\", \"slow news day\", tweet)\n",
    "    tweet = re.sub(r\"MyanmarFlood\", \"Myanmar Flood\", tweet)\n",
    "    tweet = re.sub(r\"abc7chicago\", \"ABC 7 Chicago\", tweet)\n",
    "    tweet = re.sub(r\"copolitics\", \"Colorado Politics\", tweet)\n",
    "    tweet = re.sub(r\"AdilGhumro\", \"Adil Ghumro\", tweet)\n",
    "    tweet = re.sub(r\"netbots\", \"net bots\", tweet)\n",
    "    tweet = re.sub(r\"byebyeroad\", \"bye bye road\", tweet)\n",
    "    tweet = re.sub(r\"massiveflooding\", \"massive flooding\", tweet)\n",
    "    tweet = re.sub(r\"EndofUS\", \"End of United States\", tweet)\n",
    "    tweet = re.sub(r\"35PM\", \"35 PM\", tweet)\n",
    "    tweet = re.sub(r\"greektheatrela\", \"Greek Theatre Los Angeles\", tweet)\n",
    "    tweet = re.sub(r\"76mins\", \"76 minutes\", tweet)\n",
    "    tweet = re.sub(r\"publicsafetyfirst\", \"public safety first\", tweet)\n",
    "    tweet = re.sub(r\"livesmatter\", \"lives matter\", tweet)\n",
    "    tweet = re.sub(r\"myhometown\", \"my hometown\", tweet)\n",
    "    tweet = re.sub(r\"tankerfire\", \"tanker fire\", tweet)\n",
    "    tweet = re.sub(r\"MEMORIALDAY\", \"memorial day\", tweet)\n",
    "    tweet = re.sub(r\"MEMORIAL_DAY\", \"memorial day\", tweet)\n",
    "    tweet = re.sub(r\"instaxbooty\", \"instagram booty\", tweet)\n",
    "    tweet = re.sub(r\"Jerusalem_Post\", \"Jerusalem Post\", tweet)\n",
    "    tweet = re.sub(r\"WayneRooney_INA\", \"Wayne Rooney\", tweet)\n",
    "    tweet = re.sub(r\"VirtualReality\", \"Virtual Reality\", tweet)\n",
    "    tweet = re.sub(r\"OculusRift\", \"Oculus Rift\", tweet)\n",
    "    tweet = re.sub(r\"OwenJones84\", \"Owen Jones\", tweet)\n",
    "    tweet = re.sub(r\"jeremycorbyn\", \"Jeremy Corbyn\", tweet)\n",
    "    tweet = re.sub(r\"paulrogers002\", \"Paul Rogers\", tweet)\n",
    "    tweet = re.sub(r\"mortalkombatx\", \"Mortal Kombat X\", tweet)\n",
    "    tweet = re.sub(r\"mortalkombat\", \"Mortal Kombat\", tweet)\n",
    "    tweet = re.sub(r\"FilipeCoelho92\", \"Filipe Coelho\", tweet)\n",
    "    tweet = re.sub(r\"OnlyQuakeNews\", \"Only Quake News\", tweet)\n",
    "    tweet = re.sub(r\"kostumes\", \"costumes\", tweet)\n",
    "    tweet = re.sub(r\"YEEESSSS\", \"yes\", tweet)\n",
    "    tweet = re.sub(r\"ToshikazuKatayama\", \"Toshikazu Katayama\", tweet)\n",
    "    tweet = re.sub(r\"IntlDevelopment\", \"Intl Development\", tweet)\n",
    "    tweet = re.sub(r\"ExtremeWeather\", \"Extreme Weather\", tweet)\n",
    "    tweet = re.sub(r\"WereNotGruberVoters\", \"We are not gruber voters\", tweet)\n",
    "    tweet = re.sub(r\"NewsThousands\", \"News Thousands\", tweet)\n",
    "    tweet = re.sub(r\"EdmundAdamus\", \"Edmund Adamus\", tweet)\n",
    "    tweet = re.sub(r\"EyewitnessWV\", \"Eye witness WV\", tweet)\n",
    "    tweet = re.sub(r\"PhiladelphiaMuseu\", \"Philadelphia Museum\", tweet)\n",
    "    tweet = re.sub(r\"DublinComicCon\", \"Dublin Comic Con\", tweet)\n",
    "    tweet = re.sub(r\"NicholasBrendon\", \"Nicholas Brendon\", tweet)\n",
    "    tweet = re.sub(r\"Alltheway80s\", \"All the way 80s\", tweet)\n",
    "    tweet = re.sub(r\"FromTheField\", \"From the field\", tweet)\n",
    "    tweet = re.sub(r\"NorthIowa\", \"North Iowa\", tweet)\n",
    "    tweet = re.sub(r\"WillowFire\", \"Willow Fire\", tweet)\n",
    "    tweet = re.sub(r\"MadRiverComplex\", \"Mad River Complex\", tweet)\n",
    "    tweet = re.sub(r\"feelingmanly\", \"feeling manly\", tweet)\n",
    "    tweet = re.sub(r\"stillnotoverit\", \"still not over it\", tweet)\n",
    "    tweet = re.sub(r\"FortitudeValley\", \"Fortitude Valley\", tweet)\n",
    "    tweet = re.sub(r\"CoastpowerlineTramTr\", \"Coast powerline\", tweet)\n",
    "    tweet = re.sub(r\"ServicesGold\", \"Services Gold\", tweet)\n",
    "    tweet = re.sub(r\"NewsbrokenEmergency\", \"News broken emergency\", tweet)\n",
    "    tweet = re.sub(r\"Evaucation\", \"evacuation\", tweet)\n",
    "    tweet = re.sub(r\"leaveevacuateexitbe\", \"leave evacuate exit be\", tweet)\n",
    "    tweet = re.sub(r\"P_EOPLE\", \"PEOPLE\", tweet)\n",
    "    tweet = re.sub(r\"Tubestrike\", \"tube strike\", tweet)\n",
    "    tweet = re.sub(r\"CLASS_SICK\", \"CLASS SICK\", tweet)\n",
    "    tweet = re.sub(r\"localplumber\", \"local plumber\", tweet)\n",
    "    tweet = re.sub(r\"awesomejobsiri\", \"awesome job siri\", tweet)\n",
    "    tweet = re.sub(r\"PayForItHow\", \"Pay for it how\", tweet)\n",
    "    tweet = re.sub(r\"ThisIsAfrica\", \"This is Africa\", tweet)\n",
    "    tweet = re.sub(r\"crimeairnetwork\", \"crime air network\", tweet)\n",
    "    tweet = re.sub(r\"KimAcheson\", \"Kim Acheson\", tweet)\n",
    "    tweet = re.sub(r\"cityofcalgary\", \"City of Calgary\", tweet)\n",
    "    tweet = re.sub(r\"prosyndicate\", \"pro syndicate\", tweet)\n",
    "    tweet = re.sub(r\"660NEWS\", \"660 NEWS\", tweet)\n",
    "    tweet = re.sub(r\"BusInsMagazine\", \"Business Insurance Magazine\", tweet)\n",
    "    tweet = re.sub(r\"wfocus\", \"focus\", tweet)\n",
    "    tweet = re.sub(r\"ShastaDam\", \"Shasta Dam\", tweet)\n",
    "    tweet = re.sub(r\"go2MarkFranco\", \"Mark Franco\", tweet)\n",
    "    tweet = re.sub(r\"StephGHinojosa\", \"Steph Hinojosa\", tweet)\n",
    "    tweet = re.sub(r\"Nashgrier\", \"Nash Grier\", tweet)\n",
    "    tweet = re.sub(r\"NashNewVideo\", \"Nash new video\", tweet)\n",
    "    tweet = re.sub(r\"IWouldntGetElectedBecause\", \"I would not get elected because\", tweet)\n",
    "    tweet = re.sub(r\"SHGames\", \"Sledgehammer Games\", tweet)\n",
    "    tweet = re.sub(r\"bedhair\", \"bed hair\", tweet)\n",
    "    tweet = re.sub(r\"JoelHeyman\", \"Joel Heyman\", tweet)\n",
    "    tweet = re.sub(r\"viaYouTube\", \"via YouTube\", tweet)\n",
    "    tweet = re.sub(r\"earthquake\", \"earth quake\", tweet)\n",
    "    tweet = re.sub(r\"wildfire\", \"wild fire\", tweet)\n",
    "    tweet = re.sub(r\"forestfire\", \"forest fire\", tweet)       \n",
    "    # Urls\n",
    "    tweet = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", tweet)\n",
    "        \n",
    "    # Words with punctuations and special characters\n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n",
    "    for p in punctuations:\n",
    "        tweet = tweet.replace(p, f' {p} ')\n",
    "        \n",
    "    # ... and ..\n",
    "    tweet = tweet.replace('...', ' ... ')\n",
    "    if '...' not in tweet:\n",
    "        tweet = tweet.replace('..', ' ... ')      \n",
    "        \n",
    "    # Acronyms\n",
    "    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", tweet)\n",
    "    tweet = re.sub(r\"mÌ¼sica\", \"music\", tweet)\n",
    "    tweet = re.sub(r\"okwx\", \"Oklahoma City Weather\", tweet)\n",
    "    tweet = re.sub(r\"arwx\", \"Arkansas Weather\", tweet)    \n",
    "    tweet = re.sub(r\"gawx\", \"Georgia Weather\", tweet)  \n",
    "    tweet = re.sub(r\"scwx\", \"South Carolina Weather\", tweet)  \n",
    "    tweet = re.sub(r\"cawx\", \"California Weather\", tweet)\n",
    "    tweet = re.sub(r\"tnwx\", \"Tennessee Weather\", tweet)\n",
    "    tweet = re.sub(r\"azwx\", \"Arizona Weather\", tweet)  \n",
    "    tweet = re.sub(r\"alwx\", \"Alabama Weather\", tweet)\n",
    "    tweet = re.sub(r\"wordpressdotcom\", \"wordpress\", tweet)    \n",
    "    tweet = re.sub(r\"usNWSgov\", \"United States National Weather Service\", tweet)\n",
    "    tweet = re.sub(r\"Suruc\", \"Sanliurfa\", tweet)   \n",
    "    \n",
    "    # Grouping same words without embeddings\n",
    "    tweet = re.sub(r\"Bestnaijamade\", \"bestnaijamade\", tweet)\n",
    "    tweet = re.sub(r\"SOUDELOR\", \"Soudelor\", tweet)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "df['text_cleaned'] = df['tweet'].apply(lambda s : clean(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['text_cleaned'] = df_test['text'].apply(lambda s : clean(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_clean(tweet):\n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n",
    "    for p in punctuations:\n",
    "        tweet = tweet.replace(p, f' {p} ')\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_cleaned'] = df['text_cleaned'].apply(lambda s : add_clean(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['text_cleaned'] = df_test['text_cleaned'].apply(lambda s : add_clean(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>our deeds are the reason of this   #  earth qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>forest fire near la ronge sask  .   canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>all residents asked to   '  shelter in place  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>13,000 people receive   #  wild fires evacuati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>just got sent this photo from ruby   #  alaska...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>two giant cranes holding a bridge collapse int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>@  Aria Ahrary   @  thetawniest the out of c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>m1  .  94   [  01  :  04 utc  ]    ?  5km s of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>police investigating after an e  -  bike colli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>the latest  :   more homes razed by northern c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...   \n",
       "1                Forest fire near La Ronge Sask. Canada   \n",
       "2     All residents asked to 'shelter in place' are ...   \n",
       "3     13,000 people receive #wildfires evacuation or...   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...   \n",
       "...                                                 ...   \n",
       "7608  Two giant cranes holding a bridge collapse int...   \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...   \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...   \n",
       "7611  Police investigating after an e-bike collided ...   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...   \n",
       "\n",
       "                                           text_cleaned  \n",
       "0     our deeds are the reason of this   #  earth qu...  \n",
       "1            forest fire near la ronge sask  .   canada  \n",
       "2     all residents asked to   '  shelter in place  ...  \n",
       "3     13,000 people receive   #  wild fires evacuati...  \n",
       "4     just got sent this photo from ruby   #  alaska...  \n",
       "...                                                 ...  \n",
       "7608  two giant cranes holding a bridge collapse int...  \n",
       "7609    @  Aria Ahrary   @  thetawniest the out of c...  \n",
       "7610  m1  .  94   [  01  :  04 utc  ]    ?  5km s of...  \n",
       "7611  police investigating after an e  -  bike colli...  \n",
       "7612  the latest  :   more homes razed by northern c...  \n",
       "\n",
       "[7613 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['text','text_cleaned']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to correct the Slang ASAP->As Soon As Possible\n",
    "import csv\n",
    "import re\n",
    "def translator(user_string):\n",
    "    user_string = user_string.split(\" \")\n",
    "    j = 0\n",
    "    for _str in user_string:\n",
    "        # File path which consists of Abbreviations.\n",
    "        fileName = \"Slang\"\n",
    "        # File Access mode [Read Mode]\n",
    "        accessMode = \"r\"\n",
    "        with open(fileName, accessMode) as myCSVfile:\n",
    "            # Reading file as CSV with delimiter as \"=\", so that abbreviation are stored in row[0] and phrases in row[1]\n",
    "            dataFromFile = csv.reader(myCSVfile, delimiter=\"=\")\n",
    "            # Removing Special Characters.\n",
    "            _str = re.sub('[^a-zA-Z0-9-_.]', '', _str)\n",
    "            for row in dataFromFile:\n",
    "                # Check if selected word matches short forms[LHS] in text file.\n",
    "                if _str.upper() == row[0]:\n",
    "                    # If match found replace it with its Abbreviation in text file.\n",
    "                    user_string[j] = row[1]\n",
    "            myCSVfile.close()\n",
    "        j = j + 1\n",
    "    # Replacing commas with spaces for final output.\n",
    "    temp = ' '.join(user_string)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling translator\n",
    "df['trans'] = df['text_cleaned'].apply(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling translator\n",
    "df_test['trans'] = df_test['text_cleaned'].apply(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       our deeds are the reason of this   #  earth qu...\n",
       "1              forest fire near la ronge sask  .   canada\n",
       "2       all residents asked to   '  shelter in place  ...\n",
       "3       13,000 people receive   #  wild fires evacuati...\n",
       "4       just got sent this photo from ruby   #  alaska...\n",
       "                              ...                        \n",
       "7608    two giant cranes holding a bridge collapse int...\n",
       "7609      @  Aria Ahrary   @  thetawniest the out of c...\n",
       "7610    m1  .  94   [  01  :  04 utc  ]    ?  5km s of...\n",
       "7611    police investigating after an e  -  bike colli...\n",
       "7612    the latest  :   more homes razed by northern c...\n",
       "Name: trans, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['trans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: DeprecationWarning: invalid escape sequence \\[\n",
      "<>:6: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:10: DeprecationWarning: invalid escape sequence \\w\n",
      "<>:5: DeprecationWarning: invalid escape sequence \\[\n",
      "<>:6: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:10: DeprecationWarning: invalid escape sequence \\w\n",
      "<>:5: DeprecationWarning: invalid escape sequence \\[\n",
      "<>:6: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:10: DeprecationWarning: invalid escape sequence \\w\n",
      "<ipython-input-26-e568bcaac347>:5: DeprecationWarning: invalid escape sequence \\[\n",
      "  text = re.sub('\\[.*?\\]', '', text)\n",
      "<ipython-input-26-e568bcaac347>:6: DeprecationWarning: invalid escape sequence \\S\n",
      "  text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
      "<ipython-input-26-e568bcaac347>:10: DeprecationWarning: invalid escape sequence \\w\n",
      "  text = re.sub('\\w*\\d\\w*', '', text)\n"
     ]
    }
   ],
   "source": [
    "# Removing punctuation, html tags, symbols, numbers, etc.\n",
    "import string\n",
    "def remove_noise(text):\n",
    "    # Dealing with Punctuation\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "# Calling remove_noise function in order to remove noise\n",
    "df['trans'] = df['trans'].apply(lambda x: remove_noise(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['trans'] = df_test['trans'].apply(lambda x: remove_noise(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"remove whitespaces before and after string\")\n",
    "df['trans'] = df['trans'].str.strip()\n",
    "\n",
    "#\"remove double spaces within sentence\")\n",
    "df['trans'] = df['trans'].map(lambda x: \" \".join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"remove whitespaces before and after string\")\n",
    "df_test['trans'] = df_test['trans'].str.strip()\n",
    "\n",
    "#\"remove double spaces within sentence\")\n",
    "df_test['trans'] = df_test['trans'].map(lambda x: \" \".join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>trans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>beware world ablaze sierra leone &amp;amp; guap.</td>\n",
       "      <td>beware world ablaze sierra leone guap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>burning man ablaze! by turban diva http://t.co...</td>\n",
       "      <td>burning man ablaze by turban diva via etsy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>not a diss song. people will take 1 thing and ...</td>\n",
       "      <td>not a diss song people will take thing and run...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rape victim dies as she sets herself ablaze: a...</td>\n",
       "      <td>rape victim dies as she sets herself ablaze a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>setting myself ablaze http://t.co/6vme7p5xhc</td>\n",
       "      <td>setting myself ablaze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>@ctvtoronto the bins in front of the field by ...</td>\n",
       "      <td>ctvtoronto the bins in front of the field by m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>#nowplaying alfons - ablaze 2015 on puls radio...</td>\n",
       "      <td>nowplaying alfons ablaze on puls radio pulsradio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>'burning rahm': let us hope city hall builds a...</td>\n",
       "      <td>burning rahm let us hope city hall builds a gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>@philippaeilhart @dhublath hurt but her eyes a...</td>\n",
       "      <td>philippaeilhart dhublath hurt but her eyes abl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>accident cleared in #paturnpike on patp eb bet...</td>\n",
       "      <td>accident cleared in paturnpike on patp eb betw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>just got to love burning your self on a damn c...</td>\n",
       "      <td>just got to love burning your self on a damn c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>i hate badging shit in accident</td>\n",
       "      <td>i hate badging shit in accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>#3: car recorder zeroedgeå¨ dual-lens car came...</td>\n",
       "      <td>car recorder zeroedge dual lens car camera veh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>coincidence or #curse? still #unresolved secre...</td>\n",
       "      <td>coincidence or curse still unresolved secrets ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>@traffic_southe @roadpol_east accident on a27 ...</td>\n",
       "      <td>trafficsouthe roadpoleast accident on near lew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>@sakuma_en if you pretend to feel a certain wa...</td>\n",
       "      <td>sakumaen if you pretend to feel a certain way ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>for legal and medical referral service @1800_i...</td>\n",
       "      <td>for legal and medical referral service call us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>there is a construction guy working on the dis...</td>\n",
       "      <td>there is a construction guy working on the dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>@robynjilllian @wlsdomteeths i feel like i am ...</td>\n",
       "      <td>robynjilllian wlsdomteeths i feel like i am go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>on the #m42 northbound between junctions j3 an...</td>\n",
       "      <td>on the northbound between junctions and there ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "20       beware world ablaze sierra leone &amp; guap.   \n",
       "21  burning man ablaze! by turban diva http://t.co...   \n",
       "22  not a diss song. people will take 1 thing and ...   \n",
       "23  rape victim dies as she sets herself ablaze: a...   \n",
       "24       setting myself ablaze http://t.co/6vme7p5xhc   \n",
       "25  @ctvtoronto the bins in front of the field by ...   \n",
       "26  #nowplaying alfons - ablaze 2015 on puls radio...   \n",
       "27  'burning rahm': let us hope city hall builds a...   \n",
       "28  @philippaeilhart @dhublath hurt but her eyes a...   \n",
       "29  accident cleared in #paturnpike on patp eb bet...   \n",
       "30  just got to love burning your self on a damn c...   \n",
       "31                    i hate badging shit in accident   \n",
       "32  #3: car recorder zeroedgeå¨ dual-lens car came...   \n",
       "33  coincidence or #curse? still #unresolved secre...   \n",
       "34  @traffic_southe @roadpol_east accident on a27 ...   \n",
       "35  @sakuma_en if you pretend to feel a certain wa...   \n",
       "36  for legal and medical referral service @1800_i...   \n",
       "37  there is a construction guy working on the dis...   \n",
       "38  @robynjilllian @wlsdomteeths i feel like i am ...   \n",
       "39  on the #m42 northbound between junctions j3 an...   \n",
       "\n",
       "                                                trans  \n",
       "20              beware world ablaze sierra leone guap  \n",
       "21         burning man ablaze by turban diva via etsy  \n",
       "22  not a diss song people will take thing and run...  \n",
       "23  rape victim dies as she sets herself ablaze a ...  \n",
       "24                              setting myself ablaze  \n",
       "25  ctvtoronto the bins in front of the field by m...  \n",
       "26   nowplaying alfons ablaze on puls radio pulsradio  \n",
       "27  burning rahm let us hope city hall builds a gi...  \n",
       "28  philippaeilhart dhublath hurt but her eyes abl...  \n",
       "29  accident cleared in paturnpike on patp eb betw...  \n",
       "30  just got to love burning your self on a damn c...  \n",
       "31                    i hate badging shit in accident  \n",
       "32  car recorder zeroedge dual lens car camera veh...  \n",
       "33  coincidence or curse still unresolved secrets ...  \n",
       "34  trafficsouthe roadpoleast accident on near lew...  \n",
       "35  sakumaen if you pretend to feel a certain way ...  \n",
       "36  for legal and medical referral service call us...  \n",
       "37  there is a construction guy working on the dis...  \n",
       "38  robynjilllian wlsdomteeths i feel like i am go...  \n",
       "39  on the northbound between junctions and there ...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[['text','trans']][20:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(string):    \n",
    "    tokenized = word_tokenize(string)\n",
    "    filtered_sentence = [word for word in tokenized if not word in stop_words]\n",
    "    return ' '.join(c for c in filtered_sentence)\n",
    "df['trans'] = df.apply(lambda row: remove_stopwords(row['trans']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['trans'] = df_test.apply(lambda row: remove_stopwords(row['trans']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp =spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing lemitization\n",
    "def spacy_lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "df['trans'] = df.apply(lambda row: spacy_lemmatize_text(row['trans']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['trans'] = df_test.apply(lambda row: spacy_lemmatize_text(row['trans']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#>>> df_yelp = df[df['source'] == 'yelp']\n",
    "\n",
    "sentences = df['trans'].values\n",
    "y = df['target'].values\n",
    "\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = df_test['trans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(sentences_train)\n",
    "\n",
    "X_train = vectorizer.transform(sentences_train)\n",
    "X_test  = vectorizer.transform(sentences_test)\n",
    "test_sent= vectorizer.transform(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "\n",
    "On = OneHotEncoder()\n",
    "num_classes=2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_class = y_test.reshape(-1,1)\n",
    "train_class = y_train.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_o = On.fit_transform(train_class)\n",
    "y_test_o = On.transform(test_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8014705882352942\n"
     ]
    }
   ],
   "source": [
    "#performimg Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(penalty='l1',solver='liblinear')\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.001: 0.5835084033613446\n",
      "Accuracy for C=0.01: 0.7358193277310925\n",
      "Accuracy for C=0.1: 0.8061974789915967\n",
      "Accuracy for C=0.75: 0.8114495798319328\n",
      "Accuracy for C=1: 0.8082983193277311\n",
      "Accuracy for C=2: 0.8082983193277311\n",
      "Accuracy for C=3: 0.805672268907563\n",
      "Accuracy for C=4: 0.8019957983193278\n",
      "Accuracy for C=5: 0.8014705882352942\n",
      "Accuracy for C=10: 0.8009453781512605\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "c_val = [0.001,0.01,0.1,0.75, 1, 2, 3, 4, 5, 10]\n",
    "\n",
    "for c in c_val:\n",
    "    logreg = LogisticRegression(C=c)\n",
    "    logreg.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" % (c, accuracy_score(y_test, logreg.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.001: 0.5782563025210085\n",
      "Accuracy for C=0.01: 0.5793067226890757\n",
      "Accuracy for C=0.1: 0.7337184873949579\n",
      "Accuracy for C=0.75: 0.7983193277310925\n",
      "Accuracy for C=1: 0.8014705882352942\n",
      "Accuracy for C=2: 0.7909663865546218\n",
      "Accuracy for C=3: 0.7899159663865546\n",
      "Accuracy for C=4: 0.7841386554621849\n",
      "Accuracy for C=5: 0.7851890756302521\n",
      "Accuracy for C=10: 0.7757352941176471\n"
     ]
    }
   ],
   "source": [
    "#using l1 penalty\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "c_val = [0.001,0.01,0.1,0.75, 1, 2, 3, 4, 5, 10]\n",
    "\n",
    "#Lasso \n",
    "for c in c_val:\n",
    "    logreg_l = LogisticRegression(C=c,penalty='l1',solver='liblinear')\n",
    "    logreg_l.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" % (c, accuracy_score(y_test, logreg_l.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=10: 0.7983193277310925\n"
     ]
    }
   ],
   "source": [
    "logreg_l = LogisticRegression(C=0.75,penalty='l1',solver='liblinear')\n",
    "logreg_l.fit(X_train, y_train)\n",
    "print (\"Accuracy for C=%s: %s\" % (c, accuracy_score(y_test, logreg_l.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting on test data\n",
    "df_test['target'] = logreg_l.predict(test_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = df_test[['id','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "0         0       1\n",
       "1         2       1\n",
       "2         3       1\n",
       "3         9       1\n",
       "4        11       1\n",
       "...     ...     ...\n",
       "3258  10861       1\n",
       "3259  10865       1\n",
       "3260  10868       1\n",
       "3261  10874       1\n",
       "3262  10875       1\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# random forest model creation\n",
    "depth = [2,4,6,8,10,15,20,30,40]\n",
    "for d in depth:\n",
    "    rfc = RandomForestClassifier(max_features= 'sqrt',max_depth=d,n_estimators=1000, random_state=20)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    # predictions\n",
    "    print (\"Accuracy for Depth=%s: %s\" % (d, accuracy_score(y_test, rfc.predict(X_test))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = classifier.score(X_test, y_test)\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(df['trans'],df['target'],test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           deeds reason earth quake may allah forgive us\n",
       "1                   forest fire near la ronge sask canada\n",
       "2       resident ask shelter place notify officer evac...\n",
       "3       people receive wild fire evacuation order cali...\n",
       "4       get send photo ruby alaska smoke wild fire pou...\n",
       "                              ...                        \n",
       "7608     two giant crane hold bridge collapse nearby home\n",
       "7609    Aria Ahrary thetawni control wild fire califor...\n",
       "7610                                       volcano hawaii\n",
       "7611    police investigate e bike collided car little ...\n",
       "7612    late home raze northern california wild fire a...\n",
       "Name: trans, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['trans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing lable encoading\n",
    "Encoder = LabelEncoder()\n",
    "Train_Y = Encoder.fit_transform(Train_Y)\n",
    "Test_Y = Encoder.fit_transform(Test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf\n",
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(df['trans'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test = Tfidf_vect.transform(df_test['trans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 3577, 'earth': 1345, 'quake': 3501, 'may': 2700, 'allah': 112, 'forgive': 1690, 'us': 4666, 'forest': 1686, 'fire': 1633, 'near': 2948, 'la': 2430, 'canada': 668, 'resident': 3653, 'ask': 261, 'shelter': 3943, 'place': 3311, 'officer': 3077, 'evacuation': 1490, 'order': 3131, 'expect': 1520, 'people': 3238, 'receive': 3583, 'wild': 4843, 'california': 648, 'get': 1806, 'send': 3882, 'photo': 3272, 'alaska': 103, 'smoke': 4051, 'pour': 3382, 'school': 3831, 'rockyfire': 3728, 'update': 4654, 'hwy': 2109, 'close': 844, 'direction': 1217, 'due': 1322, 'lake': 2436, 'county': 991, 'cafire': 640, 'flood': 1655, 'disaster': 1225, 'heavy': 1998, 'rain': 3529, 'cause': 712, 'flash': 1647, 'flooding': 1656, 'street': 4203, 'colorado': 887, 'spring': 4132, 'area': 222, 'top': 4474, 'hill': 2028, 'see': 3867, 'wood': 4889, 'emergency': 1410, 'happen': 1947, 'building': 606, 'across': 33, 'afraid': 73, 'tornado': 4477, 'come': 895, 'three': 4421, 'die': 1202, 'heat': 1995, 'wave': 4781, 'far': 1566, 'haha': 1923, 'south': 4094, 'tampa': 4327, 'hah': 1922, 'wait': 4753, 'second': 3861, 'live': 2549, 'gon': 1843, 'na': 2911, 'florida': 1659, 'day': 1094, 'lose': 2590, 'count': 987, 'bago': 338, 'myanmar': 2907, 'arrive': 241, 'damage': 1069, 'bus': 621, 'multi': 2886, 'car': 682, 'crash': 1007, 'breaking': 552, 'man': 2646, 'love': 2596, 'fruit': 1742, 'summer': 4246, 'lovely': 2597, 'fast': 1573, 'ridiculous': 3696, 'london': 2575, 'cool': 970, 'wonderful': 4888, 'way': 4782, 'eat': 1352, 'shit': 3951, 'nyc': 3052, 'last': 2449, 'week': 4803, 'girlfriend': 1816, 'like': 2531, 'pasta': 3206, 'end': 1428, 'wholesale': 4833, 'market': 2672, 'ablaze': 10, 'always': 130, 'try': 4570, 'bring': 567, 'metal': 2757, 'rt': 3760, 'break': 550, 'news': 2975, 'nigeria': 2995, 'flag': 1645, 'set': 3903, 'aba': 1, 'cry': 1034, 'plus': 3340, 'side': 3973, 'look': 2581, 'sky': 4026, 'night': 2999, 'build': 605, 'much': 2883, 'hype': 2112, 'around': 238, 'new': 2970, 'acquisition': 31, 'doubt': 1281, 'season': 3858, 'inec': 2189, 'office': 3076, 'abia': 8, 'jamaica': 2296, 'two': 4601, 'santa': 3806, 'cruz': 1033, 'head': 1978, 'st': 4143, 'elizabeth': 1399, 'police': 3350, 'lord': 2587, 'check': 765, 'nsfw': 3036, 'outside': 3159, 'dead': 1098, 'inside': 2214, 'awesome': 322, 'time': 4439, 'visit': 4735, 'cfc': 740, 'site': 4008, 'thanks': 4397, 'tita': 4446, 'vida': 4714, 'take': 4320, 'care': 684, 'pump': 3490, 'want': 4760, 'chicago': 779, 'hotel': 2077, 'gain': 1766, 'follower': 1673, 'know': 2414, 'your': 4956, 'sex': 3912, 'age': 81, 'grow': 1895, 'west': 4812, 'burn': 617, 'thousand': 4418, 'alone': 120, 'perfect': 3242, 'life': 2520, 'leave': 2479, 'first': 1637, 'quite': 3514, 'weird': 4807, 'well': 4810, 'wear': 4795, 'every': 1496, 'single': 3995, 'next': 2982, 'year': 4940, 'least': 2477, 'deputy': 1158, 'shoot': 3956, 'brighton': 566, 'home': 2051, 'wife': 4842, 'six': 4012, 'jail': 2294, 'niece': 2994, 'ûó': 4999, 'salmon': 3793, 'arsonist': 245, 'black': 464, 'church': 804, 'north': 3018, 'el': 1382, 'happy': 1951, 'train': 4511, 'hard': 1954, 'turkmen': 4587, 'later': 2452, 'other': 3146, 'front': 1739, 'truck': 4561, 'ave': 310, 'cargo': 688, 'section': 3863, 'heart': 1991, 'city': 815, 'gift': 1813, 'skyline': 4027, 'keep': 2378, 'it': 2283, 'simple': 3985, 'stupid': 4230, 'upon': 4657, 'lip': 2543, 'tonight': 4470, 'los': 2589, 'angeles': 152, 'ig': 2134, 'fb': 1585, 'fill': 1622, 'sunset': 4250, 'shot': 3961, 'peep': 3232, 'climate': 838, 'energy': 1433, 'wmv': 4877, 'mean': 2712, 'mac': 2615, 'en': 1421, 'route': 3751, 'dvd': 1334, 'progressive': 3457, 'greeting': 1884, 'month': 2856, 'student': 4225, 'would': 4909, 'torch': 4476, 'secret': 3862, 'fall': 1556, 'edit': 1362, 'steve': 4177, 'something': 4078, 'else': 1403, 'clown': 849, 'hood': 2061, 'nowplaye': 3031, 'ian': 2115, 'buff': 601, 'edm': 1366, 'huge': 2090, 'talk': 4324, 'go': 1832, 'make': 2636, 'work': 4893, 'kid': 2388, 'cuz': 1055, 'bicycle': 435, 'accident': 20, 'split': 4120, 'impossible': 2158, 'michael': 2773, 'father': 1579, 'traffic': 4505, 'move': 2873, 'slow': 4040, 'usual': 4675, 'center': 731, 'lane': 2443, 'block': 489, 'nb': 2940, 'great': 1877, 'america': 134, 'pkwy': 3310, 'read': 3565, 'advice': 60, 'solicitor': 4068, 'help': 2006, 'speed': 4114, 'among': 138, 'teen': 4361, 'tee': 4360, 'report': 3642, 'motor': 2866, 'vehicle': 4695, 'herman': 2011, 'rd': 3559, 'involve': 2262, 'please': 3330, 'use': 4670, 'awareness': 320, 'mile': 2792, 'marker': 2671, 'iredell': 2268, 'pm': 3342, 'sleep': 4033, 'pill': 3291, 'double': 1280, 'risk': 3710, 'av': 308, 'scene': 3827, 'owner': 3164, 'range': 3538, 'rover': 3753, 'mom': 2846, 'wish': 4867, 'horrible': 2068, 'past': 3205, 'sunday': 4249, 'finally': 1625, 'able': 11, 'thank': 4394, 'god': 1835, 'pissed': 3301, 'tell': 4364, 'another': 168, 'interstate': 2248, 'click': 835, 'sb': 3820, 'sr': 4141, 'carolina': 691, 'motorcyclist': 2868, 'cross': 1027, 'motorcycle': 2867, 'rider': 3694, 'travel': 4527, 'for': 1679, 'information': 2198, 'cad': 638, 'property': 3466, 'nhs': 2986, 'piner': 3295, 'horndale': 2067, 'dr': 1286, 'turn': 4588, 'onto': 3106, 'mma': 2835, 'taxi': 4337, 'ram': 3535, 'everyone': 1498, 'manchester': 2650, 'stop': 4187, 'back': 329, 'nh': 2984, 'delay': 1131, 'mins': 2818, 'injury': 2205, 'actress': 40, 'fatal': 1575, 'alberta': 104, 'backup': 332, 'right': 3699, 'exit': 1517, 'consider': 950, 'nc': 2944, 'change': 748, 'option': 3127, 'support': 4257, 'plan': 3315, 'treatment': 4532, 'deadly': 1099, 'today': 4456, 'detail': 1176, 'even': 1491, 'fuck': 1745, 'mfs': 2768, 'fucking': 1747, 'drive': 1301, 'previously': 3424, 'road': 3715, 'kill': 2392, 'explosion': 1531, 'still': 4180, 'hear': 1989, 'leader': 2469, 'kenya': 2381, 'forward': 1700, 'comment': 902, 'issue': 2282, 'arrestpastornganga': 240, 'aftershockdelo': 79, 'scuf': 3852, 'ps': 3483, 'game': 1770, 'you': 4952, 'effort': 1374, 'win': 4851, 'roger': 3729, 'icemoon': 2119, 'dj': 1257, 'dubstep': 1319, 'trapmusic': 4522, 'dnb': 1262, 'dance': 1075, 'ices': 2120, 'victory': 4712, 'bargain': 360, 'basement': 368, 'price': 3426, 'david': 1092, 'nobody': 3011, 'remember': 3630, 'charles': 755, 'be': 390, 'speak': 4106, 'someone': 4077, 'also': 127, 'conflict': 941, 'glorious': 1827, 'triumph': 4550, 'thomas': 4415, 'aftershock': 78, 'guess': 1905, 'one': 3102, 'actually': 42, 'free': 1722, 'tc': 4342, 'terrify': 4379, 'good': 1844, 'roller': 3735, 'coaster': 863, 'ever': 1495, 'minute': 2819, 'daily': 1064, 'habit': 1920, 'could': 985, 'really': 3573, 'improve': 2162, 'many': 2660, 'already': 125, 'protect': 3472, 'profit': 3453, 'global': 1824, 'financial': 1627, 'meltdown': 2735, 'http': 2087, 'moment': 2847, 'scary': 3826, 'guy': 1917, 'behind': 408, 'scream': 3844, 'bloody': 493, 'murder': 2892, 'full': 1751, 'streaming': 4202, 'youtube': 4958, 'book': 519, 'sometimes': 4079, 'face': 1543, 'difficulty': 1208, 'wrong': 4925, 'joel': 2325, 'thing': 4410, 'stand': 4152, 'dream': 1295, 'belief': 409, 'possible': 3373, 'brown': 585, 'avoid': 316, 'trap': 4521, 'think': 4411, 'ûò': 4998, 'job': 2322, 'orange': 3130, 'never': 2969, 'kick': 2387, 'say': 3819, 'do': 1263, 'george': 1800, 'shaw': 3933, 'shell': 3941, 'andrew': 148, 'anyone': 180, 'need': 2954, 'play': 3324, 'hybrid': 2110, 'slayer': 4032, 'eu': 1485, 'expert': 1525, 'france': 1716, 'begin': 406, 'examine': 1506, 'airplane': 96, 'debris': 1107, 'find': 1628, 'reunion': 3673, 'island': 2279, 'french': 1728, 'air': 93, 'strict': 4210, 'liability': 2512, 'context': 962, 'pilot': 3293, 'error': 1471, 'common': 907, 'component': 923, 'aviation': 315, 'cr': 1001, 'lifetime': 2522, 'odd': 3070, 'wedn': 4799, 'awwww': 325, 'family': 1560, 'member': 2736, 'bin': 449, 'laden': 2434, 'ironic': 2272, 'gov': 1854, 'suspect': 4280, 'engine': 1435, 'via': 4705, 'wing': 4858, 'coahuila': 859, 'mexico': 2766, 'july': 2355, 'four': 1705, 'include': 2172, 'state': 4162, 'government': 1855, 'official': 3078, 'wednesday': 4800, 'kca': 2376, 'rip': 3707, 'almost': 118, 'nude': 3040, 'mode': 2840, 'wreck': 4919, 'politic': 3352, 'best': 421, 'mlb': 2834, 'insane': 2212, 'airport': 97, 'aircraft': 94, 'runway': 3771, 'plane': 3316, 'festival': 1609, 'death': 1103, 'dtn': 1316, 'brazil': 547, 'exp': 1518, 'what': 4820, 'the': 4400, 'can': 666, 'ûªt': 4987, 'believe': 410, 'eye': 1540, 'victim': 4710, 'ago': 85, 'little': 2548, 'bit': 459, 'trauma': 4524, 'although': 129, 'omg': 3101, 'bro': 575, 'phone': 3271, 'ship': 3947, 'terrible': 4378, 'cop': 972, 'house': 2080, 'drone': 1305, 'worry': 4904, 'esp': 1476, 'vicinity': 4709, 'early': 1340, 'wake': 4754, 'call': 649, 'sister': 4006, 'beg': 405, 'ride': 3693, 'ambulance': 133, 'hospital': 2072, 'twelve': 4594, 'fear': 1587, 'pakistani': 3176, 'helicopter': 2001, 'serious': 3895, 'lorry': 2588, 'reuters': 3675, 'yugvani': 4964, 'lead': 2468, 'service': 3900, 'boss': 525, 'welcome': 4808, 'charity': 754, 'incident': 2169, 'halt': 1935, 'sprinter': 4134, 'automatic': 306, 'frontline': 1740, 'choice': 790, 'lez': 2509, 'compliant': 921, 'ebay': 1354, 'device': 1188, 'target': 4332, 'destroy': 1174, 'blood': 492, 'hella': 2003, 'crazy': 1011, 'fight': 1617, 'couple': 993, 'pit': 3302, 'run': 3768, 'lucky': 2608, 'fouseytube': 1707, 'ok': 3087, 'hahahah': 1926, 'pakistan': 3175, 'nine': 3002, 'medical': 2719, 'assistance': 269, 'ny': 3051, 'petition': 3259, 'per': 3240, 'hour': 2079, 'ûª': 4983, 'ems': 1419, 'parking': 3195, 'lot': 2592, 'johns': 2329, 'ûï': 4989, 'dog': 1269, 'respond': 3658, 'dual': 1317, 'siren': 4001, 'and': 146, 'worldnews': 4901, 'number': 3043, 'body': 512, 'surprise': 4268, 'practice': 3391, 'trust': 4567, 'walk': 4755, 'hate': 1968, 'episode': 1461, 'trunk': 4566, 'annihilate': 160, 'show': 3964, 'nigga': 2997, 'mercy': 2751, 'shall': 3921, 'lay': 2466, 'bare': 358, 'uribe': 4664, 'baseball': 367, 'met': 2756, 'hey': 2016, 'previous': 3423, 'meeting': 2728, 'celtic': 728, 'indeed': 2177, 'improvement': 2163, 'career': 685, 'compete': 916, 'status': 4166, 'education': 1368, 'behalf': 407, 'easy': 1351, 'luka': 2610, 'everything': 1499, 'alois': 119, 'trancy': 4515, 'un': 4617, 'fella': 1600, 'sorry': 4086, 'pull': 3489, 'drunk': 1313, 'driver': 1302, 'safety': 3788, 'hit': 2037, 'åê': 4982, 'viralspell': 4730, 'must': 2902, 'boom': 520, 'country': 990, 'britain': 570, 'sure': 4261, 'promise': 3460, 'israel': 2280, 'horror': 2070, 'iran': 2264, 'since': 3991, 'spend': 4115, 'history': 2036, 'instantly': 2223, 'become': 398, 'aware': 319, 'ability': 9, 'whole': 4832, 'humanity': 2094, 'tryout': 4572, 'fact': 1546, 'quickly': 3513, 'short': 3960, 'ball': 344, 'arabian': 219, 'hunter': 2103, 'ready': 3566, 'weather': 4796, 'forecast': 1684, 'feat': 1589, 'priest': 3428, 'rob': 3717, 'scorpion': 3839, 'gig': 1814, 'officially': 3079, 'skip': 4024, 'whatever': 4821, 'hashtag': 1965, 'review': 3679, 'bummer': 612, 'tomcatart': 4466, 'thus': 4432, 'explain': 1526, 'case': 699, 'survivor': 4277, 'evolve': 1503, 'completely': 919, 'paul': 3218, 'alive': 110, 'legion': 2489, 'imperfect': 2154, 'project': 3458, 'form': 1692, 'cell': 727, 'exactly': 1505, 'match': 2691, 'brock': 578, 'syrian': 4310, 'army': 235, 'gang': 1774, 'pile': 3289, 'food': 1674, 'bc': 389, 'pickle': 3283, 'fun': 1753, 'bar': 355, 'apart': 190, 'hell': 2002, 'fraction': 1714, 'total': 4483, 'annihilation': 161, 'destruction': 1175, 'usa': 4667, 'potus': 3380, 'maybe': 2702, 'pre': 3395, 'sell': 3879, 'river': 3711, 'civilization': 818, 'national': 2928, 'park': 3193, 'services': 3901, 'tonto': 4471, 'salt': 3794, 'horse': 2071, 'world': 4899, 'vs': 4747, 'self': 3877, 'transformation': 4518, 'alien': 109, 'attack': 283, 'human': 2093, 'sign': 3979, 'share': 3928, 'save': 3817, 'arizona': 229, 'without': 4873, 'org': 3133, 'soul': 4089, 'punish': 3492, 'mention': 2747, 'major': 2634, 'voice': 4738, 'reject': 3618, 'law': 2461, 'false': 1558, 'prophet': 3467, 'imprison': 2161, 'nation': 2927, 'fuel': 1748, 'jeb': 2307, 'christie': 800, 'less': 2500, 'away': 321, 'allow': 115, 'wildhorse': 4844, 'sing': 3992, 'long': 2580, 'current': 1048, 'tie': 4436, 'dante': 1082, 'join': 2331, 'follow': 1671, 'zone': 4977, 'johnny': 2328, 'taylor': 4339, 'soon': 4083, 'ohh': 3084, 'survive': 4276, 'apocalypse': 194, 'feel': 1596, 'poor': 3360, 'boy': 538, 'child': 783, 'birthday': 457, 'watch': 4775, 'charlie': 756, 'film': 1623, 'august': 298, 'red': 3596, 'zombie': 4976, 'stage': 4149, 'kinda': 2396, 'hot': 2076, 'radio': 3523, 'disease': 1233, 'start': 4158, 'careful': 686, 'laugh': 2454, 'out': 3150, 'loud': 2594, 'gf': 1808, 'mate': 2692, 'question': 3511, 'julie': 2353, 'version': 4699, 'minecraft': 2806, 'mod': 2839, 'bob': 511, 'wither': 4870, 'showcase': 3965, 'popularmmos': 3365, 'vi': 4704, 'snow': 4059, 'spirit': 4119, 'angel': 151, 'enormous': 1444, 'high': 2020, 'mountain': 2870, 'imagine': 2149, 'conversation': 968, 'liked': 2532, 'video': 4715, 'mo': 2837, 'planet': 3317, 'lone': 2577, 'audience': 295, 'dad': 1062, 'buy': 629, 'science': 3834, 'doc': 1264, 'impend': 2153, 'peak': 3229, 'dystopian': 1338, 'movie': 2875, 'scare': 3824, 'storm': 4189, 'late': 2450, 'totally': 4484, 'give': 1818, 'bad': 334, 'name': 2917, 'gmt': 1831, 'dark': 1084, 'reveal': 3677, 'queen': 3508, 'shadow': 3917, 'enjoy': 1443, 'action': 35, 'titan': 4447, 'poster': 3376, 'remind': 3631, 'clean': 827, 'pbban': 3222, 'temporary': 4370, 'armageddon': 232, 'xp': 4932, 'vid': 4713, 'trubgme': 4560, 'prod': 3448, 'peace': 3226, 'bed': 399, 'unless': 4642, 'revolution': 3681, 'microsoft': 2780, 'xbox': 4930, 'series': 3894, 'truelove': 4563, 'romance': 3737, 'lith': 2547, 'voodoo': 4744, 'seduction': 3866, 'astrology': 274, 'beat': 396, 'ben': 415, 'affleck': 69, 'girl': 1815, 'coat': 864, 'hand': 1940, 'sense': 3884, 'occasion': 3066, 'hide': 2018, 'door': 1278, 'data': 1088, 'mining': 2812, 'blog': 490, 'sketch': 4019, 'base': 366, 'taste': 4334, 'luck': 2607, 'tomorrow': 4467, 'point': 3347, 'rohnertparkdps': 3732, 'justice': 2363, 'department': 1154, 'pay': 3220, 'income': 2173, 'tax': 4336, 'vladimir': 4736, 'putin': 3500, 'warning': 4768, 'escape': 1474, 'kingdom': 2402, 'heavenly': 1997, 'rule': 3767, 'entertainment': 1450, 'monkey': 2852, 'sixth': 4014, 'auction': 294, 'let': 2502, 'ûªs': 4986, 'prepare': 3408, 'toddler': 4458, 'prepper': 3411, 'library': 2516, 'collection': 878, 'cd': 721, 'survival': 4275, 'yesterday': 4946, 'hail': 1928, 'sadly': 3784, 'windows': 4854, 'avert': 314, 'ultimalucha': 4615, 'blueprint': 503, 'extremely': 1539, 'impressive': 2160, 'till': 4438, 'christmas': 801, 'christians': 799, 'unite': 4636, 'lee': 2481, 'comedy': 896, 'working': 4896, 'class': 823, 'tory': 4482, 'chart': 759, 'prove': 3478, 'crisis': 1023, 'part': 3198, 'economic': 1356, 'collapse': 877, 'track': 4501, 'roberts': 3719, 'russia': 3773, 'light': 2525, 'funny': 1756, 'staff': 4148, 'finger': 1630, 'eh': 1378, 'beyonce': 431, 'pick': 3280, 'fan': 1563, 'beyhive': 430, 'directioners': 1218, 'slightly': 4038, 'friend': 1734, 'code': 870, 'salvation': 3796, 'host': 2073, 'rally': 3534, 'vote': 4745, 'queens': 3509, 'round': 3750, 'billboard': 445, 'dogs': 1270, 'lion': 2541, 'hero': 2012, 'tv': 4592, 'mens': 2744, 'quartz': 3506, 'wrist': 4921, 'sport': 4125, 'blue': 499, 'nylon': 3053, 'rea': 3561, 'swiss': 4302, 'date': 1089, 'woman': 4884, 'rubber': 3762, 'da': 1061, 'videoveranomtv': 4716, 'mtvhottest': 2882, 'britney': 573, 'del': 1130, 'rey': 3684, 'jackson': 2292, 'hope': 2063, 'satan': 3810, 'wwi': 4926, 'wwii': 4927, 'japanese': 2299, 'navy': 2938, 'military': 2795, 'japan': 2298, 'leather': 2478, 'war': 4761, 'mido': 2785, 'richard': 3688, 'learn': 2475, 'ap': 189, 'violent': 4728, 'control': 966, 'killing': 2394, 'jewish': 2317, 'terrorist': 4382, 'charge': 752, 'historic': 2035, 'arson': 244, 'ugly': 4612, 'truth': 4569, 'authority': 304, 'struggle': 4219, 'solve': 4073, 'hamilton': 1938, 'attend': 286, 'protest': 3476, 'add': 46, 'squad': 4137, 'fake': 1554, 'crime': 1021, 'lesbian': 2499, 'times': 4441, 'link': 2539, 'catch': 710, 'northern': 3020, 'newsintweet': 2977, 'mourning': 2872, 'notice': 3026, 'stab': 4144, 'stir': 4181, 'grief': 1889, 'sound': 4090, 'gay': 1782, 'admit': 53, 'scheme': 3829, 'lgbt': 2510, 'trial': 4541, 'palestinian': 3178, 'amid': 137, 'kisii': 2403, 'hunt': 2102, 'fail': 1550, 'plot': 3335, 'countynews': 992, 'mariah': 2665, 'thick': 4408, 'shoulder': 3962, 'shira': 3949, 'pray': 3393, 'treat': 4531, 'advocate': 63, 'magazine': 2623, 'tennessee': 4373, 'white': 4830, 'mc': 2704, 'arsonistmusic': 246, 'alleged': 114, 'east': 1347, 'bay': 386, 'serial': 3893, 'arrest': 239, 'minor': 2816, 'calif': 647, 'localarsonist': 2561, 'diamorfiend': 1197, 'legal': 2486, 'system': 4311, 'forget': 1689, 'casperrmg': 702, 'dick': 1200, 'anything': 181, 'nice': 2989, 'sit': 4007, 'vegetarian': 4694, 'popular': 3364, 'restaurant': 3664, 'headline': 1980, 'capture': 680, 'remove': 3633, 'american': 135, 'beach': 391, 'ca': 633, 'burning': 618, 'seek': 3870, 'trick': 4544, 'nasty': 2926, 'town': 4496, 'salem': 3792, 'melt': 2734, 'ice': 2117, 'blame': 470, 'blaze': 473, 'plastic': 3321, 'recycle': 3595, 'business': 623, 'produce': 3449, 'lmfao': 2555, 'not': 3023, 'thinking': 4412, 'newyork': 2979, 'crack': 1002, 'drink': 1299, 'green': 1881, 'court': 995, 'ass': 265, 'real': 3567, 'frank': 1717, 'elliott': 1401, 'guilty': 1909, 'terrorism': 4381, 'invite': 2260, 'brigade': 564, 'telegraph': 4363, 'big': 439, 'true': 4562, 'story': 4191, 'miss': 2823, 'stay': 4167, 'civil': 816, 'liberty': 2515, 'constant': 953, 'credit': 1015, 'inspire': 2218, 'rediscover': 3599, 'tbt': 4341, 'nashville': 2925, 'theater': 4401, 'gun': 1911, 'grabber': 1862, 'demand': 1138, 'ûïhatchet': 4991, 'terror': 4380, 'post': 3375, 'udhampur': 4609, 'demi': 1139, 'million': 2799, 'nuclear': 3039, 'claim': 820, 'suicide': 4244, 'bombing': 518, 'saudi': 3816, 'mosque': 2863, 'twitter': 4600, 'feminist': 1603, 'horrific': 2069, 'muslim': 2900, 'italy': 2285, 'blast': 472, 'accuse': 25, 'karachi': 2369, 'mumbai': 2890, 'militant': 2794, 'psychological': 3485, 'spos': 4128, 'injure': 2202, 'obama': 3057, 'weapon': 4794, 'texas': 4387, 'provide': 3479, 'acid': 28, 'private': 3440, 'steal': 4169, 'user': 4671, 'internet': 2246, 'israeli': 2281, 'force': 1682, 'raid': 3526, 'allege': 113, 'palestine': 3177, 'goat': 1834, 'rocket': 3725, 'okay': 3089, 'lie': 2519, 'senior': 3883, 'left': 2483, 'diamond': 1195, 'graveyard': 1874, 'shift': 3946, 'defend': 1124, 'idiot': 2128, 'volleyball': 4741, 'ii': 2140, 'training': 4512, 'machine': 2616, 'simulation': 3988, 'yet': 4947, 'direct': 1216, 'response': 3660, 'harper': 1960, 'yeah': 4939, 'premier': 3404, 'theatre': 4402, 'homeless': 2053, 'damn': 1073, 'latest': 2453, 'india': 2180, 'evidence': 1501, 'pak': 3174, 'oth': 3145, 'illegal': 2144, 'release': 3624, 'dhs': 1191, 'rape': 3542, 'maria': 2664, 'prior': 3435, 'strongly': 4216, 'condemn': 934, 'team': 4350, 'act': 34, 'simply': 3986, 'atomic': 282, 'bomb': 516, 'fat': 1574, 'group': 1893, 'gop': 1847, 'prevention': 3421, 'yay': 4936, 'nwo': 3049, 'anxiety': 176, 'nickcocofree': 2992, 'juliedicaro': 2354, 'attacked': 284, 'female': 1602, 'somehow': 4076, 'medium': 2724, 'civilian': 817, 'gaza': 1783, 'complete': 918, 'exercise': 1514, 'greece': 1878, 'christian': 798, 'muslims': 2901, 'temple': 4369, 'mount': 2869, 'pamela': 3183, 'geller': 1788, 'unit': 4635, 'responsible': 3662, 'youth': 4957, 'southern': 4098, 'patrick': 3214, 'cab': 634, 'ik': 2142, 'troll': 4551, 'pol': 3348, 'literally': 2546, 'abuse': 16, 'person': 3251, 'kelly': 2379, 'racist': 3520, 'donald': 1274, 'trump': 4565, 'view': 4719, 'chill': 786, 'answer': 169, 'robot': 3721, 'earn': 1341, 'bitcoin': 461, 'tweet': 4593, 'program': 3454, 'fly': 1665, 'uranium': 4662, 'mine': 2805, 'fukushima': 1750, 'dig': 1209, 'avalanche': 309, 'defense': 1126, 'louis': 2595, 'kalle': 2367, 'mattson': 2697, 'men': 2743, 'shirt': 3950, 'cotton': 984, 'nhl': 2985, 'hockey': 2042, 'tix': 4450, 'frozen': 1741, 'fury': 1760, 'king': 2401, 'row': 3754, 'aa': 0, 'mgm': 2769, 'grand': 1866, 'bet': 424, 'box': 536, 'piece': 3287, 'write': 4922, 'appreciate': 208, 'upper': 4658, 'deck': 1114, 'musician': 2898, 'recreate': 3594, 'classic': 824, 'album': 105, 'cover': 997, 'clever': 834, 'music': 2897, 'beautiful': 397, 'sweet': 4296, 'faith': 1553, 'rose': 3745, 'calgary': 646, 'flame': 1646, 'col': 874, 'inner': 2208, 'communication': 909, 'respect': 3657, 'create': 1013, 'worth': 4908, 'len': 2497, 'poplar': 3363, 'fully': 1752, 'tumble': 4580, 'band': 348, 'original': 3138, 'em': 1404, 'performance': 3245, 'gas': 1779, 'chevy': 776, 'tahoe': 4318, 'tank': 4329, 'chevrolet': 775, 'lift': 2523, 'favorite': 1583, 'deal': 1100, 'rudd': 3764, 'gordon': 1849, 'prince': 3432, 'google': 1846, 'purchase': 3494, 'rapper': 3545, 'song': 4082, 'yo': 4948, 'wide': 4839, 'jersey': 2313, 'bts': 593, 'incredible': 2175, 'listen': 2545, 'lt': 2604, 'wd': 4787, 'pickup': 3284, 'premium': 3406, 'star': 4155, 'wars': 4769, 'power': 3385, 'jedi': 2308, 'battle': 383, 'droid': 1304, 'hasbro': 1964, 'general': 1790, 'bull': 607, 'hampshire': 1939, 'letter': 2506, 'dragon': 1288, 'tomato': 4465, 'playlist': 3327, 'assistant': 270, 'bat': 376, 'chat': 761, 'ya': 4934, 'clip': 842, 'cast': 703, 'tb': 4340, 'space': 4101, 'occur': 3068, 'fleet': 1651, 'trailer': 4510, 'hole': 2045, 'saturn': 3815, 'cbsbigbrother': 718, 'rap': 3541, 'internal': 2243, 'external': 1536, 'item': 2286, 'warcraft': 4762, 'young': 4954, 'german': 1804, 'engage': 1434, 'marvel': 2682, 'dc': 1095, 'avenger': 311, 'realization': 3571, 'attention': 288, 'mass': 2686, 'painting': 3172, 'playing': 3326, 'interested': 2239, 'australia': 300, 'gallipoli': 1768, 'gary': 1778, 'son': 4081, 'electronic': 1392, 'celebration': 726, 'fedex': 1593, 'transport': 4520, 'bioterror': 452, 'germ': 1803, 'anthrax': 172, 'lab': 2431, 'mishap': 2822, 'usatoday': 4669, 'pathogens': 3210, 'jacksonville': 2293, 'potential': 3378, 'pathogen': 3209, 'tcot': 4344, 'certain': 736, 'research': 3651, 'page': 3169, 'sanction': 3801, 'rickperry': 3691, 'cut': 1053, 'foxnew': 1709, 'gopdebate': 1848, 'amp': 140, 'commerce': 903, 'hold': 2044, 'hearing': 1990, 'cdc': 722, 'willing': 4849, 'specimen': 4113, 'apple': 204, 'hmm': 2038, 'problem': 3445, 'select': 3875, 'agent': 83, 'deliver': 1133, 'atlanta': 278, 'chronicle': 802, 'manufacture': 2658, 'garden': 1776, 'breakingnew': 553, 'scoop': 3837, 'concern': 928, 'facility': 1545, 'bioterrorism': 453, 'sir': 4000, 'fema': 1601, 'region': 3611, 'iii': 2141, 'nasa': 2922, 'launch': 2456, 'carry': 694, 'handle': 1943, 'procedure': 3446, 'infectious': 2193, 'collude': 885, 'wht': 4835, 'usagov': 4668, 'auth': 302, 'hostage': 2074, 'blk': 488, 'withbioterrorism': 4869, 'lgl': 2511, 'idis': 2129, 'tale': 4321, 'virus': 4733, 'public': 3488, 'health': 1986, 'outbreak': 3153, 'homeland': 2052, 'security': 3865, 'wire': 4864, 'cia': 805, 'hollywood': 2048, 'daughter': 1091, 'aka': 99, 'fold': 1669, 'extra': 1537, 'yes': 4945, 'college': 880, 'difficult': 1207, 'volunteer': 4743, 'preparedness': 3410, 'drill': 1298, 'simulate': 3987, 'running': 3770, 'pretty': 3419, 'list': 2544, 'gm': 1829, 'scientist': 3835, 'regional': 3612, 'special': 4108, 'ebola': 1355, 'exist': 1515, 'suck': 4239, 'describe': 1163, 'future': 1761, 'therapy': 4405, 'fbi': 1586, 'abc': 5, 'prosecute': 3470, 'kidnap': 2389, 'afp': 72, 'irandeal': 2265, 'activity': 38, 'threat': 4419, 'cdcgov': 723, 'ahead': 88, 'rio': 3703, 'olympic': 3097, 'test': 4384, 'event': 1493, 'pool': 3359, 'niggas': 2998, 'dey': 1190, 'wan': 4759, 'tour': 4489, 'pendleton': 3234, 'media': 2718, 'cleveland': 833, 'living': 2551, 'apt': 214, 'jays': 2303, 'dutch': 1330, 'slave': 4031, 'hair': 1930, 'poverty': 3384, 'nee': 2953, 'fade': 1549, 'weekend': 4804, 'loss': 2591, 'nearly': 2950, 'old': 3094, 'rage': 3525, 'firefighter': 1635, 'condo': 936, 'que': 3507, 'hack': 1921, 'dem': 1137, 'detain': 1177, 'americans': 136, 'ugh': 4611, 'put': 3499, 'calorie': 651, 'pizza': 3307, 'pic': 3279, 'fort': 1696, 'computer': 925, 'mad': 2618, 'draw': 1293, 'facebook': 1544, 'feature': 1590, 'customer': 1052, 'experience': 1522, 'shout': 3963, 'hottest': 2078, 'farm': 1567, 'huh': 2092, 'leo': 2498, 'sale': 3791, 'palm': 3181, 'smooth': 4054, 'asf': 253, 'agree': 86, 'bright': 565, 'blazing': 474, 'fireman': 1636, 'party': 3201, 'wedding': 4798, 'vibez': 4706, 'nowplaying': 3032, 'trail': 4509, 'marketing': 2675, 'morgan': 2860, 'silver': 3983, 'dollar': 1271, 'gem': 1789, 'bu': 595, 'dmpl': 1261, 'cameo': 655, 'rev': 3676, 'ms': 2879, 'grade': 1864, 'bowl': 535, 'skill': 4020, 'nothing': 3025, 'silence': 3981, 'swarm': 4290, 'online': 3104, 'fm': 1666, 'inch': 2168, 'master': 2690, 'vocal': 4737, 'she': 3934, 'ash': 254, 'wo': 4879, 'lady': 2435, 'delta': 1134, 'expose': 1533, 'leak': 2473, 'picture': 3286, 'oh': 3083, 'race': 3519, 'temperature': 4368, 'roof': 3740, 'fifth': 1616, 'spanish': 4104, 'word': 4891, 'color': 886, 'dummy': 1325, 'freeze': 1727, 'cold': 875, 'enough': 1445, 'degree': 1129, 'mid': 2781, 'sun': 4248, 'rn': 3714, 'sweat': 4292, 'bullet': 608, 'fam': 1559, 'beyond': 432, 'blessed': 478, 'dude': 1321, 'fantasy': 1565, 'suit': 4245, 'bag': 336, 'bother': 528, 'tune': 4582, 'rare': 3547, 'proof': 3464, 're': 3560, 'etisalat': 1484, 'bitch': 460, 'cake': 644, 'throw': 4423, 'physical': 3277, 'ways': 4784, 'bleed': 475, 'ground': 1892, 'chicken': 781, 'nugget': 3041, 'apologize': 198, 'bleeding': 476, 'joe': 2324, 'gel': 1787, 'nose': 3022, 'bunch': 615, 'text': 4388, 'hug': 2089, 'apparently': 201, 'fine': 1629, 'rub': 3761, 'tear': 4353, 'yea': 4938, 'vampiro': 4688, 'broken': 579, 'foot': 1676, 'toe': 4459, 'step': 4173, 'glass': 1821, 'pun': 3491, 'pain': 3170, 'ear': 1339, 'cute': 1054, 'dinner': 1214, 'til': 4437, 'cam': 654, 'safe': 3786, 'violence': 4727, 'concrete': 933, 'liberal': 2514, 'favourite': 1584, 'character': 751, 'artist': 250, 'brain': 542, 'soccer': 4062, 'apartment': 191, 'he': 1977, 'pileup': 3290, 'seeing': 3869, 'te': 4346, 'timeline': 4440, 'blow': 495, 'blew': 480, 'instagram': 2220, 'active': 37, 'line': 2537, 'jazz': 2304, 'entire': 1451, 'solar': 4066, 'prob': 3442, 'cup': 1044, 'will': 4845, 'gtfo': 1901, 'notification': 3027, 'encourage': 1426, 'lolol': 2574, 'universe': 4639, 'max': 2698, 'tf': 4389, 'rick': 3690, 'ty': 4602, 'realize': 3572, 'snapchat': 4057, 'gambit': 1769, 'ironically': 2273, 'michele': 2774, 'hoax': 2039, 'catfish': 711, 'whistle': 4828, 'wind': 4852, 'skirt': 4025, 'everywhere': 1500, 'ray': 3554, 'rice': 3686, 'fiasco': 1614, 'skin': 4022, 'honey': 2059, 'recommend': 3589, 'conservative': 949, 'sean': 3856, 'zayn': 4969, 'pair': 3173, 'fighting': 1619, 'blight': 481, 'there': 4406, 'theory': 4404, 'magister': 2626, 'dwarf': 1336, 'cycle': 1056, 'reward': 3682, 'search': 3857, 'require': 3648, 'jack': 2291, 'deep': 1120, 'load': 2558, 'welfare': 4809, 'sponge': 4122, 'society': 4065, 'secure': 3864, 'bank': 352, 'heights': 2000, 'shaker': 3920, 'info': 2197, 'beer': 402, 'anti': 173, 'loan': 2559, 'housing': 2083, 'level': 2508, 'iclown': 2121, 'infinity': 2195, 'achieve': 26, 'develop': 1187, 'matter': 2695, 'author': 303, 'sexual': 3913, 'swear': 4291, 'result': 3667, 'app': 199, 'community': 910, 'policy': 3351, 'ohio': 3085, 'vacant': 4683, 'land': 2438, 'anellatulip': 150, 'open': 3114, 'gate': 1780, 'actual': 41, 'origin': 3137, 'refugee': 3603, 'tragedy': 4507, 'shame': 3923, 'article': 248, 'metro': 2763, 'detroit': 1182, 'estimate': 1480, 'square': 4138, 'art': 247, 'cunt': 1043, 'rank': 3539, 'card': 683, 'amazon': 132, 'print': 3434, 'blizzard': 485, 'blizzarddraco': 486, 'nut': 3046, 'cream': 1012, 'rock': 3723, 'dq': 1285, 'ashayo': 255, 'hi': 2017, 'dolphin': 1272, 'superstition': 4254, 'roll': 3734, 'caution': 713, 'though': 4416, 'peanut': 3230, 'butter': 627, 'mic': 2772, 'controller': 967, 'announcement': 164, 'window': 4853, 'idea': 2125, 'holy': 2050, 'trinity': 4547, 'wod': 4881, 'disappoint': 1224, 'hardcore': 1955, 'candy': 672, 'lonewolffur': 2579, 'clutch': 852, 'going': 1837, 'to': 4455, 'bout': 534, 'tho': 4414, 'hearthstone': 1993, 'pc': 3223, 'thought': 4417, 'batter': 380, 'technology': 4358, 'regret': 3614, 'dryer': 1315, 'paper': 3188, 'jeans': 2306, 'pocket': 3344, 'bruh': 587, 'nah': 2913, 'cook': 969, 'wall': 4757, 'floor': 1658, 'nail': 2914, 'morning': 2861, 'ah': 87, 'hurt': 2106, 'kindle': 2398, 'associate': 271, 'fragile': 1715, 'wounded': 4912, 'tattoo': 4335, 'donate': 1275, 'grandpa': 1869, 'smh': 4048, 'pressure': 3417, 'monitor': 2850, 'standard': 4153, 'large': 2446, 'bp': 540, 'cuff': 1041, 'stressful': 4207, 'relate': 3619, 'shoe': 3954, 'super': 4251, 'excellent': 1508, 'porridge': 3368, 'seriously': 3896, 'intend': 2235, 'hoe': 2043, 'drug': 1311, 'drown': 1309, 'document': 1267, 'somebody': 4075, 'ta': 4312, 'fuckin': 1746, 'sad': 3782, 'encounter': 1425, 'jesus': 2314, 'personal': 3252, 'royal': 3755, 'shed': 3935, 'innocent': 2210, 'sin': 3990, 'abortion': 13, 'se': 3853, 'lightning': 2530, 'everyday': 1497, 'aggressive': 84, 'enter': 1449, 'email': 1405, 'jay': 2302, 'awful': 323, 'xxx': 4933, 'nightmare': 3000, 'stephen': 4174, 'tower': 4495, 'bs': 592, 'invasion': 2255, 'insomnia': 2216, 'grrrr': 1898, 'ga': 1763, 'tokyo': 4462, 'dare': 1083, 'mary': 2683, 'tired': 4445, 'sexy': 3914, 'drool': 1306, 'effect': 1373, 'low': 2599, 'product': 3450, 'diarrhea': 1199, 'floyd': 1662, 'money': 2849, 'elbow': 1383, 'anytime': 182, 'anywhere': 185, 'landscape': 2441, 'oil': 3086, 'paint': 3171, 'bark': 361, 'hello': 2005, 'mega': 2730, 'sink': 3997, 'juice': 2350, 'meet': 2727, 'friday': 1733, 'suppose': 4258, 'arm': 231, 'chair': 742, 'drama': 1291, 'killer': 2393, 'monday': 2848, 'marlon': 2677, 'williams': 4847, 'steel': 4171, 'shuffle': 3967, 'rather': 3550, 'mystery': 2909, 'blown': 497, 'bedroom': 400, 'deactivate': 1097, 'manutd': 2659, 'wow': 4913, 'club': 850, 'player': 3325, 'inning': 2209, 'probably': 3443, 'value': 4687, 'strike': 4211, 'lmfaoooo': 2556, 'crew': 1018, 'ur': 4661, 'mini': 2810, 'bump': 613, 'approval': 211, 'soldier': 4067, 'panic': 3186, 'mind': 2804, 'website': 4797, 'todd': 4457, 'blake': 469, 'ee': 1370, 'shop': 3959, 'glad': 1820, 'half': 1932, 'anyways': 184, 'pattern': 3216, 'utterly': 4680, 'mutant': 2903, 'ankle': 158, 'seat': 3859, 'butt': 626, 'syndrome': 4308, 'toilet': 4461, 'whoever': 4831, 'grab': 1861, 'nicki': 2993, 'minaj': 2803, 'al': 100, 'as': 252, 'charm': 758, 'bash': 369, 'bottle': 529, 'tote': 4485, 'handbag': 1941, 'faux': 1580, 'fashion': 1571, 'purse': 3496, 'republicans': 3646, 'genuine': 1798, 'messenger': 2754, 'oak': 3056, 'buffalo': 602, 'vuitton': 4748, 'monogram': 2853, 'sophie': 4084, 'limited': 2536, 'edition': 1363, 'healthy': 1988, 'backpack': 331, 'tan': 4328, 'bath': 378, 'along': 121, 'corner': 977, 'thin': 4409, 'hip': 2032, 'hobo': 2041, 'women': 4885, 'cage': 641, 'vintage': 4724, 'satchel': 3811, 'zip': 4975, 'style': 4232, 'coach': 858, 'camera': 656, 'bid': 437, 'fit': 1640, 'tablet': 4315, 'jp': 2346, 'bagging': 337, 'drake': 1290, 'meek': 2725, 'tea': 4347, 'meekmill': 2726, 'rec': 3580, 'league': 2472, 'tough': 4488, 'philly': 3269, 'skim': 4021, 'straight': 4193, 'friendship': 1736, 'together': 4460, 'johnson': 2330, 'score': 3838, 'definitely': 1128, 'spot': 4129, 'ghostwriter': 1810, 'diss': 1244, 'flow': 1660, 'mark': 2670, 'mill': 2798, 'patient': 3212, 'record': 3590, 'file': 1621, 'hop': 2062, 'freshman': 1731, 'my': 2906, 'stacey': 4145, 'foxnews': 1710, 'dangerous': 1078, 'micom': 2777, 'contrast': 965, 'po': 3343, 'bestseller': 423, 'bags': 339, 'cartoon': 697, 'womens': 4886, 'buckle': 598, 'casual': 705, 'stylish': 4233, 'fossil': 1701, 'es': 1473, 'appear': 202, 'hall': 1934, 'charlotte': 757, 'concert': 930, 'variety': 4691, 'pack': 3168, 'admin': 51, 'probe': 3444, 'dept': 1156, 'football': 1678, 'locker': 2566, 'room': 3741, 'flower': 1661, 'chain': 741, 'small': 4043, 'institute': 2225, 'chairman': 743, 'russian': 3774, 'va': 4682, 'scottwalker': 3842, 'middle': 2782, 'typical': 4605, 'giant': 1811, 'sized': 4016, 'garbage': 1775, 'rise': 3709, 'colour': 888, 'congress': 942, 'leg': 2484, 'heal': 1984, 'you䞻ll': 4959, 'smile': 4049, 'geometric': 1799, 'totes': 4486, 'investigate': 2257, 'washington': 4772, 'waist': 4752, 'solid': 4069, 'coffee': 871, 'positive': 3372, 'beforeitsnews': 404, 'derivative': 1162, 'hiroshima': 2034, 'drop': 1307, 'guardian': 1903, 'legacy': 2485, 'editorial': 1365, 'cas': 698, 'fantastic': 1564, 'anyway': 183, 'location': 2563, 'gunman': 1913, 'marks': 2676, 'board': 509, 'ahh': 89, 'baby': 328, 'mail': 2629, 'dat': 1087, 'concept': 927, 'enemy': 1432, 'impact': 2152, 'kyle': 2429, 'flat': 1648, 'thursday': 4431, 'anniversary': 162, 'united': 4637, 'hatchet': 1967, 'wielding': 4841, 'pepper': 3239, 'spray': 4130, 'oops': 3111, 'miles': 2793, 'coast': 861, 'mf': 2767, 'amazing': 131, 'hutch': 2107, 'centre': 734, 'bluejays': 502, 'bell': 411, 'invade': 2253, 'libya': 2517, 'africa': 74, 'screenshot': 3850, 'islamic': 2278, 'syria': 4309, 'turkey': 4585, 'armed': 233, 'kanye': 2368, 'majority': 2635, 'jet': 2315, 'main': 2630, 'plummet': 3338, 'repeat': 3640, 'turkish': 4586, 'village': 4720, 'wound': 4911, 'bathroom': 379, 'yr': 4962, 'within': 4871, 'third': 4413, 'generation': 1793, 'democracy': 1140, 'cos': 980, 'germany': 1805, 'poland': 3349, 'harbor': 1953, 'crude': 1031, 'uncle': 4621, 'hopefully': 2064, 'afghanistan': 71, 'kurtschlichter': 2427, 'specific': 4112, 'traditional': 4504, 'nagasaki': 2912, 'forever': 1687, 'recover': 3592, 'accord': 23, 'lauren': 2458, 'recently': 3585, 'feed': 1595, 'pkk': 3309, 'obviously': 3064, 'flee': 1650, 'robert': 3718, 'belly': 412, 'teamstream': 4352, 'pass': 3202, 'yrs': 4963, 'gif': 1812, 'background': 330, 'states': 4164, 'ri': 3685, 'worse': 4905, 'hurricane': 2104, 'dive': 1249, 'daesh': 1063, 'network': 2968, 'cryptic': 1035, 'guide': 1907, 'mission': 2825, 'schedule': 3828, 'november': 3029, 'moral': 2859, 'toll': 4463, 'entrepreneur': 1452, 'nbcnews': 2943, 'justify': 2364, 'bill': 444, 'clinton': 841, 'nato': 2931, 'generalnews': 1792, 'marshall': 2681, 'dambisa': 1071, 'prosyn': 3471, 'ope': 3113, 'shaikh': 3918, 'logic': 2570, 'bridge': 561, 'ashes': 257, 'unfold': 4629, 'trent': 4539, 'cricket': 1020, 'cement': 729, 'england': 1437, 'bundle': 616, 'central': 733, 'relationship': 3621, 'btwn': 594, 'caribbean': 689, 'five': 1642, 'natural': 2932, 'calamity': 645, 'crane': 1005, 'nearby': 2949, 'icymi': 2123, 'react': 3563, 'warne': 4767, 'shock': 3953, 'epic': 1457, 'johannesburg': 2326, 'aug': 297, 'ani': 155, 'legendary': 2487, 'australian': 301, 'fox': 1708, 'mp': 2876, 'distance': 1245, 'aussie': 299, 'batting': 382, 'stuff': 4228, 'engvaus': 1441, 'gadget': 1765, 'currently': 1049, 'likely': 2533, 'construction': 955, 'restore': 3665, 'rescuer': 3650, 'upgrade': 4655, 'situation': 4011, 'sustainability': 4283, 'oooooohhhh': 3110, 'oooh': 3109, 'ooh': 3108, 'ed': 1358, 'deserve': 1166, 'upset': 4659, 'etc': 1482, 'minimehh': 2811, 'chris': 796, 'joyner': 2345, 'overlook': 3162, 'blacklive': 466, 'neighbor': 2958, 'boxer': 537, 'outta': 3160, 'mentally': 2746, 'rocky': 3727, 'threaten': 4420, 'messi': 2755, 'basic': 371, 'maintenance': 2632, 'riot': 3704, 'loot': 2585, 'infosec': 2199, 'often': 3082, 'schools': 3832, 'western': 4813, 'uganda': 4610, 'pledge': 3332, 'ruin': 3766, 'disgusting': 1234, 'leed': 2482, 'jealous': 2305, 'object': 3059, 'chase': 760, 'weak': 4789, 'winston': 4860, 'outrage': 3158, 'rockbottomradfm': 3724, 'challenge': 744, 'rescue': 3649, 'soak': 4061, 'alcohol': 106, 'gbbo': 1784, 'store': 4188, 'witness': 4874, 'explode': 1527, 'afternoon': 77, 'equipment': 1466, 'grateful': 1872, 'crowd': 1029, 'hazard': 1973, 'installation': 2221, 'non': 3013, 'clad': 819, 'www': 4928, 'cbp': 716, 'lawyer': 2465, 'mitigation': 2830, 'et': 1481, 'multiple': 2888, 'downtown': 1283, 'hinton': 2031, 'ton': 4468, 'ross': 3746, 'several': 3908, 'bust': 625, 'brick': 559, 'seven': 3907, 'different': 1206, 'architecture': 221, 'dam': 1068, 'estate': 1478, 'evacuate': 1489, 'confirmation': 940, 'grenade': 1886, 'sweden': 4294, 'grill': 1890, 'insurance': 2229, 'displace': 1239, 'troy': 4558, 'fourth': 1706, 'in': 2165, 'alarm': 102, 'co': 857, 'tip': 4443, 'expensive': 1521, 'remain': 3629, 'residential': 3654, 'prisoner': 3438, 'nazi': 2939, 'camp': 660, 'seize': 3874, 'confirm': 939, 'brother': 583, 'drift': 1297, 'progress': 3456, 'content': 961, 'derby': 1161, 'shower': 3966, 'flush': 1664, 'barn': 362, 'moon': 2858, 'cheese': 766, 'touch': 4487, 'plate': 3322, 'joke': 2333, 'fr': 1713, 'op': 3112, 'calories': 652, 'mph': 2877, 'brisk': 568, 'pace': 3166, 'myfitnesspal': 2908, 'twice': 4596, 'weight': 4806, 'mcdonald': 2705, 'boyfriend': 539, 'clothe': 847, 'crap': 1006, 'figure': 1620, 'complex': 920, 'acre': 32, 'contain': 959, 'gettin': 1807, 'social': 4063, 'fool': 1675, 'coverage': 998, 'pro': 3441, 'material': 2693, 'dirt': 1220, 'bike': 442, 'spark': 4105, 'brush': 590, 'all': 111, 'bug': 604, 'candle': 671, 'nap': 2920, 'at': 276, 'keyboard': 2386, 'letsfootball': 2505, 'skanndtyagi': 4018, 'fed': 1591, 'that': 4399, 'skinny': 4023, 'workout': 4897, 'john': 2327, 'except': 1509, 'idk': 2130, 'zionism': 4973, 'coil': 872, 'certainly': 737, 'er': 1467, 'return': 3671, 'method': 2760, 'muscle': 2895, 'shape': 3927, 'desire': 1168, 'goal': 1833, 'lou': 2593, 'demon': 1144, 'boat': 510, 'memorie': 2739, 'stack': 4146, 'apply': 206, 'parent': 3190, 'counter': 988, 'jerry': 2312, 'press': 3416, 'conference': 938, 'you䞻re': 4960, 'understand': 4625, 'soup': 4092, 'diet': 1203, 'recipe': 3586, 'ticket': 4433, 'accidentally': 21, 'parenthood': 3191, 'demonstration': 1146, 'pp': 3388, 'strut': 4220, 'bg': 434, 'ashe': 256, 'stone': 4186, 'individual': 2185, 'rioter': 3705, 'looter': 2586, 'tech': 4354, 'flip': 1653, 'table': 4314, 'screw': 3851, 'ppl': 3389, 'magic': 2625, 'proud': 3477, 'attempt': 285, 'amongst': 139, 'description': 1164, 'retweet': 3672, 'monster': 2855, 'port': 3369, 'cancer': 670, 'puppy': 3493, 'contemplate': 960, 'striker': 4212, 'chinese': 788, 'kitten': 2407, 'murderer': 2893, 'express': 1534, 'mistreat': 2829, 'drinking': 1300, 'happening': 1948, 'bush': 622, 'volcano': 4739, 'winter': 4861, 'sydney': 4305, 'ted': 4359, 'ûïwe': 4995, 'mitt': 2831, 'omfg': 3100, 'freaking': 1721, 'drought': 1308, 'fuels': 1749, 'seem': 3872, 'traumatise': 4525, 'hillary': 2029, 'standwithpp': 4154, 'holiday': 2046, 'relax': 3623, 'spain': 4103, 'swimming': 4300, 'water': 4776, 'scotland': 3840, 'wet': 4814, 'warming': 4765, 'catastrophic': 709, 'photographer': 3273, 'advise': 61, 'egg': 1375, 'casualty': 706, 'warfighting': 4763, 'reduce': 3600, 'ban': 346, 'premature': 3403, 'thankful': 4395, 'evil': 1502, 'afghan': 70, 'intensifie': 2236, 'sharp': 3931, 'honest': 2057, 'eventually': 1494, 'countless': 989, 'globe': 1826, 'reminder': 3632, 'account': 24, 'yemen': 4943, 'sorrow': 4085, 'shooter': 3957, 'operation': 3119, 'ally': 117, 'fix': 1643, 'reach': 3562, 'purple': 3495, 'thankfully': 4396, 'command': 900, 'wrap': 4917, 'warn': 4766, 'iraq': 2267, 'ûïthe': 4994, 'pave': 3219, 'hypocrisy': 2113, 'iranian': 2266, 'idc': 2124, 'washingtonpost': 4773, 'pertain': 3256, 'derailment': 1160, 'patna': 3213, 'hire': 2033, 'toronto': 4479, 'course': 994, 'rate': 3549, 'movement': 2874, 'commercial': 904, 'they': 4407, 'ûªd': 4984, 'cord': 976, 'ocean': 3069, 'gray': 1875, 'whale': 4818, 'population': 3366, 'pacific': 3167, 'endanger': 1429, 'prediction': 3400, 'quality': 3502, 'license': 2518, 'jobs': 2323, 'denver': 1152, 'arsenal': 243, 'bound': 531, 'chance': 747, 'tag': 4317, 'recall': 3581, 'industry': 2188, 'by': 631, 'september': 3891, 'metric': 2762, 'setlist': 3904, 'greenharvard': 1882, 'cancel': 669, 'warship': 4770, 'nytime': 3055, 'con': 926, 'strong': 4215, 'insurer': 2230, 'bore': 524, 'nuke': 3042, 'ten': 4371, 'mix': 2832, 'catastrophe': 708, 'waste': 4774, 'joy': 2344, 'suffer': 4243, 'pjnet': 3308, 'ccot': 720, 'interest': 2238, 'republic': 3644, 'continue': 964, 'failure': 1551, 'athlete': 277, 'teacher': 4349, 'principle': 3433, 'duty': 1332, 'iphone': 2263, 'ultimate': 4616, 'megyn': 2732, 'prevent': 3420, 'adoption': 54, 'peter': 3258, 'jukes': 2351, 'political': 3353, 'silent': 3982, 'prime': 3431, 'bless': 477, 'diff': 1204, 'barry': 364, 'solo': 4071, 'generally': 1791, 'company': 914, 'repair': 3638, 'emotional': 1413, 'mother': 2865, 'field': 1615, 'mutual': 2904, 'payday': 3221, 'promote': 3461, 'finance': 1626, 'imdb': 2150, 'hilarious': 2027, 'ceo': 735, 'temper': 4367, 'anymore': 179, 'perfectly': 3243, 'reckless': 3587, 'bottom': 530, 'regular': 3615, 'denali': 1147, 'wolf': 4882, 'finish': 1631, 'bbc': 388, 'studio': 4226, 'discussion': 1231, 'sheer': 3937, 'success': 4238, 'vegas': 4692, 'eruption': 1472, 'benefit': 417, 'acute': 43, 'ptsd': 3486, 'mistake': 2828, 'diplomacy': 1215, 'replace': 3641, 'newyorker': 2980, 'custom': 1051, 'manager': 2649, 'uniform': 4632, 'british': 571, 'wealthy': 4793, 'brit': 569, 'don': 1273, 'netanyahu': 2967, 'learning': 2476, 'yorker': 4951, 'satellite': 3812, 'excited': 1511, 'debt': 1108, 'audio': 296, 'stream': 4201, 'assume': 273, 'defendant': 1125, 'åç': 4980, 'clear': 829, 'åè': 4981, 'hazardous': 1974, 'chemical': 769, 'management': 2648, 'brothers': 584, 'armory': 234, 'sf': 3915, 'chem': 768, 'illinois': 2145, 'nu': 3038, 'sharethis': 3929, 'hazmat': 1975, 'biological': 451, 'radioactive': 3524, 'nbc': 2942, 'md': 2709, 'cameron': 657, 'bee': 401, 'krefeld': 2424, 'underway': 4628, 'cnn': 856, 'instead': 2224, 'toxic': 4498, 'harm': 1959, 'spill': 4117, 'seattle': 3860, 'dependency': 1155, 'counselor': 986, 'intern': 2242, 'crews': 1019, 'pipe': 3297, 'cocaine': 866, 'marijuana': 2667, 'heroine': 2014, 'alert': 107, 'plant': 3320, 'term': 4377, 'obsess': 3063, 'cleanup': 828, 'hungry': 2100, 'fortunately': 1698, 'responder': 3659, 'pop': 3362, 'cliff': 837, 'proceed': 3447, 'neil': 2961, 'decent': 1111, 'humble': 2096, 'slip': 4039, 'dignity': 1212, 'chunk': 803, 'china': 787, 'brian': 558, 'ruebs': 3765, 'endures': 1431, 'climb': 840, 'bride': 560, 'groom': 1891, 'alex': 108, 'minion': 2813, 'favor': 1582, 'elite': 1398, 'balance': 343, 'snap': 4056, 'streak': 4200, 'lil': 2534, 'trip': 4548, 'landslide': 2442, 'ferry': 1608, 'knock': 2413, 'lower': 2600, 'edge': 1360, 'ha': 1919, 'stroke': 4214, 'easily': 1346, 'dublin': 1318, 'technique': 4357, 'paramedic': 3189, 'vinyl': 4725, 'yahoo': 4935, 'relive': 3628, 'economy': 1357, 'interview': 2249, 'talkradio': 4326, 'invest': 2256, 'strategy': 4199, 'wealth': 4792, 'blowout': 498, 'circus': 812, 'tent': 4375, 'enable': 1422, 'revenue': 3678, 'propose': 3469, 'absolutely': 15, 'image': 2148, 'correction': 979, 'sa': 3779, 'president': 3415, 'oral': 3129, 'wi': 4836, 'sham': 3922, 'growth': 1897, 'dry': 1314, 'bite': 462, 'pin': 3294, 'musik': 2899, 'bake': 341, 'dorret': 1279, 'chocolate': 789, 'gateau': 1781, 'jan': 2297, 'phil': 3265, 'gravel': 1873, 'angry': 154, 'burst': 619, 'lung': 2613, 'district': 1248, 'tongue': 4469, 'cum': 1042, 'breathe': 555, 'zimbabwe': 4972, 'sewer': 3910, 'heartless': 1994, 'whip': 4826, 'animal': 156, 'betray': 426, 'rid': 3692, 'heel': 1999, 'fav': 1581, 'greek': 1880, 'cost': 982, 'trusty': 4568, 'spontaneously': 4124, 'ep': 1456, 'collide': 881, 'ripple': 3708, 'tide': 4435, 'cigarette': 806, 'immediately': 2151, 'lock': 2564, 'awkward': 324, 'trek': 4535, 'pluto': 3341, 'choose': 793, 'shanghai': 3924, 'avenue': 312, 'thunderstorm': 4429, 'masse': 2688, 'singing': 3994, 'register': 3613, 'pierce': 3288, 'reality': 3570, 'strange': 4197, 'castle': 704, 'supernatural': 4253, 'electric': 1386, 'vancouver': 4690, 'tlc': 4451, 'suddenly': 4241, 'houston': 2084, 'powerful': 3386, 'coincide': 873, 'wattpad': 4780, 'either': 1380, 'cyclist': 1057, 'collided': 882, 'runner': 3769, 'roanoke': 3716, 'greenway': 1883, 'verdict': 4697, 'none': 3014, 'manage': 2647, 'selfie': 3878, 'newswatch': 2978, 'sts': 4221, 'emerg': 1408, 'however': 2085, 'quest': 3510, 'pleasure': 3331, 'gold': 1839, 'monsoon': 2854, 'suv': 4285, 'highway': 2022, 'wheeler': 4823, 'anna': 159, 'harwich': 1963, 'have': 1971, 'towards': 4493, 'horn': 2066, 'exchange': 1510, 'tyre': 4606, 'happily': 1949, 'commence': 901, 'fishing': 1639, 'dan': 1074, 'hughes': 2091, 'intersection': 2247, 'crashing': 1009, 'san': 3800, 'antonio': 175, 'stars': 4157, 'cart': 696, 'guard': 1902, 'bicyclist': 436, 'justin': 2365, 'bieber': 438, 'oo': 3107, 'harrybecareful': 1962, 'journey': 2343, 'cow': 999, 'victoria': 4711, 'collision': 884, 'broadway': 577, 'local': 2560, 'knob': 2412, 'sac': 3780, 'elkhorn': 1400, 'blvd': 506, 'trfc': 4540, 'enrt': 1447, 'rainier': 3531, 'franklin': 1718, 'eb': 1353, 'opp': 3121, 'enroute': 1446, 'ag': 80, 'springs': 4133, 'inj': 2201, 'rear': 3576, 'injured': 2203, 'spur': 4135, 'baltimore': 345, 'mchenry': 2706, 'tunnel': 4583, 'techesback': 4355, 'bernardino': 419, 'southbound': 4096, 'coal': 860, 'innovation': 2211, 'valley': 4686, 'tragic': 4508, 'occupant': 3067, 'sacramento': 3781, 'division': 1255, 'offramp': 3080, 'approach': 209, 'niagara': 2987, 'separate': 3889, 'writing': 4924, 'ûïyou': 4997, 'layout': 2467, 'meinl': 2733, 'cymbals': 1059, 'slam': 4028, 'lesson': 2501, 'lover': 2598, 'knight': 2411, 'alright': 126, 'cash': 700, 'source': 4093, 'fwy': 1762, 'rant': 3540, 'apparent': 200, 'playstation': 3329, 'prayer': 3394, 'feelin': 1597, 'photoshop': 3276, 'tool': 4472, 'heroin': 2013, 'liable': 2513, 'hills': 2030, 'download': 1282, 'tutorial': 4591, 'maj': 2633, 'muzzamil': 2905, 'mi': 2770, 'mansehra': 2655, 'microlight': 2778, 'uk': 4613, 'crashed': 1008, 'intact': 2232, 'lifts': 2524, 'glide': 1823, 'sea': 3854, 'euro': 1487, 'northwest': 3021, 'tuesday': 4579, 'log': 2568, 'bear': 394, 'doctor': 1266, 'type': 4603, 'ant': 170, 'note': 3024, 'honestly': 2058, 'crush': 1032, 'wce': 4785, 'fez': 1612, 'gorgeous': 1850, 'vine': 4723, 'wen': 4811, 'holland': 2047, 'wrapup': 4918, 'cable': 637, 'disney': 1236, 'jimmy': 2320, 'fallon': 1557, 'squirrel': 4140, 'bio': 450, 'worstsummerjob': 4907, 'stress': 4205, 'bluejay': 501, 'gym': 1918, 'clearly': 830, 'priority': 3436, 'christ': 797, 'km': 2408, 'nike': 3001, 'sportwatch': 4127, 'gp': 1858, 'reap': 3575, 'nick': 2990, 'agency': 82, 'jones': 2337, 'empire': 1416, 'slate': 4030, 'wine': 4857, 'curfew': 1047, 'fighter': 1618, 'aim': 92, 'impose': 2157, 'shut': 3968, 'campus': 665, 'teenager': 4362, 'violation': 4726, 'stuck': 4224, 'brief': 562, 'wnd': 4878, 'cld': 826, 'inst': 2219, 'apch': 193, 'rwy': 3776, 'oper': 3117, 'taxiway': 4338, 'foxtrot': 1711, 'navbl': 2935, 'tmp': 4453, 'nope': 3016, 'tutor': 4590, 'grove': 1894, 'clock': 843, 'blind': 482, 'smoking': 4052, 'final': 1624, 'goodbye': 1845, 'cat': 707, 'appointment': 207, 'decide': 1112, 'freedom': 1723, 'fi': 1613, 'di': 1192, 'worried': 4903, 'unlocking': 4645, 'judge': 2349, 'dis': 1221, 'wonder': 4887, 'cyclone': 1058, 'annual': 166, 'photography': 3274, 'tropical': 4555, 'de': 1096, 'jhaustin': 2318, 'tribune': 4543, 'retract': 3669, 'pam': 3182, 'wordpress': 4892, 'condition': 935, 'severe': 3909, 'bulletin': 609, 'typhoon': 4604, 'hannaph': 1945, 'soudelor': 4088, 'calm': 650, 'ante': 171, 'ne': 2947, 'komen': 2418, 'devastate': 1184, 'tom': 4464, 'talent': 4322, 'ignite': 2136, 'phantom': 3261, 'asia': 258, 'station': 4165, 'bangladesh': 351, 'address': 50, 'spin': 4118, 'mercury': 2750, 'gt': 1899, 'cobra': 865, 'emperor': 1415, 'quarter': 3505, 'panel': 3185, 'maximum': 2699, 'gust': 1915, 'fate': 1578, 'wfp': 4816, 'rohingya': 3731, 'houses': 2082, 'cherry': 772, 'outfit': 3155, 'union': 4633, 'prepared': 3409, 'gta': 1900, 'fitness': 1641, 'knee': 2409, 'solution': 4072, 'phoenix': 3270, 'windy': 4856, 'advanced': 57, 'transit': 4519, 'drum': 1312, 'af': 64, 'tree': 4533, 'fence': 1605, 'homeowner': 2054, 'tor': 4475, 'enhance': 1442, 'key': 2385, 'opinion': 3120, 'lighting': 2528, 'nda': 2946, 'offer': 3075, 'trillion': 4546, 'crosse': 1028, 'instant': 2222, 'np': 3033, 'inc': 2166, 'inmate': 2207, 'cleric': 831, 'isis': 2276, 'model': 2841, 'pussy': 3498, 'asian': 259, 'devil': 1189, 'fluid': 1663, 'madison': 2621, 'ar': 217, 'foreign': 1685, 'initial': 2200, 'blizzheroe': 487, 'constantly': 954, 'frequently': 1729, 'danger': 1077, 'rockin': 3726, 'preview': 3422, 'claytonbryant': 825, 'artistsunite': 251, 'fish': 1638, 'preservation': 3414, 'african': 75, 'indian': 2181, 'govt': 1857, 'grain': 1865, 'anger': 153, 'fettilootch': 1610, 'slanglucci': 4029, 'oppression': 3125, 'slide': 4036, 'especially': 1477, 'uber': 4607, 'recent': 3584, 'afc': 65, 'underwater': 4627, 'cave': 714, 'diving': 1254, 'protection': 3474, 'spider': 4116, 'critical': 1024, 'rail': 3527, 'rest': 3663, 'complaint': 917, 'atmosphere': 280, 'forbid': 1681, 'goku': 1838, 'legit': 2493, 'luis': 2609, 'emmerdale': 1411, 'nd': 2945, 'attractive': 292, 'gameplay': 1771, 'hundred': 2098, 'migrant': 2787, 'capsize': 678, 'askcharley': 262, 'debate': 1105, 'perspective': 3255, 'study': 4227, 'trend': 4538, 'mikeparractor': 2790, 'bail': 340, 'before': 403, 'taiwan': 4319, 'val': 4685, 'sue': 4242, 'summerfate': 4247, 'bff': 433, 'rep': 3637, 'adult': 55, 'plague': 3312, 'console': 951, 'ring': 3702, 'ferguson': 1607, 'era': 1468, 'prez': 3425, 'hang': 1944, 'rss': 3759, 'recognize': 3588, 'certificate': 738, 'shitty': 3952, 'unto': 4652, 'arianagrande': 228, 'screamqueen': 3847, 'katherine': 2373, 'starve': 4161, 'bashes': 370, 'comparison': 915, 'cyprus': 1060, 'analysis': 142, 'mullah': 2885, 'omar': 3098, 'taliban': 4323, 'journalist': 2342, 'predict': 3399, 'cecilthelion': 725, 'ari': 226, 'legionnaires': 2491, 'deaths': 1104, 'measle': 2713, 'complication': 922, 'loose': 2584, 'legionnaire': 2490, 'bigamist': 440, 'pregnant': 3402, 'voter': 4746, 'shipwreck': 3948, 'insas': 2213, 'milk': 2796, 'worldwide': 4902, 'pedestrian': 3231, 'physician': 3278, 'dear': 1102, 'resistant': 3656, 'irony': 2274, 'usually': 4676, 'datum': 1090, 'focus': 1667, 'lunch': 2612, 'stewart': 4178, 'julian': 2352, 'annoying': 165, 'auto': 305, 'equate': 1465, 'blackpool': 467, 'kowing': 2422, 'adam': 45, 'minister': 2814, 'arabia': 218, 'related': 3620, 'increase': 2174, 'percent': 3241, 'walmart': 4758, 'average': 313, 'pls': 3336, 'th': 4392, 'malaysia': 2639, 'airlines': 95, 'flight': 1652, 'disappear': 1222, 'discover': 1227, 'wash': 4771, 'float': 1654, 'rì': 3778, 'york': 4950, 'surely': 4262, 'disappearance': 1223, 'novel': 3028, 'relative': 3622, 'discovery': 1228, 'malaysian': 2640, 'billneelynbc': 448, 'najib': 2915, 'belong': 413, 'interesting': 2240, 'deluge': 1135, 'nature': 2933, 'important': 2156, 'inevitable': 2190, 'worker': 4894, 'sympathy': 4306, 'wa': 4750, 'stock': 4183, 'despite': 1172, 'camping': 664, 'naked': 2916, 'embrace': 1406, 'torso': 4480, 'scale': 3821, 'unprecedented': 4646, 'client': 836, 'swing': 4301, 'period': 3247, 'shepherd': 3944, 'portion': 3370, 'surf': 4263, 'programme': 3455, 'dress': 1296, 'perhaps': 3246, 'truly': 4564, 'heaven': 1996, 'warm': 4764, 'evening': 1492, 'misha': 2821, 'collins': 883, 'teach': 4348, 'healing': 1985, 'vince': 4722, 'mcmahon': 2708, 'nfl': 2983, 'blessing': 479, 'peaceful': 3227, 'william': 4846, 'turner': 4589, 'darkness': 1085, 'invoice': 2261, 'hyderabad': 2111, 'rainfall': 3530, 'poorly': 3361, 'indie': 2183, 'abe': 7, 'deluged': 1136, 'quiz': 3515, 'senate': 3880, 'republican': 3645, 'junk': 2359, 'com': 890, 'accionempresa': 22, 'urs': 4665, 'hatred': 1969, 'patience': 3211, 'intensity': 2237, 'feeling': 1598, 'capital': 676, 'demolish': 1142, 'enugu': 1453, 'structure': 4218, 'international': 2245, 'architect': 220, 'birmingham': 455, 'patriot': 3215, 'workers': 4895, 'brady': 541, 'hahah': 1924, 'bal': 342, 'radar': 3521, 'boundary': 532, 'negative': 2956, 'midget': 2783, 'abbswinston': 4, 'zionist': 4974, 'jordan': 2339, 'freak': 1719, 'ex': 1504, 'assembly': 266, 'laundry': 2457, 'root': 3743, 'businessman': 624, 'xd': 4931, 'rihanna': 3701, 'partner': 3200, 'whether': 4825, 'engineer': 1436, 'folk': 1670, 'integrity': 2233, 'just': 2362, 'saddlebrooke': 3783, 'salvi': 3797, 'bistro': 458, 'former': 1694, 'palestinians': 3179, 'election': 1385, 'carlos': 690, 'demolition': 1143, 'davidvonderhaar': 1093, 'selection': 3876, 'freestyle': 1725, 'halifax': 1933, 'maintain': 2631, 'factor': 1547, 'psychiatric': 3484, 'frog': 1738, 'michigan': 2776, 'waterway': 4778, 'achimota': 27, 'fkn': 1644, 'med': 2717, 'comp': 913, 'cont': 957, 'giveaway': 1819, 'kit': 2405, 'kaduna': 2366, 'planning': 3319, 'widespread': 4840, 'jose': 2340, 'mr': 2878, 'derail': 1159, 'destiny': 1173, 'here': 2010, 'wmata': 4876, 'sms': 4055, 'birth': 456, 'bloomberg': 494, 'madhya': 2619, 'pradesh': 3392, 'gridlock': 1888, 'simultaneous': 3989, 'incredibly': 2176, 'necessary': 2951, 'dallas': 1067, 'tube': 4578, 'mood': 2857, 'smithsonian': 4050, 'campaign': 661, 'announce': 163, 'dozen': 1284, 'floods': 1657, 'italian': 2284, 'realdonaldtrump': 3568, 'democrats': 1141, 'recovery': 3593, 'unsuckdcmetro': 4651, 'modi': 2842, 'subject': 4235, 'federal': 1592, 'sw': 4286, 'passenger': 3203, 'commute': 911, 'illustration': 2147, 'subway': 4237, 'title': 4449, 'chief': 782, 'identitytheft': 2126, 'reopen': 3636, 'empty': 1418, 'breakfast': 551, 'suspend': 4281, 'jon': 2334, 'honor': 2060, 'barely': 359, 'wheel': 4822, 'mt': 2881, 'sports': 4126, 'si': 3970, 'relief': 3626, 'mess': 2752, 'dw': 1335, 'girls': 1817, 'biker': 443, 'undercover': 4623, 'path': 3208, 'garfield': 1777, 'residual': 3655, 'railway': 3528, 'trains': 4513, 'freakiest': 1720, 'followback': 1672, 'modiministry': 2844, 'calumet': 653, 'consent': 947, 'csx': 1037, 'knoxville': 2415, 'sentinel': 3887, 'villager': 4721, 'applaud': 203, 'prabhu': 3390, 'indi': 2179, 'indiannews': 2182, 'ignore': 2138, 'twin': 4597, 'tte': 4577, 'helpline': 2007, 'anxious': 177, 'cta': 1038, 'condolence': 937, 'disruptive': 1243, 'desolate': 1170, 'transform': 4517, 'attitude': 290, 'soundcloud': 4091, 'swell': 4297, 'daniel': 1079, 'abomination': 12, 'maketh': 2638, 'neighborhood': 2959, 'dixon': 1256, 'electro': 1389, 'brutally': 591, 'abused': 17, 'mum': 2889, 'redeemeth': 3598, 'servant': 3897, 'matt': 2694, 'desolation': 1171, 'comingsoon': 899, 'photoset': 3275, 'sheeran': 3938, 'hobbit': 2040, 'smaug': 4046, 'premiere': 3405, 'conspiracy': 952, 'thriller': 4422, 'suspense': 4282, 'lizards': 2553, 'digital': 1211, 'hd': 1976, 'trouble': 4556, 'forth': 1697, 'sudden': 4240, 'neither': 2962, 'wicke': 4837, 'december': 1110, 'carpet': 692, 'addiction': 48, 'loop': 2582, 'tumblr': 4581, 'employ': 1417, 'defeat': 1122, 'ops': 3126, 'multiplayer': 2887, 'shantae': 3926, 'smash': 4045, 'meme': 2737, 'camper': 662, 'asleep': 263, 'creativity': 1014, 'crackdown': 1003, 'server': 3899, 'saturate': 3813, 'commonwealth': 908, 'weakness': 4791, 'zaynmalik': 4970, 'lets': 2504, 'tryna': 4571, 'sj': 4017, 'childhood': 784, 'tend': 4372, 'hunger': 2099, 'rich': 3687, 'decrease': 1119, 'moth': 2864, 'rating': 3551, 'swim': 4299, 'lonely': 2578, 'emotionally': 1414, 'basis': 373, 'illusion': 2146, 'reportedly': 3643, 'ut': 4677, 'phew': 3264, 'meat': 2715, 'bot': 527, 'pitcher': 3304, 'recap': 3582, 'porn': 3367, 'size': 4015, 'cock': 867, 'chop': 794, 'stuart': 4223, 'broad': 576, 'ford': 1683, 'ontario': 3105, 'restrict': 3666, 'impress': 2159, 'demonstratio': 1145, 'border': 523, 'import': 2155, 'ukraine': 4614, 'independent': 2178, 'divide': 1253, 'matthew': 2696, 'megadeth': 2731, 'symphony': 4307, 'equal': 1463, 'marquei': 2678, 'como': 912, 'banco': 347, 'tinyjecht': 4442, 'detonate': 1179, 'antioch': 174, 'apollo': 196, 'ft': 1743, 'stun': 4229, 'aw': 317, 'rapping': 3546, 'lemon': 2496, 'meter': 2759, 'surface': 4264, 'apollobrown': 197, 'message': 2753, 'grandeur': 1868, 'ûªve': 4988, 'autumn': 307, 'marine': 2669, 'ignition': 2137, 'detonation': 1180, 'sensor': 3886, 'senso': 3885, 'connector': 945, 'connecto': 944, 'wpt': 4916, 'quote': 3517, 'esteem': 1479, 'belt': 414, 'regard': 3608, 'electrical': 1387, 'bmw': 507, 'arnley': 237, 'famous': 1562, 'brand': 545, 'fashionable': 1572, 'mountaineer': 2871, 'leisure': 2495, 'tab': 4313, 'consequence': 948, 'devastated': 1185, 'talkin': 4325, 'anybody': 178, 'ûïrichmond': 4992, 'sitting': 4009, 'jam': 2295, 'malik': 2642, 'declare': 1116, 'saipan': 3790, 'declaration': 1115, 'barack': 356, 'marian': 2666, 'laughing': 2455, 'realise': 3569, 'actor': 39, 'abcnew': 6, 'indoor': 2187, 'i䞻m': 2290, 'cain': 642, 'devastation': 1186, 'utter': 4679, 'alternative': 128, 'cnbc': 855, 'decade': 1109, 'nepal': 2964, 'livelihood': 2550, 'scandal': 3822, 'environmental': 1455, 'korea': 2419, 'settlement': 3906, 'juror': 2361, 'starbuck': 4156, 'urgent': 4663, 'mediterranean': 2723, 'witho': 4872, 'strengthen': 4204, 'haiyan': 1931, 'philippines': 3268, 'jeff': 2309, 'locke': 2565, 'pirate': 3299, 'acquire': 30, 'starter': 4159, 'mì¼sica': 2910, 'cinema': 809, 'lack': 2433, 'emotion': 1412, 'iron': 2271, 'ke': 2377, 'surprised': 4269, 'scifi': 3836, 'browser': 586, 'pitch': 3303, 'van': 4689, 'displaced': 1240, 'sittwe': 4010, 'prison': 3437, 'genocide': 1797, 'ihhen': 2139, 'msf': 2880, 'strategicpatience': 4198, 'idp': 2132, 'internally': 2244, 'osborn': 3142, 'openly': 3116, 'nema': 2963, 'idps': 2133, 'wom': 4883, 'museum': 2896, 'philippine': 3267, 'trace': 4500, 'nm': 3007, 'township': 4497, 'torture': 4481, 'elem': 1393, 'pomo': 3356, 'fund': 1755, 'label': 2432, 'nv': 3047, 'affect': 66, 'extreme': 1538, 'llf': 2554, 'okanagan': 3088, 'relentless': 3625, 'humidity': 2097, 'treasure': 4530, 'drain': 1289, 'columbia': 889, 'sep': 3888, 'cloud': 848, 'scared': 3825, 'ego': 1376, 'surround': 4271, 'weed': 4802, 'desert': 1165, 'spread': 4131, 'rapidly': 3544, 'georgia': 1801, 'anchor': 143, 'choke': 791, 'native': 2930, 'insult': 2228, 'noise': 3012, 'memory': 2741, 'potentially': 3379, 'horizon': 2065, 'steak': 4168, 'clev': 832, 'sort': 4087, 'memorial': 2738, 'gamergate': 1772, 'proxy': 3482, 'existence': 1516, 'rat': 3548, 'cafe': 639, 'kiss': 2404, 'asylum': 275, 'seeker': 3871, 'tracking': 4502, 'caitlin': 643, 'kiernan': 2390, 'centipede': 732, 'grace': 1863, 'superhero': 4252, 'epilepsy': 1460, 'request': 3647, 'drowning': 1310, 'clap': 821, 'rs': 3757, 'ns': 3035, 'dust': 1329, 'divert': 1252, 'atmospheric': 281, 'newzsacramento': 2981, 'april': 213, 'weaken': 4790, 'lubbock': 2605, 'outflow': 3156, 'yell': 4941, 'fee': 1594, 'thunder': 4428, 'eastbound': 1348, 'riyadh': 3712, 'combat': 891, 'magnum': 2628, 'opus': 3128, 'dakota': 1066, 'raw': 3553, 'formation': 1693, 'barrier': 363, 'circle': 810, 'sismo': 4005, 'interlaken': 2241, 'utc': 4678, 'seismic': 3873, 'tasmania': 4333, 'shelli': 3942, 'sicily': 3971, 'usgs': 4672, 'eq': 1462, 'twentynine': 4595, 'ssw': 4142, 'anza': 186, 'anchorage': 144, 'oklahoma': 3091, 'limit': 2535, 'epicenter': 1458, 'sm': 4042, 'significant': 3980, 'fart': 1570, 'hawaii': 1972, 'legislation': 2492, 'rossum': 3747, 'nnw': 3008, 'encouragement': 1427, 'apocalyptic': 195, 'tsunami': 4574, 'emsc': 1420, 'kindly': 2399, 'electrocute': 1390, 'blanket': 471, 'battery': 381, 'extend': 1535, 'niall': 2988, 'teslas': 4383, 'unsafe': 4649, 'unlocked': 4644, 'danisnotonfire': 1080, 'delete': 1132, 'wouldn䞻t': 4910, 'asshole': 267, 'fair': 1552, 'connection': 943, 'jr': 2347, 'beard': 395, 'senator': 3881, 'stick': 4179, 'electrocuted': 1391, 'lightening': 2527, 'backyard': 333, 'elephant': 1395, 'factory': 1548, 'penalty': 3233, 'map': 2661, 'got': 1851, 'plug': 3337, 'charger': 753, 'golem': 1841, 'nankana': 2919, 'sahib': 3789, 'design': 1167, 'dock': 1265, 'elsa': 1402, 'vacation': 4684, 'lawsuit': 2463, 'supervisor': 4255, 'cree': 1016, 'knife': 2410, 'surgery': 4267, 'tooth': 4473, 'dental': 1151, 'contact': 958, 'medicine': 2720, 'kuwait': 2428, 'specialist': 4109, 'present': 3413, 'seed': 3868, 'vegetable': 4693, 'gear': 1786, 'english': 1438, 'nurse': 3044, 'healthcare': 1987, 'ak': 98, 'reinstate': 3617, 'buffer': 603, 'protectdenaliwolve': 3473, 'excuse': 1512, 'advance': 56, 'lean': 2474, 'quick': 3512, 'addict': 47, 'blonde': 491, 'sirens': 4002, 'batfanuk': 377, 'decline': 1117, 'supply': 4256, 'troop': 4553, 'household': 2081, 'activate': 36, 'municipal': 2891, 'reddit': 3597, 'yyc': 4965, 'capacity': 675, 'nw': 3048, 'workplace': 4898, 'retail': 3668, 'primarily': 3429, 'sick': 3972, 'incase': 2167, 'accept': 18, 'electricity': 1388, 'feinstein': 1599, 'vet': 4701, 'enact': 1423, 'vietnamese': 4718, 'conclude': 931, 'overload': 3161, 'prompt': 3462, 'tflbusalert': 4390, 'providence': 3480, 'nursing': 3045, 'opening': 3115, 'fte': 1744, 'serve': 3898, 'committee': 906, 'personnel': 3254, 'ensure': 1448, 'veteran': 4702, 'access': 19, 'proper': 3465, 'cope': 973, 'brooklyn': 582, 'budget': 600, 'thur': 4430, 'missionhill': 2826, 'robinson': 3720, 'shutdown': 3969, 'kodiak': 2416, 'goulburn': 1853, 'henry': 2009, 'hysteria': 2114, 'grey': 1887, 'indifference': 2184, 'normal': 3017, 'simon': 3984, 'engulf': 1439, 'parley': 3196, 'canyon': 673, 'tribal': 4542, 'madinah': 2620, 'prophetmuhammad': 3468, 'islam': 2277, 'freeway': 1726, 'chaos': 750, 'tidal': 4434, 'engulfed': 1440, 'jumper': 2357, 'equally': 1464, 'emerge': 1409, 'golf': 1842, 'ghost': 1809, 'epicentre': 1459, 'capitalism': 677, 'tanzania': 4330, 'census': 730, 'tire': 4444, 'nowhere': 3030, 'lollapalooza': 2573, 'poem': 3345, 'tram': 4514, 'powerline': 3387, 'jump': 2356, 'metlife': 2761, 'efak': 1372, 'dealbreaker': 1101, 'abandon': 2, 'rutherford': 3775, 'refuse': 3607, 'milkshake': 2797, 'tournament': 4490, 'criminal': 1022, 'murderous': 2894, 'disrupt': 1242, 'roosevelt': 3742, 'mall': 2643, 'trafford': 4506, 'odeon': 3071, 'swells': 4298, 'portland': 3371, 'miami': 2771, 'casper': 701, 'of': 3072, 'hahahaha': 1927, 'tr': 4499, 'sandy': 3805, 'safely': 3787, 'unstable': 4650, 'zabadani': 4966, 'katunews': 2375, 'closed': 845, 'min': 2801, 'surfer': 4265, 'faan': 1542, 'mandatory': 2651, 'sheriff': 3945, 'advisory': 62, 'vietnam': 4717, 'patch': 3207, 'refund': 3606, 'bend': 416, 'hr': 2086, 'ridge': 3695, 'voluntary': 4742, 'inciweb': 2171, 'aid': 91, 'yellow': 4942, 'cabin': 636, 'owners': 3165, 'pickerel': 3282, 'reid': 3616, 'ink': 2206, 'ûïa': 4990, 'manly': 2654, 'ryan': 3777, 'earner': 1342, 'upwards': 4660, 'ks': 2425, 'schwarber': 3833, 'kendall': 2380, 'jenner': 2310, 'stout': 4192, 'philadelphia': 3266, 'ill': 2143, 'duck': 1320, 'chick': 780, 'pets': 3260, 'screen': 3849, 'otrametlife': 3148, 'whose': 4834, 'thebeginne': 4403, 'reasons': 3578, 'microphone': 2779, 'squeeze': 4139, 'sarcasm': 3808, 'bean': 393, 'bastard': 375, 'hahaha': 1925, 'joint': 2332, 'protector': 3475, 'blackberry': 465, 'mechanical': 2716, 'pharaoh': 3262, 'dancer': 1076, 'attendance': 287, 'gmmbc': 1830, 'saturday': 3814, 'southeast': 4097, 'logo': 2572, 'mental': 2745, 'greg': 1885, 'dye': 1337, 'explore': 1530, 'writer': 4923, 'correct': 978, 'kindermorgan': 2397, 'arena': 223, 'button': 628, 'discuss': 1229, 'eyewitness': 1541, 'penn': 3235, 'wpri': 4915, 'dramatic': 1292, 'climatechange': 839, 'hamburg': 1937, 'deny': 1153, 'investigation': 2258, 'pony': 3358, 'seal': 3855, 'wftv': 4817, 'tn': 4454, 'particularly': 3199, 'dk': 1258, 'denmark': 1150, 'closure': 846, 'shedding': 3936, 'marry': 2680, 'macia': 2617, 'testify': 4385, 'crematoria': 1017, 'provoke': 3481, 'famine': 1561, 'memories': 2740, 'soviet': 4099, 'function': 1754, 'poll': 3355, 'feast': 1588, 'example': 1507, 'ethiopian': 1483, 'regime': 3610, 'eastern': 1349, 'artificial': 249, 'experienced': 1523, 'smell': 4047, 'conquest': 946, 'kiev': 2391, 'presence': 3412, 'lego': 2494, 'manslaughter': 2657, 'sh': 3916, 'elijah': 1397, 'walker': 4756, 'attraction': 291, 'investigator': 2259, 'waimate': 4751, 'vermont': 4698, 'stockton': 4184, 'kindness': 2400, 'permanent': 3248, 'statement': 4163, 'shooting': 3958, 'fatality': 1577, 'eight': 1379, 'las': 2448, 'push': 3497, 'mortal': 2862, 'kombat': 2417, 'fatalities': 1576, 'stretch': 4208, 'mkx': 2833, 'me': 2710, 'concerned': 929, 'tch': 4343, 'whitbourne': 4829, 'bn': 508, 'zero': 4971, 'tremor': 4536, 'language': 2445, 'cousin': 996, 'driving': 1303, 'yep': 4944, 'logistic': 2571, 'display': 1241, 'irish': 2270, 'dm': 1260, 'vgbootcamp': 4703, 'zss': 4979, 'sws': 4304, 'captain': 679, 'falcon': 1555, 'rig': 3698, 'homie': 2055, 'wit': 4868, 'kosciusko': 2420, 'grass': 1871, 'lebanon': 2480, 'combine': 892, 'midnight': 2784, 'organization': 3135, 'primary': 3430, 'understanding': 4626, 'opposite': 3123, 'growingupblack': 1896, 'unaware': 4619, 'depth': 1157, 'solitude': 4070, 'mas': 2685, 'shake': 3919, 'unknown': 4641, 'gd': 1785, 'pity': 3305, 'sterling': 4176, 'combo': 893, 'retro': 3670, 'politifiact': 3354, 'harry': 1961, 'planned': 3318, 'wcw': 4786, 'tha': 4393, 'reno': 3635, 'sigalert': 3977, 'trash': 4523, 'langley': 2444, 'taco': 4316, 'phase': 3263, 'irvine': 2275, 'chp': 795, 'detour': 1181, 'babe': 327, 'wayne': 4783, 'ori': 3136, 'njturnpike': 3006, 'besides': 420, 'mnpdnashville': 2836, 'sept': 3890, 'we': 4788, 'curb': 1046, 'vip': 4729, 'saving': 3818, 'badge': 335, 'brave': 546, 'heading': 1979, 'hat': 1966, 'cap': 674, 'embroidered': 1407, 'luchaunderground': 2606, 'guest': 1906, 'maryland': 2684, 'mansion': 2656, 'damaged': 1070, 'pivot': 3306, 'firefighte': 1634, 'mississauga': 2827, 'flatten': 1649, 'penny': 3237, 'zouma': 4978, 'bird': 454, 'chest': 774, 'iconic': 2122, 'raynor': 3557, 'bluetooth': 504, 'steam': 4170, 'jewelry': 2316, 'earring': 1344, 'sided': 3974, 'linerless': 2538, 'slowly': 4041, 'toward': 4492, 'jkl': 2321, 'wx': 4929, 'curved': 1050, 'offroad': 3081, 'fog': 1668, 'lamp': 2437, 'prom': 3459, 'beam': 392, 'alloy': 116, 'cue': 1040, 'utv': 4681, 'outdoor': 3154, 'coastal': 862, 'trench': 4537, 'shelby': 3940, 'cdt': 724, 'alabama': 101, 'grant': 1870, 'edt': 1367, 'nws': 3050, 'prone': 3463, 'cuban': 1039, 'midwest': 2786, 'environment': 1454, 'random': 3537, 'vision': 4734, 'insubcontinent': 2227, 'lrt': 2603, 'assist': 268, 'unveil': 4653, 'travis': 4529, 'farrakhan': 1569, 'tornados': 4478, 'rspca': 3758, 'erode': 1470, 'priceless': 3427, 'bengal': 418, 'cm': 853, 'mamata': 2645, 'banerjee': 349, 'dvc': 1333, 'bjp': 463, 'nj': 3004, 'otherwise': 3147, 'itunes': 2288, 'apc': 192, 'donation': 1276, 'bayelsa': 387, 'negros': 2957, 'memphis': 2742, 'combust': 894, 'careless': 687, 'campfire': 663, 'pisgah': 3300, 'routecomplex': 3752, 'neck': 2952, 'cole': 876, 'pet': 3257, 'stamp': 4150, 'deni': 1148, 'indonesia': 2186, 'rogue': 3730, 'siskiyou': 4004, 'tape': 4331, 'clash': 822, 'cite': 813, 'mike': 2789, 'arkansas': 230, 'massive': 2689, 'outage': 3151, 'cheryl': 773, 'sturgi': 4231, 'kotaweather': 2421, 'coming': 898, 'boston': 526, 'unr': 4647, 'pennington': 3236, 'sustainable': 4284, 'organic': 3134, 'pound': 3381, 'lawrence': 2462, 'hailstorm': 1929, 'profile': 3452, 'nixon': 3003, 'wicked': 4838, 'sto': 4182, 'northeast': 3019, 'logan': 2569, 'plain': 3313, 'pot': 3377, 'strand': 4195, 'pcps': 3224, 'cupcake': 1045, 'reboot': 3579, 'survey': 4274, 'cc': 719, 'ad': 44, 'scar': 3823, 'better': 427, 'continually': 963, 'foul': 1703, 'trigger': 4545, 'lifestyle': 2521, 'kraft': 2423, 'battlefield': 384, 'duke': 1323, 'argument': 225, 'eden': 1359, 'highly': 2021, 'personally': 3253, 'dismiss': 1235, 'switch': 4303, 'chelsea': 767, 'lionel': 2542, 'precious': 3397, 'willian': 4848, 'londonfire': 2576, 'swansea': 4289, 'basically': 372, 'dlh': 1259, 'outlook': 3157, 'hwo': 2108, 'jax': 2301, 'crystal': 1036, 'nestleindia': 2965, 'magginoodle': 2624, 'consumption': 956, 'olap': 3093, 'friendly': 1735, 'quarrel': 3504, 'adventure': 58, 'keratin': 2382, 'brazilian': 548, 'copycat': 974, 'meg': 2729, 'breathing': 556, 'sneak': 4058, 'needle': 2955, 'rush': 3772, 'vulnerable': 4749, 'olive': 3095, 'surge': 4266, 'ireland': 2269, 'arnhem': 236, 'expand': 1519, 'prefer': 3401, 'triple': 4549, 'digit': 1210, 'scout': 3843, 'cooler': 971, 'temp': 4366, 'hellfire': 2004, 'afterlife': 76, 'beware': 429, 'surah': 4260, 'humaza': 2095, 'reflect': 3602, 'don䞻t': 1277, 'let䞻s': 2507, 'ianhellfire': 2116, 'kind': 2395, 'sub': 4234, 'sewing': 3911, 'dame': 1072, 'diablo': 1193, 'tow': 4491, 'missile': 2824, 'tension': 4374, 'jonathan': 2335, 'hijack': 2023, 'pdp': 3225, 'sight': 3978, 'homosexuality': 2056, 'os': 3141, 'exploit': 1528, 'bypass': 632, 'password': 3204, 'ars': 242, 'technica': 4356, 'nigerian': 2996, 'transfer': 4516, 'southampton': 4095, 'virgil': 4731, 'dijk': 1213, 'cherokee': 771, 'specially': 4110, 'chan': 746, 'oregon': 3132, 'cmon': 854, 'governor': 1856, 'parole': 3197, 'hijacker': 2024, 'linkury': 2540, 'medieval': 2721, 'earning': 1343, 'diver': 1250, 'fresno': 1732, 'weighs': 4805, 'infamous': 2192, 'hijacking': 2025, 'funtenna': 1757, 'prebreak': 3396, 'samaritans': 3798, 'perform': 3244, 'croatian': 1026, 'egypt': 1377, 'croat': 1025, 'europe': 1488, 'marc': 2662, 'andreessen': 147, 'affiliation': 68, 'tlvface': 4452, 'wreckage': 4920, 'modify': 2843, 'stadium': 4147, 'surrender': 4270, 'journalism': 2341, 'hostages': 2075, 'fortune': 1699, 'quot': 3516, 'sinjar': 3996, 'massacre': 2687, 'yazidis': 4937, 'holmgren': 2049, 'role': 3733, 'cod': 869, 'ts': 4573, 'nri': 3034, 'trs': 4559, 'tdp': 4345, 'refer': 3601, 'minhaz': 2809, 'merchant': 2749, 'bang': 350, 'boko': 514, 'haram': 1952, 'aq': 215, 'guillermo': 1908, 'noaa': 3009, 'looping': 2583, 'bluedio': 500, 'turbine': 4584, 'wireless': 4866, 'stereo': 4175, 'headphone': 1981, 'headset': 1983, 'meteoearth': 2758, 'coral': 975, 'mock': 2838, 'define': 1127, 'katrina': 2374, 'patton': 3217, 'oswalt': 3144, 'lately': 2451, 'bolster': 515, 'suryaray': 4279, 'foster': 1702, 'settle': 3905, 'scott': 3841, 'wilshere': 4850, 'elderly': 1384, 'cosponsor': 981, 'renew': 3634, 'mcilroy': 2707, 'rory': 3744, 'breakingnews': 554, 'forgiven': 1691, 'possibly': 3374, 'injurie': 2204, 'regardless': 3609, 'inflict': 2196, 'male': 2641, 'professional': 3451, 'inevitably': 2191, 'forsure': 1695, 'strain': 4194, 'director': 1219, 'devalue': 1183, 'volga': 4740, 'ergo': 1469, 'rotator': 3749, 'dunbar': 1326, 'stem': 4172, 'playoff': 3328, 'cowboys': 1000, 'bwp': 630, 'advertise': 59, 'inundate': 2250, 'inundated': 2251, 'application': 205, 'safari': 3785, 'bread': 549, 'sample': 3799, 'inundation': 2252, 'boot': 522, 'platform': 3323, 'realtime': 3574, 'unfortunately': 4630, 'sinkhole': 3998, 'yougov': 4953, 'dundee': 1327, 'paris': 3192, 'geneva': 1795, 'sand': 3802, 'opposition': 3124, 'alp': 123, 'smart': 4044, 'lava': 2459, 'youngheroesid': 4955, 'pantherattack': 3187, 'gotten': 1852, 'breed': 557, 'shark': 3930, 'oliver': 3096, 'lightne': 2529, 'pinpoint': 3296, 'reshape': 3652, 'mineral': 2808, 'gusty': 1916, 'certify': 739, 'blink': 483, 'lawton': 2464, 'pics': 3285, 'footage': 1677, 'unconfirmed': 4622, 'neighbour': 2960, 'abbott': 3, 'chew': 777, 'gum': 1910, 'peacefully': 3228, 'telly': 4365, 'singe': 3993, 'impulse': 2164, 'commit': 905, 'endorse': 1430, 'noahanyname': 3010, 'royalcarribean': 3756, 'faroeisland': 1568, 'cbc': 715, 'administration': 52, 'minority': 2817, 'debatequestionswewanttohear': 1106, 'blah': 468, 'baruch': 365, 'goldstein': 1840, 'dickhead': 1201, 'aspect': 264, 'decision': 1113, 'incite': 2170, 'channel': 749, 'cbs': 717, 'march': 2663, 'permanently': 3249, 'barackobama': 357, 'mayhem': 2703, 'hanneman': 1946, 'raynbowaffair': 3555, 'editor': 1364, 'diamondkesawn': 1196, 'ramag': 3536, 'andy': 149, 'difference': 1205, 'bully': 611, 'disco': 1226, 'comic': 897, 'bass': 374, 'whenever': 4824, 'sp': 4100, 'def': 1121, 'bubble': 596, 'mil': 2791, 'ushanka': 4673, 'fur': 1758, 'experiment': 1524, 'magner': 2627, 'discusses': 1230, 'leadership': 2470, 'lulgzimbestpict': 2611, 'intelligence': 2234, 'teamhendrick': 4351, 'supreme': 4259, 'oks': 3092, 'sputnik': 4136, 'stu': 4222, 'mudslide': 2884, 'oso': 3143, 'britishbakeoff': 572, 'icelandreview': 2118, 'maker': 2637, 'unique': 4634, 'spoil': 4121, 'mirage': 2820, 'addition': 49, 'unavoidable': 4618, 'socialism': 4064, 'net': 2966, 'billion': 447, 'unrest': 4648, 'cheap': 763, 'ay': 326, 'guatemala': 1904, 'executive': 1513, 'chernobyl': 770, 'tepco': 4376, 'anonymous': 167, 'radiation': 3522, 'refugees': 3604, 'denial': 1149, 'sweep': 4295, 'mole': 2845, 'pretend': 3418, 'responsibility': 3661, 'reuter': 3674, 'reactor': 3564, 'finnish': 1632, 'ancient': 145, 'circuit': 811, 'sideline': 3975, 'newest': 2973, 'ministers': 2815, 'fennovoima': 1606, 'rolling': 3736, 'obliterate': 3060, 'genius': 1796, 'jonvoyage': 2338, 'sarah': 3807, 'palin': 3180, 'battleship': 385, 'twitt': 4599, 'obliterated': 3061, 'michelebachman': 2775, 'inferno': 2194, 'globalwarme': 1825, 'meal': 2711, 'strip': 4213, 'mets': 2765, 'tt': 4576, 'obliteration': 3062, 'canaanite': 667, 'worldnetdaily': 4900, 'sis': 4003, 'kerry': 2384, 'startup': 4160, 'larger': 2447, 'refugio': 3605, 'costlier': 983, 'bigger': 441, 'pipeline': 3298, 'nyt': 3054, 'bankstown': 353, 'wattle': 4779, 'plains': 3314, 'briefing': 563, 'slicker': 4035, 'disea': 1232, 'edinburgh': 1361, 'bronx': 580, 'legio': 2488, 'trophy': 4554, 'worst': 4906, 'absolute': 14, 'pandemonium': 3184, 'element': 1394, 'tgirl': 4391, 'disorder': 1237, 'mania': 2653, 'newbie': 2972, 'acoustic': 29, 'attic': 289, 'hurry': 2105, 'hardy': 1958, 'obispo': 3058, 'sponsor': 4123, 'atlantic': 279, 'dailykos': 1065, 'distribute': 1247, 'bounty': 533, 'karma': 2370, 'milwaukee': 2800, 'university': 4640, 'quarantine': 3503, 'freespeech': 1724, 'offensive': 3073, 'wired': 4865, 'offensiveåêcontent': 3074, 'founder': 1704, 'huffman': 2088, 'specif': 4111, 'subreddit': 4236, 'edward': 1369, 'monitoring': 2851, 'wipp': 4863, 'elevated': 1396, 'doe': 1268, 'rainstorm': 3532, 'gloucester': 1828, 'yobe': 4949, 'jim': 2319, 'alps': 124, 'rome': 3738, 'crown': 1030, 'robotrainstorm': 3722, 'hardly': 1957, 'thru': 4425, 'raze': 3558, 'sandiego': 3803, 'njenga': 3005, 'married': 2679, 'newlywed': 2974, 'dutton': 1331, 'lnp': 2557, 'cameroon': 658, 'repatriate': 3639, 'wowo': 4914, 'whao': 4819, 'banquet': 354, 'vice': 4708, 'oc': 3065, 'ud': 4608, 'structural': 4217, 'ypres': 4961, 'nations': 2929, 'opera': 3118, 'zakbagans': 4967, 'briton': 574, 'bury': 620, 'await': 318, 'stranded': 4196, 'naval': 2934, 'mediterran': 2722, 'carr': 693, 'carryi': 695, 'marin': 2668, 'nan': 2918, 'migrants': 2788, 'feminists': 1604, 'nickcannon': 2991, 'mandy': 2652, 'itunesmusic': 2289, 'itune': 2287, 'blowmandyup': 496, 'headquarters': 1982, 'drag': 1287, 'salvador': 3795, 'rioting': 3706, 'happiness': 1950, 'cindy': 807, 'noonan': 3015, 'cindynoonan': 808, 'heartbreak': 1992, 'undergroundrailraod': 4624, 'plenty': 3333, 'championship': 745, 'insurers': 2231, 'wee': 4801, 'venezuela': 4696, 'gr': 1860, 'rubble': 3763, 'forbes': 1680, 'gerenciatodos': 1802, 'fresh': 1730, 'greedy': 1879, 'sandstorm': 3804, 'swallow': 4287, 'dumb': 1324, 'darude': 1086, 'screamed': 3845, 'lyric': 2614, 'mina': 2802, 'chonce': 792, 'brooke': 581, 'cheat': 764, 'encore': 1424, 'camila': 659, 'cabello': 635, 'stan': 4151, 'screaming': 3846, 'idol': 2131, 'screams': 3848, 'pillow': 3292, 'rly': 3713, 'vibrate': 4707, 'appropriation': 210, 'exploration': 1529, 'oppose': 3122, 'gabon': 1764, 'somalia': 4074, 'insight': 2215, 'speaker': 4107, 'detectado': 1178, 'japìn': 2300, 'okinawa': 3090, 'jst': 2348, 'pond': 3357, 'lowndes': 2602, 'trolley': 4552, 'bethlehem': 425, 'crater': 1010, 'swallows': 4288, 'wew': 4815, 'sinking': 3999, 'unhappiness': 4631, 'titanic': 4448, 'gander': 1773, 'leading': 2471, 'mafia': 2622, 'bumper': 614, 'foxysiren': 1712, 'jennifer': 2311, 'aniston': 157, 'buck': 597, 'argue': 224, 'fevwarrior': 1611, 'thu': 4426, 'sleeping': 4034, 'diverse': 1251, 'marketforce': 2674, 'winner': 4859, 'raise': 3533, 'gunfire': 1912, 'blunt': 505, 'alot': 122, 'locomotive': 2567, 'instruction': 2226, 'snowstorm': 4060, 'desk': 1169, 'sassy': 3809, 'hunk': 2101, 'smoky': 4053, 'aom': 188, 'boost': 521, 'treeporn': 4534, 'june': 2358, 'sweater': 4293, 'boeing': 513, 'cockpit': 868, 'slight': 4037, 'nasahurricane': 2923, 'stretcher': 4209, 'invalid': 2254, 'graze': 1876, 'rexyy': 3683, 'towel': 4494, 'siding': 3976, 'pllolz': 3334, 'witter': 4875, 'ûïstretcher': 4993, 'virgin': 4732, 'galactic': 1767, 'spaceship': 4102, 'braking': 544, 'brake': 543, 'rightways': 3700, 'inspection': 2217, 'defect': 1123, 'ntsb': 3037, 'unlock': 4643, 'citizen': 814, 'vest': 4700, 'bestnaijamade': 422, 'bomber': 517, 'mercado': 2748, 'explosive': 1532, 'compound': 924, 'suruì': 4273, 'esh': 1475, 'premonition': 3407, 'kurdish': 2426, 'hamas': 1936, 'suruc': 4272, 'shatter': 3932, 'aquarium': 216, 'ornament': 3140, 'decor': 1118, 'stomach': 4185, 'trading': 4503, 'forex': 1688, 'affiliate': 67, 'marketer': 2673, 'narendra': 2921, 'clueless': 851, 'session': 3902, 'survivors': 4278, 'haunting': 1970, 'drawn': 1294, 'glenn': 1822, 'quran': 3518, 'omega': 3099, 'religion': 3627, 'rayner': 3556, 'newsarama': 2976, 'handed': 1942, 'chattanooga': 762, 'unity': 4638, 'naved': 2937, 'metrofmtalk': 2764, 'disown': 1238, 'nave': 2936, 'overnight': 3163, 'generational': 1794, 'privacy': 3439, 'originalfunko': 3139, 'buddys': 599, 'lighten': 2526, 'mama': 2644, 'durant': 1328, 'nba': 2941, 'usnwsgov': 4674, 'oun': 3149, 'pickens': 3281, 'livingsafely': 2552, 'helsinki': 2008, 'eastward': 1350, 'brunette': 589, 'distinct': 1246, 'rotation': 3748, 'recount': 3591, 'collective': 879, 'ûïwhen': 4996, 'plunge': 3339, 'unawares': 4620, 'grande': 1867, 'miner': 2807, 'chile': 785, 'somewhere': 4080, 'eudrylantiqua': 1486, 'zarry': 4968, 'lavender': 2460, 'poetry': 3346, 'landing': 2440, 'ptsdchat': 3487, 'richmond': 3689, 'traumatised': 4526, 'godslove': 1836, 'thanku': 4398, 'tsunamiesh': 4575, 'twister': 4598, 'shania': 3925, 'outbid': 3152, 'oneself': 3103, 'hew': 2015, 'landfall': 2439, 'precipitation': 3398, 'measurement': 2714, 'captures': 681, 'gpm': 1859, 'bullseye': 610, 'upheaval': 4656, 'newberg': 2971, 'betz': 428, 'mayan': 2701, 'hieroglyphic': 2019, 'lowly': 2601, 'perquisite': 3250, 'diageo': 1194, 'stresses': 4206, 'revolt': 3680, 'pov': 3383, 'amsterdam': 141, 'watertown': 4777, 'bruise': 588, 'stormchase': 4190, 'ef': 1371, 'schiphol': 3830, 'auckland': 293, 'nasasolarsystem': 2924, 'jupiter': 2360, 'blitz': 484, 'traverse': 4528, 'raung': 3552, 'aogashima': 187, 'diaporama': 1198, 'sixpenceee': 4013, 'karymsky': 2371, 'located': 2562, 'shoes': 3955, 'asics': 260, 'ronnie': 3739, 'kith': 2406, 'thuggin': 4427, 'lethal': 2503, 'rifle': 3697, 'dannyonpc': 1081, 'hardline': 1956, 'throwingknife': 4424, 'testimony': 4386, 'billings': 446, 'rapid': 3543, 'cheyenne': 778, 'whirlwind': 4827, 'ûªm': 4985, 'aria': 227, 'ahrary': 90, 'troubling': 4557, 'wipe': 4862, 'parker': 3194, 'sheet': 3939, 'idfire': 2127, 'windstorm': 4855, 'ekiti': 1381, 'hike': 2026, 'association': 272, 'approve': 212, 'wocowae': 4880, 'gunshot': 1914, 'kerricktrial': 2383, 'jonathanferrell': 2336, 'kashmir': 2372, 'woodlawn': 4890, 'friggin': 1737, 'sequel': 3892, 'conclusively': 932, 'furious': 1759, 'cramer': 1004, 'iger': 2135}\n"
     ]
    }
   ],
   "source": [
    "print(Tfidf_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4378)\t0.6418585590850897\n",
      "  (0, 1947)\t0.4905576134423346\n",
      "  (0, 1007)\t0.4046491568691504\n",
      "  (0, 682)\t0.42852056878061195\n",
      "  (1, 4167)\t0.3609440059041365\n",
      "  (1, 3786)\t0.41763052758306285\n",
      "  (1, 3501)\t0.33879208437358693\n",
      "  (1, 1989)\t0.32470931653252705\n",
      "  (1, 1498)\t0.32838515522818507\n",
      "  (1, 1345)\t0.3238268345493893\n",
      "  (1, 1206)\t0.4051263556185501\n",
      "  (1, 815)\t0.3134644435818234\n",
      "  (2, 4203)\t0.34534853688596606\n",
      "  (2, 4129)\t0.3564282435933445\n",
      "  (2, 3817)\t0.30512701866112635\n",
      "  (2, 3357)\t0.44440122526570863\n",
      "  (2, 1686)\t0.3075346603428988\n",
      "  (2, 1650)\t0.43467248535231573\n",
      "  (2, 1633)\t0.21044156810593648\n",
      "  (2, 33)\t0.3678246297945927\n",
      "  (3, 4843)\t0.3992294203409694\n",
      "  (3, 2528)\t0.6970486740170907\n",
      "  (3, 1633)\t0.31100434729809345\n",
      "  (3, 194)\t0.5079520764274087\n",
      "  (4, 4604)\t0.4227099316795677\n",
      "  :\t:\n",
      "  (3259, 2531)\t0.16568712366184288\n",
      "  (3259, 2449)\t0.22253683469065402\n",
      "  (3259, 2104)\t0.25306261482474035\n",
      "  (3259, 2037)\t0.23340082223551267\n",
      "  (3259, 1954)\t0.27547326491409313\n",
      "  (3259, 815)\t0.22653983162656416\n",
      "  (3259, 516)\t0.19717523364719963\n",
      "  (3259, 238)\t0.25306261482474035\n",
      "  (3260, 2537)\t0.4906482467774154\n",
      "  (3260, 1881)\t0.5210833921160468\n",
      "  (3260, 1160)\t0.46905416750573387\n",
      "  (3260, 779)\t0.5174210899650163\n",
      "  (3261, 4796)\t0.3256230992480034\n",
      "  (3261, 3157)\t0.43808916688432853\n",
      "  (3261, 2729)\t0.5071705643481929\n",
      "  (3261, 2282)\t0.34027434904745846\n",
      "  (3261, 2108)\t0.4441345290661022\n",
      "  (3261, 1974)\t0.3630198138187717\n",
      "  (3262, 4189)\t0.2558910334060675\n",
      "  (3262, 3315)\t0.27858826145985854\n",
      "  (3262, 2891)\t0.3870845774580159\n",
      "  (3262, 1410)\t0.24845858242223268\n",
      "  (3262, 815)\t0.2835995125570288\n",
      "  (3262, 646)\t0.6471448888264194\n",
      "  (3262, 36)\t0.3822525757366312\n"
     ]
    }
   ],
   "source": [
    "print(Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  79.2907180385289\n"
     ]
    }
   ],
   "source": [
    "# fit the training dataset on the NB classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X_Tfidf,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['target_naive'] = Naive.predict(Test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "3258    1\n",
       "3259    0\n",
       "3260    1\n",
       "3261    1\n",
       "3262    1\n",
       "Name: target_naive, Length: 3263, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['target_naive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  78.94045534150614\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['target_Svm'] = SVM.predict(Test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['id'],submission['target'] = df_test['id'],df_test['target_Svm']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submitting the SVM result\n",
    "submission.to_csv('submission_Svm.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.4.3)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.18.1)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from h5py->keras) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                117070    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 117,081\n",
      "Trainable params: 117,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model building\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))# Number of features\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "               \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                     epochs=100,\n",
    "                     verbose=False,\n",
    "                     validation_data=(X_test, y_test),\n",
    "                     batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9863\n",
      "Testing Accuracy:  0.7390\n"
     ]
    }
   ],
   "source": [
    "#evaluating the model\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world annihilation vs self transformation alien attack exterminate human\n",
      "[44, 529, 699, 265, 2384, 1930, 26, 3049, 530]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "print(sentences_train[2])\n",
    "print(X_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[236 202 344 581 311 387 119 855 581 110 107   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "#padding the sequence to length of 100\n",
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "print(X_train[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 50)           595650    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                50010     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 645,671\n",
      "Trainable params: 645,671\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "#with one embedding layer\n",
    "embedding_dim = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.5677\n",
      "Testing Accuracy:  0.5783\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 50)           595650    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 596,171\n",
      "Trainable params: 596,171\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#with Global pooling\n",
    "embedding_dim = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9839\n",
      "Testing Accuracy:  0.7736\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=50,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['trans']\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           deeds reason earth quake may allah forgive us\n",
       "1                   forest fire near la ronge sask canada\n",
       "2       resident ask shelter place notify officer evac...\n",
       "3       people receive wild fire evacuation order cali...\n",
       "4       get send photo ruby alaska smoke wild fire pou...\n",
       "                              ...                        \n",
       "7608     two giant crane hold bridge collapse nearby home\n",
       "7609    Aria Ahrary thetawni control wild fire califor...\n",
       "7610                                       volcano hawaii\n",
       "7611    police investigate e bike collided car little ...\n",
       "7612    late home raze northern california wild fire a...\n",
       "Name: trans, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1186    0\n",
       "4071    1\n",
       "5461    1\n",
       "5787    1\n",
       "7445    0\n",
       "       ..\n",
       "5226    0\n",
       "5390    0\n",
       "860     0\n",
       "7603    1\n",
       "7270    1\n",
       "Name: target, Length: 5329, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#reviews = x_mix.values\n",
    "#sentiments = y_mix.values\n",
    "\n",
    "train_reviews = np.array(X_train)\n",
    "train_sentiments = np.array(y_train)\n",
    "\n",
    "test_reviews = np.array(X_test)\n",
    "test_sentiments = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_reviews = np.array(df_test['trans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train_reviews = train_reviews\n",
    "norm_test_reviews = test_reviews\n",
    "#norm_bench_reviews = bench_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_bench_reviews= t_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.keras.preprocessing.text.Tokenizer(oov_token='<UNK>')\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(norm_train_reviews)\n",
    "t.word_index['<PAD>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('symptom', 11449), ('<PAD>', 0), 1)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), min([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), t.word_index['<UNK>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting text to sequence\n",
    "train_sequences = t.texts_to_sequences(norm_train_reviews)\n",
    "test_sequences = t.texts_to_sequences(norm_test_reviews)\n",
    "bench_sequences = t.texts_to_sequences(norm_bench_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_lens = [len(s) for s in train_sequences]\n",
    "test_lens = [len(s) for s in test_sequences]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12, 6))\n",
    "h1 = ax[0].hist(train_lens)\n",
    "h2 = ax[1].hist(test_lens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the docs\n",
    "MAX_SEQUENCE_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5329, 100), (2284, 100), (3263, 100))"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad dataset to a maximum review length in words\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "ben_X_test = tf.keras.preprocessing.sequence.pad_sequences(bench_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "X_train.shape, X_test.shape,ben_X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoading labels\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "le = LabelEncoder()\n",
    "num_classes=2\n",
    "On = OneHotEncoder()\n",
    " # positive -> 1, negative -> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentiments = test_sentiments.reshape(-1,1)\n",
    "train_sentiments = train_sentiments.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing lable encoading\n",
    "y_train = le.fit_transform(train_sentiments)\n",
    "y_test = le.transform(test_sentiments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11913"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11450"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(t.word_index)\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 300)          3435000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100, 128)          219648    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 3,819,513\n",
      "Trainable params: 3,819,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 300 # dimension for dense embeddings for each token\n",
    "LSTM_DIM = 128 # total LSTM units\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(tf.keras.layers.SpatialDropout1D(0.1))\n",
    "model.add(tf.keras.layers.LSTM(LSTM_DIM, return_sequences=True))\n",
    "model.add(tf.keras.layers.LSTM(LSTM_DIM, return_sequences=False))\n",
    "#model.add(tf.keras.layers.LSTM(LSTM_DIM, return_sequences=False))\n",
    "#model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_DIM, return_sequences=False)))\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"Adam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "43/43 [==============================] - 11s 251ms/step - loss: 0.6166 - accuracy: 0.6601 - val_loss: 0.4717 - val_accuracy: 0.7880\n",
      "Epoch 2/4\n",
      "43/43 [==============================] - 10s 238ms/step - loss: 0.3071 - accuracy: 0.8773 - val_loss: 0.4834 - val_accuracy: 0.7795\n",
      "Epoch 3/4\n",
      "43/43 [==============================] - 10s 239ms/step - loss: 0.1739 - accuracy: 0.9376 - val_loss: 0.6382 - val_accuracy: 0.7402\n",
      "Epoch 4/4\n",
      "43/43 [==============================] - 10s 240ms/step - loss: 0.1051 - accuracy: 0.9639 - val_loss: 0.7137 - val_accuracy: 0.7580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f65bc5e0550>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "model.fit(X_train, y_train, epochs=4, batch_size=batch_size, \n",
    "          shuffle=True, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.66%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre trained word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cut: invalid option -- '0'\r\n",
      "head: cannot open ‘data/glove_word_embeddings/glove.6B.50d.txt’ for readingTry 'cut --help' for more information.\r\n",
      ": No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "#installing pretained model\n",
    "!head -n 1 data/glove_word_embeddings/glove.6B.50d.txt | cut -c-50 the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.04445\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the word embeddings into embedding matrix\n",
    "embedding_dim = 50\n",
    "embedding_matrix = create_embedding_matrix('glove.6B.50d.txt',tokenizer.word_index, embedding_dim)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.1591    , -0.21427999,  0.63099003, ...,  0.021215  ,\n",
       "        -0.14218999,  0.66955   ],\n",
       "       [ 0.50905001, -0.36805001,  0.41275001, ..., -0.03356   ,\n",
       "         0.056012  , -0.43283001],\n",
       "       ...,\n",
       "       [-0.51574999, -0.48277   ,  0.011665  , ..., -0.80877   ,\n",
       "        -0.068774  ,  0.060801  ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.047172  , -0.48464   ,  0.70864999, ..., -0.22487999,\n",
       "         0.11842   ,  0.27173001]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7254260052043986"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the percentage of our data on the pretrained model\n",
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "nonzero_elements / vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 50)           595650    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 596,171\n",
      "Trainable params: 521\n",
      "Non-trainable params: 595,650\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
    "                           weights=[embedding_matrix], \n",
    "                           input_length=maxlen, \n",
    "                           trainable=False))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.6649\n",
      "Testing Accuracy:  0.5968\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=50,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 50)           595650    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 596,171\n",
      "Trainable params: 596,171\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Retraining the model with uncovered data\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
    "                           weights=[embedding_matrix], \n",
    "                           input_length=maxlen, \n",
    "                           trainable=True))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9874\n",
      "Testing Accuracy:  0.7452\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=50,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Grid Search\n",
    "param_grid = dict(num_filters=[32, 64, 128],\n",
    "                  kernel_size=[3, 5, 7],\n",
    "                  vocab_size=[5000], \n",
    "                  embedding_dim=[50],\n",
    "                  maxlen=[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Main settings\n",
    "epochs = 20\n",
    "embedding_dim = 50\n",
    "maxlen = 100\n",
    "output_file = 'data/output.txt'\n",
    "\n",
    "\n",
    "sentences = df['trans'].values\n",
    "y = df['target'].values\n",
    "\n",
    "# Train-test split\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "        sentences, y, test_size=0.25, random_state=1000)\n",
    "\n",
    "    # Tokenize words\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "\n",
    "    # Adding 1 because of reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    # Pad sequences with zeros\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "    # Parameter grid for grid search\n",
    "param_grid = dict(num_filters=[32, 64, 128],\n",
    "                      kernel_size=[3, 5, 7],\n",
    "                      vocab_size=[vocab_size],\n",
    "                      embedding_dim=[embedding_dim],\n",
    "                      maxlen=[maxlen])\n",
    "model = KerasClassifier(build_fn=create_model,\n",
    "                            epochs=epochs, batch_size=10,\n",
    "                            verbose=False)\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n",
    "                              cv=4, verbose=1, n_iter=5)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate testing set\n",
    "test_accuracy = grid.score(X_test, y_test)\n",
    "\n",
    "    # Save and evaluate results\n",
    "    \n",
    "        \n",
    "print(grid_result.best_score_)\n",
    "print(grid_result.best_params_)\n",
    "print(test_accuracy)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = create_model(64, 5, 13421, 50, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9841\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'KerasClassifier' object has no attribute 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-d25dc19ef1f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Accuracy: {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Testing Accuracy:  {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KerasClassifier' object has no attribute 'evaluate'"
     ]
    }
   ],
   "source": [
    "history = fin.fit(X_train, y_train,\n",
    "                    epochs=50,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)\n",
    "loss, accuracy = fin.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy:  0.7353\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = fin.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with Bert embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-for-tf2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.14.4)\n",
      "Requirement already satisfied: params-flow>=0.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from bert-for-tf2) (0.8.2)\n",
      "Requirement already satisfied: py-params>=0.9.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from bert-for-tf2) (0.9.7)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.1)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.44.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: sentencepiece in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.1.91)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tensorflow in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.2.0)\n",
      "Collecting hub\n",
      "  Downloading hub-0.5.0.0-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.18.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.9.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.30.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.4.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (2.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.12.2)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: retrying<2,>=1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from hub) (1.3.3)\n",
      "Collecting tenacity<7,>=5\n",
      "  Downloading tenacity-6.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: pillow<8,>=6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from hub) (7.0.0)\n",
      "Requirement already satisfied: botocore>=1.12.204 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from hub) (1.17.16)\n",
      "Requirement already satisfied: boto3<3,>=1.9.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from hub) (1.14.16)\n",
      "Requirement already satisfied: pytest in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from hub) (5.4.1)\n",
      "Collecting google-cloud-storage<2,>=1\n",
      "  Downloading google_cloud_storage-1.29.0-py2.py3-none-any.whl (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 4.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting lz4<4,>=3\n",
      "  Downloading lz4-3.1.0-cp36-cp36m-manylinux2010_x86_64.whl (1.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8 MB 23.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click<8,>=6.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from hub) (7.1.1)\n",
      "Collecting pathos==0.2.2.1\n",
      "  Downloading pathos-0.2.2.1.tar.gz (161 kB)\n",
      "\u001b[K     |████████████████████████████████| 161 kB 71.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.23.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.19.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (46.1.3.post20200330)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.12.204->hub) (0.9.4)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.12.204->hub) (1.25.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.12.204->hub) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.12.204->hub) (0.15.2)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3<3,>=1.9.2->hub) (0.3.3)\n",
      "Requirement already satisfied: py>=1.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pytest->hub) (1.8.1)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pytest->hub) (20.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pytest->hub) (19.3.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pytest->hub) (8.2.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pytest->hub) (0.13.1)\n",
      "Requirement already satisfied: wcwidth in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pytest->hub) (0.1.9)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pytest->hub) (1.5.0)\n",
      "Collecting google-resumable-media<0.6dev,>=0.5.0\n",
      "  Downloading google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB)\n",
      "Collecting google-cloud-core<2.0dev,>=1.2.0\n",
      "  Downloading google_cloud_core-1.3.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting ppft>=1.6.4.8\n",
      "  Downloading ppft-1.6.6.2.zip (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 70.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill>=0.2.8.2\n",
      "  Downloading dill-0.3.2.zip (177 kB)\n",
      "\u001b[K     |████████████████████████████████| 177 kB 63.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pox>=0.2.4\n",
      "  Downloading pox-0.2.8.zip (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 72.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multiprocess>=0.70.6.1\n",
      "  Downloading multiprocess-0.70.10.zip (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 60.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2020.4.5.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging->pytest->hub) (2.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=0.12->pytest->hub) (2.2.0)\n",
      "Collecting google-api-core<2.0.0dev,>=1.16.0\n",
      "  Downloading google_api_core-1.21.0-py2.py3-none-any.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 14.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: pytz in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage<2,>=1->hub) (2019.3)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.6.0\n",
      "  Downloading googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 11.9 MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pathos, ppft, dill, pox, multiprocess\n",
      "  Building wheel for pathos (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathos: filename=pathos-0.2.2.1-py3-none-any.whl size=77224 sha256=3e65a7462566f3d0b561ac1aa75cde0f6d7f995b13b45eee92dffd9e03f8157a\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/1d/06/19/41a532edab81c60002e6e0efde4954337a1a8f09c893196f8c\n",
      "  Building wheel for ppft (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ppft: filename=ppft-1.6.6.2-py3-none-any.whl size=64742 sha256=ac7298ddb2aab293f5ed68d2fa4ce1f1df6228d893b4545c29db68777e4e8c9e\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/fc/13/89/f55a8959ba9e54898fc2f6436e8f8326a5a8fb40f1c71c1d62\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.2-py3-none-any.whl size=78912 sha256=0d2a983b99a63de462df99e8a1855c07179e0b1f074d7ffb2c9f843cc8e7014f\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/02/49/cf/660924cd9bc5fcddc3a0246fe39800c83028d3ccea244de352\n",
      "  Building wheel for pox (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pox: filename=pox-0.2.8-py3-none-any.whl size=28290 sha256=6511e81c4be8336b1202beb254aceb60fde29dc1c69e7c57bd6516d1a6f3c802\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/ce/39/65/a5c5edbe8ec4d429578cb5242f01efdc92a54b4abac45b63a0\n",
      "  Building wheel for multiprocess (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for multiprocess: filename=multiprocess-0.70.10-py3-none-any.whl size=100506 sha256=e968de7b33d8c76d67a03b6cd7380bf00282177fda22fbae8719e71f7cea6878\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/af/2b/9b/5c0341ebf3f5dac0ecdf4ec463b1909f5a304e182ac45e53b3\n",
      "Successfully built pathos ppft dill pox multiprocess\n",
      "Installing collected packages: tenacity, google-resumable-media, googleapis-common-protos, google-api-core, google-cloud-core, google-cloud-storage, lz4, ppft, dill, pox, multiprocess, pathos, hub\n",
      "Successfully installed dill-0.3.2 google-api-core-1.21.0 google-cloud-core-1.3.0 google-cloud-storage-1.29.0 google-resumable-media-0.5.1 googleapis-common-protos-1.52.0 hub-0.5.0.0 lz4-3.1.0 multiprocess-0.70.10 pathos-0.2.2.1 pox-0.2.8 ppft-1.6.6.2 tenacity-6.2.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-for-tf2\n",
    "!pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_hub\n",
      "  Downloading tensorflow_hub-0.8.0-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 6.9 MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow_hub) (1.18.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow_hub) (1.14.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow_hub) (3.12.2)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorflow_hub) (46.1.3.post20200330)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.8.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the bert tokenizer and loadind the corpus\n",
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_reviews(text_reviews):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df['trans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing the text\n",
    "tokenized_reviews = [tokenize_reviews(review) for review in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the total number of tokens\n",
    "reviews_with_len = [[review, y[i], len(review)]\n",
    "                 for i, review in enumerate(tokenized_reviews)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffling the tokenized reviews\n",
    "random.shuffle(reviews_with_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_reviews_labels = [(review_lab[0], review_lab[1]) for review_lab in reviews_with_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping the data ready to load into the model\n",
    "processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_reviews_labels, output_types=(tf.int32, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FlatMapDataset shapes: (<unknown>, <unknown>), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pading the data\n",
    "BATCH_SIZE = 32\n",
    "batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spliting into trainn and test\n",
    "TOTAL_BATCHES = math.ceil(len(sorted_reviews_labels) / BATCH_SIZE)\n",
    "TEST_BATCHES = TOTAL_BATCHES // 10\n",
    "batched_dataset.shuffle(TOTAL_BATCHES)\n",
    "test_data = batched_dataset.take(TEST_BATCHES)\n",
    "train_data = batched_dataset.skip(TEST_BATCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SkipDataset shapes: ((None, None), (None,)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "class TEXT_MODEL(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocabulary_size,\n",
    "                 embedding_dimensions=128,\n",
    "                 cnn_filters=50,\n",
    "                 dnn_units=512,\n",
    "                 model_output_classes=2,\n",
    "                 dropout_rate=0.1,\n",
    "                 training=False,\n",
    "                 name=\"text_model\"):\n",
    "        super(TEXT_MODEL, self).__init__(name=name)\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocabulary_size,\n",
    "                                          embedding_dimensions)\n",
    "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=2,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=3,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=4,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "        \n",
    "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        if model_output_classes == 2:\n",
    "            self.last_dense = layers.Dense(units=1,\n",
    "                                           activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units=model_output_classes,\n",
    "                                           activation=\"softmax\")\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        l = self.embedding(inputs)\n",
    "        l_1 = self.cnn_layer1(l) \n",
    "        l_1 = self.pool(l_1) \n",
    "        l_2 = self.cnn_layer2(l) \n",
    "        l_2 = self.pool(l_2)\n",
    "        l_3 = self.cnn_layer3(l)\n",
    "        l_3 = self.pool(l_3) \n",
    "        \n",
    "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n",
    "        concatenated = self.dense_1(concatenated)\n",
    "        concatenated = self.dropout(concatenated, training)\n",
    "        model_output = self.last_dense(concatenated)\n",
    "        \n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the model parameters\n",
    "VOCAB_LENGTH = len(tokenizer.vocab)\n",
    "EMB_DIM = 200\n",
    "CNN_FILTERS = 100\n",
    "DNN_UNITS = 256\n",
    "OUTPUT_CLASSES = 2\n",
    "\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "NB_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the model parameters\n",
    "text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n",
    "                        embedding_dimensions=EMB_DIM,\n",
    "                        cnn_filters=CNN_FILTERS,\n",
    "                        dnn_units=DNN_UNITS,\n",
    "                        model_output_classes=OUTPUT_CLASSES,\n",
    "                        dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OUTPUT_CLASSES == 2:\n",
    "    text_model.compile(loss=\"binary_crossentropy\",\n",
    "                       optimizer=\"adam\",\n",
    "                       metrics=[\"accuracy\"])\n",
    "else:\n",
    "    text_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                       optimizer=\"adam\",\n",
    "                       metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 7s 33ms/step - loss: 0.0761 - accuracy: 0.9802\n",
      "Epoch 2/5\n",
      "110/215 [==============>...............] - ETA: 3s - loss: 0.0530 - accuracy: 0.9807"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "text_model.fit(train_data,epochs=NB_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 5ms/step - loss: 0.9619 - accuracy: 0.7731\n",
      "[0.9619171023368835, 0.773097813129425]\n"
     ]
    }
   ],
   "source": [
    "#evaluating the model\n",
    "results = text_model.evaluate(test_data)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
