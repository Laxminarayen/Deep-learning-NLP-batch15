{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Using cached Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from h5py->keras) (1.14.0)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.4.3\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mysql-connector-python-rf\n",
      "  Downloading mysql-connector-python-rf-2.2.2.tar.gz (11.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.9 MB 6.4 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: mysql-connector-python-rf\n",
      "  Building wheel for mysql-connector-python-rf (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mysql-connector-python-rf: filename=mysql_connector_python_rf-2.2.2-cp36-cp36m-linux_x86_64.whl size=249455 sha256=a1512e7a7b692317b9d58991b51282c88ebd86e0c25b66e651b407e1c9e7fd22\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/51/40/3f/136443b7177ee453aa9d6c8862fc2d1a1ea8ff8ee0999d1971\n",
      "Successfully built mysql-connector-python-rf\n",
      "Installing collected packages: mysql-connector-python-rf\n",
      "Successfully installed mysql-connector-python-rf-2.2.2\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting JPype1==0.6.3\n",
      "  Downloading JPype1-0.6.3.tar.gz (168 kB)\n",
      "\u001b[K     |████████████████████████████████| 168 kB 7.5 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: JPype1\n",
      "  Building wheel for JPype1 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for JPype1: filename=JPype1-0.6.3-cp36-cp36m-linux_x86_64.whl size=2508729 sha256=73d79a50d31924e76dcf4011f9a534e4c5f21964d8534c1183ae4edc64083662\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/26/1a/2a/2efafac3f262c651b1720038416cb6d196d283a32de884576e\n",
      "Successfully built JPype1\n",
      "Installing collected packages: JPype1\n",
      "Successfully installed JPype1-0.6.3\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting snowflake-sqlalchemy\n",
      "  Downloading snowflake_sqlalchemy-1.2.3-py2.py3-none-any.whl (29 kB)\n",
      "Collecting snowflake-connector-python<3.0.0\n",
      "  Downloading snowflake_connector_python-2.2.9-cp36-cp36m-manylinux2010_x86_64.whl (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 9.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: sqlalchemy<2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from snowflake-sqlalchemy) (1.3.15)\n",
      "Collecting azure-common<2.0.0\n",
      "  Downloading azure_common-1.1.25-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.26.0,>=1.20 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: pyOpenSSL<21.0.0,>=16.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (19.1.0)\n",
      "Requirement already satisfied, skipping upgrade: cffi<1.15,>=1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (2.9)\n",
      "Collecting pycryptodomex!=3.5.0,<4.0.0,>=3.2\n",
      "  Downloading pycryptodomex-3.9.8-cp36-cp36m-manylinux1_x86_64.whl (13.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.7 MB 64.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyjwt<2.0.0\n",
      "  Downloading PyJWT-1.7.1-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied, skipping upgrade: cryptography<3.0.0,>=2.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: pytz<2021.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: requests<2.24.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: boto3<1.15,>=1.4.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (1.14.16)\n",
      "Requirement already satisfied, skipping upgrade: asn1crypto<2.0.0,>0.24.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (1.3.0)\n",
      "Collecting azure-storage-blob<13.0.0,>=12.0.0; python_version >= \"3.5.2\"\n",
      "  Downloading azure_storage_blob-12.3.2-py2.py3-none-any.whl (280 kB)\n",
      "\u001b[K     |████████████████████████████████| 280 kB 74.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: certifi<2021.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (2020.4.5.2)\n",
      "Collecting oscrypto<2.0.0\n",
      "  Downloading oscrypto-1.2.0-py2.py3-none-any.whl (192 kB)\n",
      "\u001b[K     |████████████████████████████████| 192 kB 78.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.5.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyOpenSSL<21.0.0,>=16.2.0->snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from cffi<1.15,>=1.9->snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (2.20)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<2.24.0->snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.18.0,>=1.17.16 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3<1.15,>=1.4.4->snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (1.17.16)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3<1.15,>=1.4.4->snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3<1.15,>=1.4.4->snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (0.9.4)\n",
      "Collecting azure-core<2.0.0,>=1.6.0\n",
      "  Downloading azure_core-1.7.0-py2.py3-none-any.whl (121 kB)\n",
      "\u001b[K     |████████████████████████████████| 121 kB 77.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting msrest>=0.6.10\n",
      "  Downloading msrest-0.6.17-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 5.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.16->boto3<1.15,>=1.4.4->snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (0.15.2)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.16->boto3<1.15,>=1.4.4->snowflake-connector-python<3.0.0->snowflake-sqlalchemy) (2.8.1)\n",
      "Collecting requests-oauthlib>=0.5.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting isodate>=0.6.0\n",
      "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 5.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 78.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: azure-common, pycryptodomex, pyjwt, azure-core, oauthlib, requests-oauthlib, isodate, msrest, azure-storage-blob, oscrypto, snowflake-connector-python, snowflake-sqlalchemy\n",
      "Successfully installed azure-common-1.1.25 azure-core-1.7.0 azure-storage-blob-12.3.2 isodate-0.6.0 msrest-0.6.17 oauthlib-3.1.0 oscrypto-1.2.0 pycryptodomex-3.9.8 pyjwt-1.7.1 requests-oauthlib-1.3.0 snowflake-connector-python-2.2.9 snowflake-sqlalchemy-1.2.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting jaydebeapi\n",
      "  Downloading JayDeBeApi-1.2.3-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: JPype1; python_version > \"2.7\" and platform_python_implementation != \"Jython\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from jaydebeapi) (0.6.3)\n",
      "Installing collected packages: jaydebeapi\n",
      "Successfully installed jaydebeapi-1.2.3\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting emoji\n",
      "  Downloading emoji-0.5.4.tar.gz (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 2.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for emoji: filename=emoji-0.5.4-py3-none-any.whl size=42175 sha256=876dcec2dfd2f889d522cd5cac86a5d32946e5d9855cd047469cab3d8c987702\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/22/95/98/a21ffcc81fac65949a085ceff9dca4a145a32d9bbfcbf1cb31\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-0.5.4\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install mysql-connector-python-rf\n",
    "!pip install JPype1==0.6.3 --force-reinstall\n",
    "!pip install --upgrade snowflake-sqlalchemy\n",
    "#!pip install jaydebeapi\n",
    "#!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (516.2 MB)\n",
      "\u001b[K     |█████████████████████████████   | 466.2 MB 86.1 MB/s eta 0:00:01"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 516.2 MB 16 kB/s \n",
      "\u001b[?25hCollecting absl-py>=0.7.0\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 74.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.18.1)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting keras-preprocessing>=1.1.0\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 2.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.2.1-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 3.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.14.0)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.30.0-cp36-cp36m-manylinux2010_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 74.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<2.3.0,>=2.2.0\n",
      "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 71.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 7.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py<2.11.0,>=2.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (2.10.0)\n",
      "Collecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.4.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.34.2)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "\u001b[K     |████████████████████████████████| 454 kB 65.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.12.2)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (46.1.3.post20200330)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.23.0)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.19.1-py2.py3-none-any.whl (91 kB)\n",
      "\u001b[K     |████████████████████████████████| 91 kB 16.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "\u001b[K     |████████████████████████████████| 779 kB 54.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.5.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2020.4.5.2)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 75.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
      "Building wheels for collected packages: absl-py, termcolor\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121931 sha256=c1fa42483a11b87b69a391b1b0691216e83deff89f0092bf639ce710919a3939\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/c3/af/84/3962a6af7b4ab336e951b7877dcfb758cf94548bb1771e0679\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=5c28fd3154adc5b372450c0d509b03a7afc3d2a2e00a7aeff5be40da4d2be7e2\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "Successfully built absl-py termcolor\n",
      "Installing collected packages: absl-py, gast, keras-preprocessing, opt-einsum, grpcio, markdown, pyasn1-modules, cachetools, google-auth, tensorboard-plugin-wit, google-auth-oauthlib, tensorboard, google-pasta, astunparse, termcolor, tensorflow-estimator, tensorflow\n",
      "Successfully installed absl-py-0.9.0 astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 google-auth-1.19.1 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.30.0 keras-preprocessing-1.1.2 markdown-3.2.2 opt-einsum-3.2.1 pyasn1-modules-0.2.8 tensorboard-2.2.2 tensorboard-plugin-wit-1.7.0 tensorflow-2.2.0 tensorflow-estimator-2.2.0 termcolor-1.1.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT PACKAGES \n",
    "import os\n",
    "#os.system('python -m spacy download en')\n",
    "\n",
    "#to Ignore the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "from pandas import Series, DataFrame\n",
    "import re\n",
    "\n",
    "#to tokenize the text\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize#to tokenize the text\n",
    "import nltk\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from textblob import Word, Blobber\n",
    "import configparser\n",
    "\n",
    "\n",
    "import mysql.connector\n",
    "import psycopg2\n",
    "from collections import OrderedDict\n",
    "#for train and test splitting\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer# to convert the text into sequence of vectors\n",
    "# For evalution\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import boto3\n",
    "import textwrap\n",
    "#for encoading\n",
    "import ftfy\n",
    "from ftfy import fix_encoding\n",
    "from ftfy import fix_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#For lemmitization\n",
    "import en_core_web_sm\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords# for stopwords removal\n",
    "#for Bert modelling\n",
    "from sqlalchemy import create_engine\n",
    "from snowflake.sqlalchemy import URL\n",
    "\n",
    "#for loading pretrained word embedding\n",
    "import gensim \n",
    "from gensim import corpora\n",
    "import pyLDAvis \n",
    "import pyLDAvis.gensim \n",
    " \n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "#for mathematical operation\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For constructing Depp leaning network\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional,RepeatVector,SpatialDropout1D\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing training dataset\n",
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       61\n",
       "location    2533\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, keyword, location, text, target]\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for duplicates\n",
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting to lower text\n",
    "df['tweet'] = df.apply(lambda row: str(row['text']).lower(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this #earthquake m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask. canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to 'shelter in place' are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby #alaska as ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                              tweet  \n",
       "0       1  our deeds are the reason of this #earthquake m...  \n",
       "1       1             forest fire near la ronge sask. canada  \n",
       "2       1  all residents asked to 'shelter in place' are ...  \n",
       "3       1  13,000 people receive #wildfires evacuation or...  \n",
       "4       1  just got sent this photo from ruby #alaska as ...  "
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fix Encoding Issues\n",
    "df['tweet'] = df['tweet'].astype(str).apply(fix_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand contractions\n",
    "\n",
    "contractions_dict = {\n",
    "    \"didn't\": \"did not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    #\"cant\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    #\"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    #\"didnt\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    #\"doesnt \": \"does not \",\n",
    "    \"don't\": \"do not\",\n",
    "    #\"dont \" : \"do not \",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"here's\":\"here is\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he had\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'd\": \"i had\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she had\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they had\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we had\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "\n",
    "def expand_contractions(s, contractions_dict=contractions_dict):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, s)\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       our deeds are the reason of this #earthquake m...\n",
       "1                  forest fire near la ronge sask. canada\n",
       "2       all residents asked to 'shelter in place' are ...\n",
       "3       13,000 people receive #wildfires evacuation or...\n",
       "4       just got sent this photo from ruby #alaska as ...\n",
       "                              ...                        \n",
       "7608    two giant cranes holding a bridge collapse int...\n",
       "7609    @aria_ahrary @thetawniest the out of control w...\n",
       "7610    m1.94 [01:04 utc]?5km s of volcano hawaii. htt...\n",
       "7611    police investigating after an e-bike collided ...\n",
       "7612    the latest: more homes razed by northern calif...\n",
       "Name: tweet, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to correct the Slang ASAP->As Soon As Possible\n",
    "def translator(user_string):\n",
    "    user_string = user_string.split(\" \")\n",
    "    j = 0\n",
    "    for _str in user_string:\n",
    "        # File path which consists of Abbreviations.\n",
    "        fileName = \"Slang\"\n",
    "        # File Access mode [Read Mode]\n",
    "        accessMode = \"r\"\n",
    "        with open(fileName, accessMode) as myCSVfile:\n",
    "            # Reading file as CSV with delimiter as \"=\", so that abbreviation are stored in row[0] and phrases in row[1]\n",
    "            dataFromFile = csv.reader(myCSVfile, delimiter=\"=\")\n",
    "            # Removing Special Characters.\n",
    "            _str = re.sub('[^a-zA-Z0-9-_.]', '', _str)\n",
    "            for row in dataFromFile:\n",
    "                # Check if selected word matches short forms[LHS] in text file.\n",
    "                if _str.upper() == row[0]:\n",
    "                    # If match found replace it with its Abbreviation in text file.\n",
    "                    user_string[j] = row[1]\n",
    "            myCSVfile.close()\n",
    "        j = j + 1\n",
    "    # Replacing commas with spaces for final output.\n",
    "    temp = ' '.join(user_string)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling translator\n",
    "df['trans'] = df['tweet'].apply(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       our deeds are the reason of this #earthquake m...\n",
       "1                  forest fire near la ronge sask. canada\n",
       "2       all residents asked to 'shelter in place' are ...\n",
       "3       13,000 people receive #wildfires evacuation or...\n",
       "4       just got sent this photo from ruby #alaska as ...\n",
       "                              ...                        \n",
       "7608    two giant cranes holding a bridge collapse int...\n",
       "7609    @aria_ahrary @thetawniest the out of control w...\n",
       "7610    m1.94 [01:04 utc]?5km s of volcano hawaii. htt...\n",
       "7611    police investigating after an e-bike collided ...\n",
       "7612    the latest: more homes razed by northern calif...\n",
       "Name: trans, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['trans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuation, html tags, symbols, numbers, etc.\n",
    "\n",
    "def remove_noise(text):\n",
    "    # Dealing with Punctuation\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling remove_noise function in order to remove noise\n",
    "df['trans'] = df['trans'].apply(lambda x: remove_noise(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "def remove_punctuations(string):\n",
    "    return ''.join(c for c in string if c not in punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trans'] = df.apply(lambda row: remove_punctuations(row['trans']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"remove whitespaces before and after string\")\n",
    "df['trans'] = df['trans'].str.strip()\n",
    "\n",
    "#\"remove double spaces within sentence\")\n",
    "df['trans'] = df['trans'].map(lambda x: \" \".join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(string):    \n",
    "    tokenized = word_tokenize(string)\n",
    "    filtered_sentence = [word for word in tokenized if not word in stop_words]\n",
    "    return ' '.join(c for c in filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trans'] = df.apply(lambda row: remove_stopwords(row['trans']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemetization\n",
    "def spacy_lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['check']  = df['trans'] .apply(spacy_lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def stemming(text):\n",
    "    text = [stemmer.stem(word) for word in text.split()]\n",
    "    return ' '.join(text)\n",
    "\n",
    "df['tweet']  = df['tweet'] .apply(stemming)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tweet</th>\n",
       "      <th>trans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this #earthquake m...</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask. canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to 'shelter in place' are ...</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby #alaska as ...</td>\n",
       "      <td>get send photo ruby alaska smoke wildfire pour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                              tweet  \\\n",
       "0       1  our deeds are the reason of this #earthquake m...   \n",
       "1       1             forest fire near la ronge sask. canada   \n",
       "2       1  all residents asked to 'shelter in place' are ...   \n",
       "3       1  13,000 people receive #wildfires evacuation or...   \n",
       "4       1  just got sent this photo from ruby #alaska as ...   \n",
       "\n",
       "                                               trans  \n",
       "0       deeds reason earthquake may allah forgive us  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  resident ask shelter place notify officer evac...  \n",
       "3  people receive wildfire evacuation order calif...  \n",
       "4  get send photo ruby alaska smoke wildfire pour...  "
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            deeds reason earthquake may allah forgive us\n",
       "1                   forest fire near la ronge sask canada\n",
       "2       resident ask shelter place notify officer evac...\n",
       "3       people receive wildfire evacuation order calif...\n",
       "4       get send photo ruby alaska smoke wildfire pour...\n",
       "                              ...                        \n",
       "7608     two giant crane hold bridge collapse nearby home\n",
       "7609    ariaahrary thetawniest control wild fire calif...\n",
       "7610                                       volcano hawaii\n",
       "7611    police investigate ebike collided car little p...\n",
       "7612    late home raze northern california wildfire ab...\n",
       "Name: trans, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['trans']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['trans']\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            deeds reason earthquake may allah forgive us\n",
       "1                   forest fire near la ronge sask canada\n",
       "2       resident ask shelter place notify officer evac...\n",
       "3       people receive wildfire evacuation order calif...\n",
       "4       get send photo ruby alaska smoke wildfire pour...\n",
       "                              ...                        \n",
       "7608     two giant crane hold bridge collapse nearby home\n",
       "7609    ariaahrary thetawniest control wild fire calif...\n",
       "7610                                       volcano hawaii\n",
       "7611    police investigate ebike collided car little p...\n",
       "7612    late home raze northern california wildfire ab...\n",
       "Name: trans, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_X, test_X, train_y, test_y = train_test_split(X,y,test_size=0.3,random_state=0,stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613,)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3024\n",
       "1    2305\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1318\n",
       "1     966\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#reviews = x_mix.values\n",
    "#sentiments = y_mix.values\n",
    "\n",
    "train_reviews = np.array(X_train)\n",
    "train_sentiments = np.array(y_train)\n",
    "\n",
    "test_reviews = np.array(X_test)\n",
    "test_sentiments = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train_reviews = train_reviews\n",
    "norm_test_reviews = test_reviews\n",
    "#norm_bench_reviews = bench_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.keras.preprocessing.text.Tokenizer(oov_token='<UNK>')\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(norm_train_reviews)\n",
    "t.word_index['<PAD>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('symptoms', 13322), ('<PAD>', 0), 1)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), min([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), t.word_index['<UNK>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting to sequence\n",
    "train_sequences = t.texts_to_sequences(norm_train_reviews)\n",
    "test_sequences = t.texts_to_sequences(norm_test_reviews)\n",
    "#bench_sequences = t.texts_to_sequences(norm_bench_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size=11901\n",
      "Number of Documents=5329\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size={}\".format(len(t.word_index)))\n",
    "print(\"Number of Documents={}\".format(t.document_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_lens = [len(s) for s in train_sequences]\n",
    "test_lens = [len(s) for s in test_sequences]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12, 6))\n",
    "h1 = ax[0].hist(train_lens)\n",
    "h2 = ax[1].hist(test_lens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the docs\n",
    "MAX_SEQUENCE_LENGTH = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5329, 1000), (2284, 1000))"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad dataset to a maximum review length in words\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "#ben_X_test = tf.keras.preprocessing.sequence.pad_sequences(bench_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "X_train.shape, X_test.shape#,ben_X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoading labels\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "le = LabelEncoder()\n",
    "num_classes=2 # positive -> 1, negative -> 0\n",
    "On = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentiments = test_sentiments.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiments = train_sentiments.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = le.fit_transform(train_sentiments)\n",
    "y_test = le.transform(test_sentiments)\n",
    "#ben_y_test = le.transform(bench_senti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = On.fit_transform(train_sentiments)\n",
    "y_test = On.transform(test_sentiments)\n",
    "#ben_y_test = le.transform(bench_senti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11901"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(t.word_index)\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 1000, 300)         3570300   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_11 (Spatia (None, 1000, 300)         0         \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 1000, 128)         219648    \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 3,954,813\n",
      "Trainable params: 3,954,813\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 300 # dimension for dense embeddings for each token\n",
    "LSTM_DIM = 128 # total LSTM units\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(tf.keras.layers.SpatialDropout1D(0.1))\n",
    "model.add(tf.keras.layers.LSTM(LSTM_DIM, return_sequences=True))\n",
    "model.add(tf.keras.layers.LSTM(LSTM_DIM, return_sequences=False))\n",
    "#model.add(tf.keras.layers.LSTM(LSTM_DIM, return_sequences=False))\n",
    "#model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_DIM, return_sequences=False)))\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"Adam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "43/43 [==============================] - 109s 3s/step - loss: 0.6231 - accuracy: 0.6453 - val_loss: 0.4839 - val_accuracy: 0.7692\n",
      "Epoch 2/5\n",
      "43/43 [==============================] - 108s 3s/step - loss: 0.3247 - accuracy: 0.8722 - val_loss: 0.5044 - val_accuracy: 0.7608\n",
      "Epoch 3/5\n",
      "43/43 [==============================] - 108s 3s/step - loss: 0.1625 - accuracy: 0.9432 - val_loss: 0.6519 - val_accuracy: 0.7552\n",
      "Epoch 4/5\n",
      "43/43 [==============================] - 108s 3s/step - loss: 0.1013 - accuracy: 0.9629 - val_loss: 0.7412 - val_accuracy: 0.7570\n",
      "Epoch 5/5\n",
      "43/43 [==============================] - 109s 3s/step - loss: 0.0696 - accuracy: 0.9765 - val_loss: 0.6638 - val_accuracy: 0.7495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3d681dd6d8>"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=batch_size, \n",
    "          shuffle=True, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.56%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without stop words and stemming done\n",
    "#2 layer 74.65%\n",
    "#Bi- dir 75.22%\n",
    "\n",
    "#with stop word and without stemming\n",
    "#2 layer 76.80%\n",
    "#Bi- dir 74.43%\n",
    "#Accuracy: 77.15% MAX_SEQUENCE_LENGTH = 1000\n",
    "\n",
    "#with Slang conversion\n",
    "#2 layer 74.12%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Depth=2: 0.5770577933450087\n",
      "Accuracy for Depth=4: 0.6138353765323993\n",
      "Accuracy for Depth=6: 0.6256567425569177\n",
      "Accuracy for Depth=8: 0.6361646234676007\n",
      "Accuracy for Depth=10: 0.6357267950963222\n",
      "Accuracy for Depth=15: 0.648861646234676\n",
      "Accuracy for Depth=20: 0.6414185639229422\n",
      "Accuracy for Depth=30: 0.6396672504378283\n",
      "Accuracy for Depth=40: 0.6475481611208407\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# random forest model creation\n",
    "depth = [2,4,6,8,10,15,20,30,40]\n",
    "for d in depth:\n",
    "    rfc = RandomForestClassifier(max_depth=d, random_state=20)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    # predictions\n",
    "    print (\"Accuracy for Depth=%s: %s\" % (d, accuracy_score(y_test, rfc.predict(X_test))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Depth=40: 0.6440455341506129\n",
      "Accuracy for Depth=60: 0.6457968476357268\n",
      "Accuracy for Depth=70: 0.6457968476357268\n",
      "Accuracy for Depth=80: 0.6457968476357268\n",
      "Accuracy for Depth=90: 0.6457968476357268\n",
      "Accuracy for Depth=100: 0.6457968476357268\n"
     ]
    }
   ],
   "source": [
    "depth = [40,60,70,80,90,100]\n",
    "for d in depth:\n",
    "    rfc = RandomForestClassifier(max_features= 'sqrt',max_depth=d,n_estimators=1000, random_state=20)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    # predictions\n",
    "    print (\"Accuracy for Depth=%s: %s\" % (d, accuracy_score(y_test, rfc.predict(X_test))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.667544 using {'max_features': 'sqrt', 'n_estimators': 1000}\n",
      "0.630512 (0.016347) with: {'max_features': 'sqrt', 'n_estimators': 10}\n",
      "0.656910 (0.012803) with: {'max_features': 'sqrt', 'n_estimators': 100}\n",
      "0.667544 (0.013331) with: {'max_features': 'sqrt', 'n_estimators': 1000}\n",
      "0.629446 (0.019090) with: {'max_features': 'log2', 'n_estimators': 10}\n",
      "0.655407 (0.015326) with: {'max_features': 'log2', 'n_estimators': 100}\n",
      "0.667232 (0.015529) with: {'max_features': 'log2', 'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "#Grid search'\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# define dataset\n",
    "x, y = make_blobs(n_samples=1000, centers=2, n_features=100, cluster_std=20)\n",
    "# define models and parameters\n",
    "model = RandomForestClassifier()\n",
    "n_estimators = [10, 100, 1000]\n",
    "max_features = ['sqrt', 'log2']\n",
    "# define grid search\n",
    "grid = dict(n_estimators=n_estimators,max_features=max_features)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tf-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\n",
    "tfidf.fit(train_X)\n",
    "train_tfidf = tfidf.fit_transform(train_X)\n",
    "test_tf_idf = tfidf.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5329x7542 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 50290 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Using CountVectorizer to change the teweets to vectors\n",
    "count_vectorizer = CountVectorizer(analyzer='word', binary=True)\n",
    "count_vectorizer.fit(train_X)\n",
    "\n",
    "train_vectors = count_vectorizer.fit_transform(train_X)\n",
    "\n",
    "\n",
    "\n",
    "# Printing first vector\n",
    "print(train_vectors[0].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vectors = count_vectorizer.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.71779141, 0.7030303 , 0.69756839])"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a simple MultinomialNB model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import model_selection, feature_extraction, linear_model\n",
    "model = MultinomialNB(alpha=1)\n",
    "\n",
    "# Using cross validation to print out our scores\n",
    "scores = model_selection.cross_val_score(model, train_tfidf, train_y, cv=3, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.71779141, 0.7030303 , 0.69756839])"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training model with train_vectors and target variable\n",
    "model.fit(train_tfidf, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7968476357267951"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_y, model.predict(test_tf_idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See You As Soon As Possible\n"
     ]
    }
   ],
   "source": [
    "translator('CU ASAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from en_core_web_sm==2.3.1) (2.3.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.44.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.23.0)\n",
      "Requirement already satisfied: thinc==7.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (46.1.3.post20200330)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.18.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.4.5.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.9)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/en_core_web_sm\n",
      "-->\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp =spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how do your thought process work when he tell you he be angry'"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_lemmatize_text(\"how did your thought process work when he told you he was angry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the official tokenization script created by the Google team\n",
    "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-hub\n",
      "  Downloading tensorflow_hub-0.8.0-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 5.7 MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow-hub) (1.18.1)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow-hub) (3.12.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow-hub) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorflow-hub) (46.1.3.post20200330)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.8.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 7.8 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.91\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import re\n",
    "import tokenization\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(clf_output)\n",
    "    \n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = bert_encode(train_X.values, tokenizer, max_len=160)\n",
    "#test_input = bert_encode(test.text.values, tokenizer, max_len=160)\n",
    "train_labels = train_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  101, 26539,  5488, ...,     0,     0,     0],\n",
       "        [  101,  5811,  2757, ...,     0,     0,     0],\n",
       "        [  101,  2606,  5635, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  1038, 24652, ...,     0,     0,     0],\n",
       "        [  101, 22091,  6799, ...,     0,     0,     0],\n",
       "        [  101,  4575,  2713, ...,     0,     0,     0]]),\n",
       " array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0]]),\n",
       " array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]))"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = bert_encode(test_X.values, tokenizer, max_len=160)\n",
    "test_labels = test_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 1024)]       0           keras_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 1)            1025        tf_op_layer_strided_slice[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 335,142,914\n",
      "Trainable params: 335,142,913\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model building\n",
    "model = build_model(bert_layer, max_len=160)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "285/285 [==============================] - 2358s 8s/step - loss: 0.0692 - accuracy: 0.9772 - val_loss: 0.6988 - val_accuracy: 0.8227\n",
      "Epoch 3/4\n",
      "285/285 [==============================] - 2380s 8s/step - loss: 0.0309 - accuracy: 0.9855 - val_loss: 1.0220 - val_accuracy: 0.7917\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "train_history = model.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_split=0.2,\n",
    "    epochs=4,\n",
    "    callbacks=[checkpoint],\n",
    "    batch_size=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.151060</td>\n",
       "      <td>0.946047</td>\n",
       "      <td>0.614087</td>\n",
       "      <td>0.829268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.069213</td>\n",
       "      <td>0.977246</td>\n",
       "      <td>0.698764</td>\n",
       "      <td>0.822702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.039174</td>\n",
       "      <td>0.984518</td>\n",
       "      <td>0.744638</td>\n",
       "      <td>0.825516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.030891</td>\n",
       "      <td>0.985456</td>\n",
       "      <td>1.021999</td>\n",
       "      <td>0.791745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  0.151060  0.946047  0.614087      0.829268\n",
       "1  0.069213  0.977246  0.698764      0.822702\n",
       "2  0.039174  0.984518  0.744638      0.825516\n",
       "3  0.030891  0.985456  1.021999      0.791745"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics=pd.DataFrame(model.history.history)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.04%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_input, test_labels, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
